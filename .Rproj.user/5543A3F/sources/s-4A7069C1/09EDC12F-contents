---
title: "R Notebook"
output: html_notebook
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

Update this chunk to include everything i want to start a project with

```{r setup, eval=TRUE, include=TRUE}
# copy the following into each script   
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = FALSE
)

# script-specific libraries
suppressPackageStartupMessages({
  library(stringdist)
  library(vegan)
})

# general-use packages
suppressPackageStartupMessages({
  library(here)
  library(fs)
  library(glue)
  library(tidyverse) # loads a bunch of packages (see below)
  library(readxl)
  library(cowplot)
  library(lubridate)
  library(patchwork)
  library(broom)
  library(ggeffects)
  library(viridis)
  library(arsenal) # for summary(comparedf())
  library(sjmisc) # for rotate_df()
  library(envDocument)
  library(inspectdf)
  library(conflicted)
})

tidyverse_packages(include_self = TRUE) # this lists the tidyverse packages that are loaded by just running the single command `library(tidyverse)
#  [1] "broom"      "cli"        "crayon"     "dbplyr"     "dplyr"      "forcats"    "ggplot2"    "haven"     
#  [9] "hms"        "httr"       "jsonlite"   "lubridate"  "magrittr"   "modelr"     "pillar"     "purrr"     
# [17] "readr"      "readxl"     "reprex"     "rlang"      "rstudioapi" "rvest"      "stringr"    "tibble"    
# [25] "tidyr"      "xml2"       "tidyverse" 

# Sometimes, two or more packages use the same function names. the {conflicted} package lets you set which package gets precedence. For example, the next line enforces that filter() refers to the {dplyr} package. If you want to use the command filter() from a different package, you just need to precede it with the desired package name, like this: stats::filter.  
conflict_prefer("filter", "dplyr", quiet = TRUE)
conflict_prefer("mutate", "dplyr", quiet = TRUE)
conflict_prefer("select", "dplyr", quiet = TRUE)
conflict_prefer("summarise", "dplyr", quiet = TRUE)
conflict_prefer("first", "dplyr", quiet = TRUE)
conflict_prefer("here", "here", quiet = TRUE)
conflict_prefer("separate", "tidyr", quiet = TRUE)
conflict_prefer("unite", "tidyr", quiet = TRUE)
conflict_prefer("intersect", "dplyr", quiet = TRUE)
conflict_prefer("setdiff", "dplyr", quiet = TRUE) # w/out this, R crashes
conflict_prefer("to_factor", "sjmisc", quiet = TRUE)
conflict_prefer("trim", "glue", quiet = TRUE)

# Provide real numbers, not scientific notation.
options(scipen = 999)
```

Add to end of script. the git2r::repository() command returns the github
repo URL

<details>

<summary>Reproducibility receipt</summary>

```{r}
# datetime
Sys.time()

# repository
git2r::repository(here::here())

envDocument::env_doc("table", git = FALSE)
# this reports the R environment that you are using, especially the package and R versions. It is a good idea to include this information at the end of every report you generate, to document the details of your computing environment. If you want to really ensure replicable science, RStudio Desktop (not cloud) provides the {renv} package, which lets you create a private snapshot of the packages inside your project folder. This is beyond the scope of this class, but at least you know about it now.
# sessionInfo() # base R method
# sessioninfo::session_info() # package sessioninfo method
```

Interesting new packages to try out

```{r install packages if needed}
# https://gist.github.com/DrK-Lo/a945a29d6606b899022d0f03109b9483
packages_needed <- c("eulerr", "ggeffects", "hablar", "klaR", "modelr", "snakecase", "UpSetR", "zetadiv", "arsenal", "viridis", "sjstats", "sjPlot", "sjmisc", "grateful", "mmtravis", "propr", "mosaic", "AHMbook")

for (i in 1:length(packages_needed)){
if(!(packages_needed[i] %in% installed.packages())){install.packages(packages_needed[i], dependencies = TRUE)}
}

for (i in 1:length(packages_needed)){
  library(packages_needed[i], character.only = TRUE)
}

# packages used by ENV-7033B
install.packages(c("arsenal", "beanplot", "beeswarm", "BiocManager", "broom", "car", "colourpicker", "conflicted", "corrplot", "cowplot", "DataExplorer", "datapasta", "devtools", "envDocument", "eulerr", "foreign", "gapminder", "ggbeeswarm", "ggeffects", "ggrepel", "ggthemes", "glue", "gridExtra", "hablar", "here", "iNEXT", "iNextPD",  "inspectdf", "janitor", "knitr", "labelled", "lattice", "lme4", "lmtest", "MASS", "modelr", "MuMIn", "mvabund", "nlme", "nycflights13", "patchwork", "popbio", "RColorBrewer", "readxl", "repurrrsive", "rmarkdown", "remotes", "sessioninfo", "sjmisc", "sjPlot", "sjstats", "stargazer", "swirl", "tidyverse", "tufte", "tufterhandout", "UpSetR", "vegan", "vcd", "viridis", "xlsx"), dependencies = TRUE)

remotes::install_github('JohnsonHsieh/iNextPD')

# additional packages to install
install.packages(c("aPCoA", "ezknitr", "stringdist", "arm", "DHARMa", "snakecase", "zetadiv", "corrr", "pivottabler", "ggforce", "ggeasy", "propr", "mosaic", "AHMbook", dep = TRUE))
remotes::install_github("fishsciences/artemis", build_vignettes = TRUE)
remotes::install_github("ibecav/CGPfunctions")
remotes::install_github("tobiasgf/lulu")
remotes::install_github("TYMichaelsen/mmtravis")
remotes::install_github("EvaMaeRey/flipbookr")


library(BiocManager)
BiocManager::install(c("GenomicRanges", "Biobase", "IRanges", "AnnotationDbi", "dada2", "phyloseq", "GenomicAlignments", "Organism.dplyr", "dada2"))
```

DHARMa: residual diagnostics for mixed-effects models eulerr:
alternative to venn diagrams hablar: convert data types (e.g. factor to
chr) klaR: functions for classification and visualization, e.g.
discriminant analysis modelr: Modelling Functions that Work with the
Pipe (see also broom) snakecase: Convert strings to any case UpsetR:
alternative to venn/eulerr diagrams zetadiv: zeta diversity arsenal: An
Arsenal of 'R' Functions for Large-Scale Statistical Summaries (e.g.
compare dataframes with comparedf) performance: Utilities for computing
measures to assess model quality that are not directly provided by R's
'base' or 'stats' packages. R2 for different models including mixed
effects models, intraclass correlation coefficient, compare model
performance easystats: a set of utility packages for extracting
information from models: insight, performance, bayestestR, parameters,
correlation, estimate, see, report
`r remotes::install_github("easystats/easystats")` sjstats: convenience
functions for calculating summaries of anovas, mixed-effect models, and
bayesian models, including effect-size estimates sjPlot: convenience
functions for generating marginal effects and interactions figures, and
tables of effects (like anova tables) sjmisc: complements dplyr package,
by doing recoding, dichotomizing, or grouping variables, setting and
replacing missing values, etc. Also good for exploring datasets.
rotate\_df to transpose dataframes (see also data.table::transpose)
janitor: utiities to clean up data frames, including clean\_names()
(clean names) grateful: to cite R packages labelled: to add and
manipulate value labels (var\_label) conflicted: to detect command
conflicts between packages. just load the package. if there is a name
conflict, conflicted will issue an error message library(conflicted)
conflict\_prefer("mutate", "dplyr") conflict\_prefer("select", "dplyr")
conflict\_prefer("summarise", "dplyr") conflict\_prefer("filter",
"dplyr") \# [conflicted] Will prefer dplyr::filter over any other
package

mmtravis and propr:
<https://albertsenlab.org/what-is-wrong-with-correlating-relative-abundance-everything/>
to analyse compositional data (i.e. relative abundances without knowing
the absolute abundances) interactions:
<https://interactions.jacob-long.com/index.html> to visualise
statistical interactions mosaic: to teach R from the book Statistical
Modelling: A Fresh Approach install.packages("AHMbook") \# Applied
Hierarchical Modelling Kery and Royle

# graphics packages

<http://r-graph-gallery.com> \# website with graphics examples

<http://www.ggplot2-exts.org> \# website with ggplot extensions
<https://github.com/AtherEnergy/ggTimeSeries> \# a timeseries ggplot
extension. see calendar\_heatmaps for ArcDyn ggparcoord: parallel
coordinates plots. similar to alluvial plots ggparallel: parallel
coordinates plots. more flexible ggalluvial: alluvial plots ggridges:
'joyplots' aka ridgeline plots
(<https://blog.revolutionanalytics.com/2017/07/joyplots.html>) ggrepel:
move labels around so that they don't overlap ggforce: extensions to
ggplot2 that are quite useful
<https://rviews.rstudio.com/2019/09/19/intro-to-ggforce/> ggeasy: theme
transformations made easy, axis label rotations
<https://github.com/jonocarroll/ggeasy> ggtext: rich text formatting
<https://wilkelab.org/ggtext/> ggeffects: compute marginal effects from
statistical models and return as data frames that can be used with
ggplot2, e.g. ggpredict. used to make dataframes with predicted values
from fitted models, plus CIs ggplot2 tutorial book by Claus Wilke
<https://wilkelab.org/practicalgg/>
<https://github.com/clauswilke/dataviz>
<https://github.com/clauswilke/dviz.supp> ggsignif: add posthoc
significance comparisons patchwork: alternative to facet\_wrap,
facet\_grid, grid.arrange gtsummary: make summary tables of model
results and of data using a language requires gt package:
remotes::install\_github("rstudio/gt", ref = gtsummary::gt\_sha) vcd:
mosaic plots (nested boxes of categorical data) slopegraphs (this is
what i want for paired data, including the live-edit analysis) \# this
is a good tutorial, see code below under vidyakesavan
<https://github.com/vidyakesavan/slopegraph-in-R>
<http://daydreamingnumbers.com/blog/slopegraph-in-r/>

# this is another good tutorial, see code below under newcancer

devtools::install\_github("ibecav/CGPfunctions")
<https://cran.r-project.org/web/packages/CGPfunctions/vignettes/Using-newggslopegraph.html>
library(CGPfunctions) data(newcancer) newggslopegraph(newcancer, Year,
Survival, Type)

# alternatives

remotes::install\_github("leeper/slopegraph")
<https://rdrr.io/github/leeper/slopegraph/man/ggslopegraph.html>
<https://ibecav.github.io/slopegraph/>

Filepaths here <https://malco.io> \# see code below

easystats: <https://github.com/easystats>;
<https://easystats.github.io/blog/> useful utilities for frequentist and
bayesian stats packages: insight, see, bayestestR, blog, easystats,
estimate, parameters, performance, circus, correlation
devtools::install\_github("easystats/correlation")

Bayesian First aid: install\_github("rasmusab/bayesian\_first\_aid")
<http://sumsar.net>
<https://www.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r>

lulu devtools::install\_github("tobiasgf/lulu")

pivottabler: pivot tables RVerbalExpressions: make regex expressions
with commands

making pretty tables in R
<https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html>
<https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r/>
<https://www.littlemissdata.com/blog/prettytables>

artemis: design and analysis of eDNA survey studies
devtools::install\_github("fishsciences/artemis", build\_vignettes =
TRUE)

vroom: very fast version of readr, <https://vroom.r-lib.org> fs: clean
file-system commands (e.g. dir\_ls() vs. list.files() vs. ls)
<https://fs.r-lib.org>

flipbookr: to create side by side, step-throughs of pipelines
devtools::install\_github("EvaMaeRey/flipbookr") Basic protocol is to
write a markdown document (\*.Rmd) with Xaringan syntax and use to
create an HTML file. Inside the Xaringan-formatted document, there are
flipbookr commands to convert named code chunks to step-by-step reveals.
The easiest way to see this is to open a new file: File -\> New File -\>
R Markdown... -\> From Template -\> A Minimal Flipbook. That will open
an Rmd file that i knit with xaringan into the flipbook html

corrr: correlations as dataframes, and make network plots
<https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr>

```{r}
library(corrr)
x <- correlate(mtcars)
x
network_plot(x)
network_plot(x, min_cor = .1)
network_plot(x, min_cor = .6)
network_plot(x, min_cor = .7, colors = c("red", "green"), legend = TRUE)
```

library(arm) \# package to accompany Gelman's book on Applied Regression
and Multilevel Models library(faraway) \# utilities by Julian Faraway

packagefinder <http://www.zuckarelli.de/packagefinder/tutorial.html>
findPackage(c("slopegraph"))

renv \# manage packages for a project
<https://rstudio.github.io/renv/index.html>

repurrrsive <https://github.com/jennybc/repurrrsive> Examples of
Recursive lists and nested or split data frames, useful for learning
purrr <https://jennybc.github.io/purrr-tutorial/>

anonymizer anonymize personally identifable information
devtools::install\_github("paulhendricks/anonymizer")

update this

To 'un-attach' a loaded package

```{r}
detach(package:knitr)
```

```{r str_c}
library(tidyverse)
# Missing inputs give missing outputs
str_c(c("a", NA, "b"), "-d")
# Use str_replace_NA to display literal NAs:
str_c(str_replace_na(c("a", NA, "b")), "-d")

# there were NAs in some of the taxon columns, so str_c would just return an NA for the whole concatenation. use str_replace_na to replace NAs with literal NAs
gbifdf <- gbifdf %>% 
    mutate(
    consensusClassification = case_when(
        matchType == "BLAST_EXACT_MATCH" ~ str_c(str_replace_na(Class), str_replace_na(Order), str_replace_na(Family), str_replace_na(Genus), str_replace_na(Species), scientificName, sep = "_"),
        matchType == "BLAST_CLOSE_MATCH" ~ str_c(str_replace_na(Class), str_replace_na(Order), str_replace_na(Family), str_replace_na(Genus), scientificName, sep = "_"),
        matchType == "BLAST_WEAK_MATCH" ~ str_c(str_replace_na(Class), str_replace_na(Order), scientificName, sep = "_"),
        )
    ) %>% 

```

```{r remove objects from environment}
# rm(list=ls())
```

great post on how to change a variable (using various versions of
dplyr::mutate)
<https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe>

```{r replace NAs with 0s}
# replace the NAs in the OTU read number columns with 0s (see stackoverflow post below)

otu_MTB_REF_A <- otu_MTB_REF_A %>%
    dplyr::mutate_at(vars(Hhmlbody:OTUreadtot), funs(replace(., is.na(.), 0))) # the . stands in for the variables that are being assessed

# up to date syntax
otu_MTB_REF_A <- otu_MTB_REF_A %>%
    dplyr::mutate_at(vars(Hhmlbody:OTUreadtot), ~replace(., is.na(.), 0)) # the . stands in for the variables that are being assessed

assign(paste0("otu_MTB_REF_", i), dplyr::mutate_at(get(paste0("otu_MTB_REF_", i)), vars(Hhmlbody:OTUreadtot), ~replace(., is.na(.), 0)))

# example from my code where the function is sum(as.numeric(.))
# calculate sum of all COI spike reads for each sample. 
sum_idx_genomecov <- idx_genomecov %>%
    group_by(site, trap, period, dilution, COI_Species) %>%
    summarise_at(vars(mapped_reads), ~sum(as.numeric(.)))

# If you want to apply multiple transformations, pass a list of
# functions. When there are multiple functions, they create new
# variables instead of modifying the variables in place:
iris %>% mutate_if(is.numeric, list(scale2, log))

# The list can contain purrr-style formulas:
iris %>% mutate_if(is.numeric, list(~scale2(.), ~log(.)))

# Note how the new variables include the function name, in order to
# keep things distinct. The default names are not always helpful
# but you can also supply explicit names:
iris %>% mutate_if(is.numeric, list(scale = scale2, log = log))

# When there's only one function in the list, it modifies existing
# variables in place. Give it a name to instead create new variables:
iris %>% mutate_if(is.numeric, list(scale2))
iris %>% mutate_if(is.numeric, list(scale = scale2))


```

mutate\_at

```{r}
iris <- as_tibble(iris)

# All variants can be passed functions and additional arguments,
# purrr-style. The _at() variants directly support strings. Here
# we'll scale the variables `height` and `mass`:
scale2 <- function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)
starwars <- starwars
starwars %>% mutate_at(c("height", "mass"), scale2) # doesn't work because there are NA values

# You can pass additional arguments to the function:
starwars %>% mutate_at(c("height", "mass"), scale2, na.rm = TRUE) # now it works

# You can also pass formulas to create functions on the spot, purrr-style:
starwars %>% mutate_at(c("height", "mass"), ~scale2(., na.rm = TRUE))

# You can also supply selection helpers to _at() functions but you have
# to quote them with vars():
iris
iris %>% mutate_at(vars(matches("Sepal")), log)
iris %>% transmute_at(vars(starts_with("Sepal")), log)

# do mutate with a function, where the function has a conditional (here, converting count to presence absence)
iris
library(vegan)
binarise1 <- function(x, na.rm = FALSE) decostand(x, method = "pa") # pa == presence absence
binarise2 <- function(x, na.rm = FALSE) if_else(x < 3.5, 0, 1)
iristest <- iris %>%
    mutate_at(vars(starts_with("Sepal")), binarise2)
head(iristest)
tail(iristest)

# The _if() variants apply a predicate function (a function that
# returns TRUE or FALSE) to determine the relevant subset of
# columns. Here we divide all the numeric columns by 100:
starwars %>% mutate_if(is.numeric, scale2, na.rm = TRUE)

# mutate_if() is particularly useful for transforming variables from
# one type to another
iris %>% mutate_if(is.factor, as.character)
iris %>% mutate_if(is.double, as.integer)


# Multiple transformations ----------------------------------------

# If you want to apply multiple transformations, pass a list of
# functions. When there are multiple functions, they create new
# variables instead of modifying the variables in place:
iris %>% mutate_if(is.numeric, list(scale2, log))

# The list can contain purrr-style formulas:
iris %>% mutate_if(is.numeric, list(~scale2(.), ~log(.)))

# Note how the new variables include the function name, in order to
# keep things distinct. The default names are not always helpful
# but you can also supply explicit names:
iris %>% mutate_if(is.numeric, list(scale = scale2, log = log))

# When there's only one function in the list, it modifies existing
# variables in place. Give it a name to instead create new variables:
iris %>% mutate_if(is.numeric, list(scale2))
iris %>% mutate_if(is.numeric, list(scale = scale2))
```

To create an empty vector that i can fill later
<https://stackoverflow.com/questions/3413879/how-to-create-an-empty-r-vector-to-add-new-items>

```{r}
a<-rep(NA, 10)
a
a[1] <- 1
a

vec <- rep(NA, 15)
vec[1] <- t.test(BB_obs, CL_obs)[["p.value"]]
vec[2] <- t.test(BB_obs, EC_obs)[["p.value"]]
```

<https://stackoverflow.com/questions/28873057/sum-across-multiple-columns-with-dplyr>
Sum across a particular set of multiple columns

```{r}
df=data.frame(
  x1=c(1,0,0,NA,0,1,1,NA,0,3),
  x2=c(1,1,NA,1,1,0,NA,NA,0,1),
  x3=c(0,1,0,1,1,0,NA,NA,0,1),
  x4=c(1,0,NA,1,0,0,NA,0,0,1),
  x5=c(1,1,NA,1,1,1,NA,1,0,1))

# this is the one that works best
df %>% mutate(sum = select(., x1:x5) %>% rowSums(., na.rm = TRUE))
df %>% mutate(sum = select(., x3:x5) %>% rowSums(., na.rm = TRUE))
df %>% mutate(sum = select(., c(x1,x2,x5)) %>% rowSums(., na.rm = TRUE))

# conditional without column selection
rowSums(df > 1, na.rm = TRUE)

# this also works
df$x3x5.total <- df %>%
    select(x3:x5) %>%
    rowSums(na.rm=TRUE)
head(df)

# i haven't been able to apply this yet
df %>% mutate(sum = select(., x1:x5) %>% apply(1, sum, na.rm=TRUE))  # apply is a generic base function that applies a function to either the rows (1) or cols (2) of a selection
```

```{r read in tables}
protaxfolder1 <- "Vietnam_leeches_swarm_lulu_20180404"
protaxfolder2 <- "doug_vietnam_16S_Vietnam_weighted_Protax_20180618"
rank <- c("order", "family", "genus", "species")

loadFile1 <- function(rank) {
   filename <- str_c("all_2015WWFWCS_otu_table_swarm_lulu_vert16S_20180404.fas.w_", rank, "_probs")
   # e.g. "all_2015WWFWCS_otu_table_swarm_lulu_vert16S_20180404.fas.w_order_probs"

   df <- read_tsv(file.path(protaxfolder1, protaxfolder2, filename),
   	col_names = c("OTU", "taxid", "logprob", "rank", "protax"),
  	na = "NA",
	  col_types = cols(
	     OTU = col_character(),
	     taxid = col_number(),
	     logprob = col_double(),
	     rank = col_integer(),
	     protax = col_character()
	     )
	 	)

    df$prob <- exp(df$logprob) # add prob column

    df <- df %>% dplyr::select(OTU, taxid, logprob, prob, rank, protax) # reorder

    df # output is df
}

# run the function for four different files
for(rank in rank) {
    assign(str_c("protax_", rank), loadFile1(rank))
    }

# x == otu_table, y == env_table, z == "Arthropoda", "Insecta", "Araneae|Insecta", etc.
taxonSubset <- function(x, y, z) {
    pits_working_all <- x %>%
        filter(str_detect(Blast_taxonomy, z)) %>%
        dplyr::select(OTU_ID, everything()) %>%
        dplyr::select(-(origorder:Blast_taxonomy)) %>%
        sjmisc::rotate_df(rn = "samplecode", cn = TRUE) %>% # rn = NULL puts samplecodes in rownames
        left_join(y, by = "samplecode") %>%
        arrange(Experiment_Project, Label_Tube_line_3, Label_Tube_line_4, Label_Tube_line_2)

    otus <- pits_working_all %>%
        select(-samplecode, -(origorder:Lane))

    env <- pits_working_all %>%
        select(samplecode, Sample_Description:Reads) %>%
        rename(Experiment_Control = Experiment_Project, Ring = Label_Tube_line_2, Trap = Label_Tube_line_3, CO2 = Label_Tube_line_4)

    output <- list(pits_working_otus = otus, pits_working_env = env)
    return(output)
}
```

df \<- dd %\>% mutate( exp = case\_when( soup &in& c("A", "B",
"C","D","E") \~ "cross-spp", soup &in& c("a", "a", "c","d","e","f","g")
\~ "within-spp", TRUE \~ as.character(exp) )

write functions, complex function

```{r}
# x is the year that i want to extract from mergedf, add some spike info and then summarise
mergesumm <- function(x) {
    df <- mergedf %>%
        filter(year(date) == x) %>%
        mutate(
            COI = case_when(
                sp == "spike1" ~ "Lepidoptera_Bombycidae_Bombyx_mori_COI_SPIKE_0.2",
                sp == "spike2" ~ "Coleoptera_Mordellidae_COI_SPIKE_0.4",
                sp == "spike3" ~ "Coleoptera_Elateridae_COI_SPIKE_0.8",
                TRUE ~ as.character(COI)
            )
        ) %>%
        mutate(
            PC = case_when(
                str_starts(sp, "spike") == TRUE ~ 1.0,
                TRUE ~ as.numeric(PC)
            )
        ) %>%
        group_by(COI) %>%
        summarise(
            max_PC = max(PC),
            sum_mapped_reads=sum(mapped_reads),
            Order = first(Order),
            Family = first(Family),
            Genus = first(Genus),
            Species_BOLD = first(Species_BOLD),
            mitogenome = first(mitogenome)
        ) %>%
        arrange(desc(sum_mapped_reads))
    df
    }

mergedf2003 <- mergesumm(2003)
mergedf2004 <- mergesumm(2004)
mergedf2005 <- mergesumm(2005)



# x == pits_working_otus
phyloseqFilterPlot <- function(x) {
    TotalCounts <- c(colSums(x))
    tdt = data.table(OTUs = colnames(x), TotalCounts = colSums(x), OTU = colnames(x))
    # ggplot(tdt, aes(TotalCounts)) +
    #   geom_histogram() +
    #   ggtitle("Histogram of Total Counts per OTU")
    taxcumsum = tdt[ .N, by = TotalCounts]
    setkey(taxcumsum, TotalCounts)
    taxcumsum[, CumSum := cumsum(N)]
    # Define the plot
    pCumSum = ggplot(taxcumsum, aes(TotalCounts, CumSum)) +
      geom_point() +
      xlab("Filtering Threshold:  Minimum Read Number per OTU") +
      ylab("Number of OTUs That Would Be Filtered Out") +
      ggtitle("Number of OTUs that would be filtered out at different minimum OTU read-numbers")
    pCumSum + scale_x_continuous(breaks = scales::pretty_breaks(n = 25), limits = c(0, 500)) + scale_y_continuous(breaks = scales::pretty_breaks(n = 25))
}

# remove OTUs < phyloseq-determined threshold size
# x == pits_working_otus, y = threshold_otu_size == 40
phyloseqFilterOTUMin <- function(x, y) {
    cat("There are", dim(pits_working_otus)[2], "OTUs before phyloseq filtering. \n")
    threshold_otu_size <- y
    pits_working_otus <- pits_working_otus[, colSums(pits_working_otus) >= threshold_otu_size]
    cat("There are", dim(pits_working_otus)[2], "OTUs after phyloseq filtering. \n")
    return(pits_working_otus)
}


# set cell to 0 if < y% of its colSum
# function includes an anonymous function inside, using map
# map2_dfc is a variant of map that (1) takes 2 input parameters and (2) colbinds the output list into a dataframe
# i'm not sure about the double use of x,y in the map2() and the function, but the function seems to work correctly. i looked at the outputs
# repair names (chk for duplicate col names, add ".." to names beginning with a digit)
# x == otu_table, y == 1 (for 1%)
minOTUPct <- function(x, y) {
    pits_working_otus <- map2_dfc(x,y, function(x,y) {x[x < sum(x)*(y/100)] <- 0; x})
    pits_working_otus <- as_tibble(pits_working_otus, .name_repair = "universal")
    return(pits_working_otus)
}
```

remove OTUs with fewer than some number of incidences, occupancies

```{r}
# keep OTUs with more than some number of incidences
minocc <- 5
otu.data <- otu.data[ , specnumber(otu.data, MARGIN = 2) >= minocc]
```

conditional mutation using if\_else() \# there is also a base R
ifelse(), which is not type-strict

```{r conditional mutation using if_else}
x <- c(-5:5, NA); x
if_else(x < 0, NA_integer_, x)
if_else(x < 0, "negative", "positive", "missing")


# Unlike ifelse, if_else preserves types
x <- factor(sample(letters[1:5], 10, replace = TRUE))
ifelse(x %in% c("a", "b", "c"), x, factor(NA))
if_else(x %in% c("a", "b", "c"), x, factor(NA))
# Attributes are taken from the `true` vector,

# nested syntax using ifelse, because 'false' outcome is a logical
# see also case_when below
assay2 <- assay2 %>%
    mutate(Plate = ifelse(Plate == 4, 1, ifelse(Plate == 5, 2, ifelse(Plate == 6, 3, NA))))

# problem:  i have a list of mitogenome names:
# 3-1_Coleoptera_Mordellidae_20ng_COI
# CAN_43_AAL9618_Diptera_Chironomidae_Cricotopus_triannulatus
# ...
#
# I need to create a new variable whose value is dependent on whether the mitogenome name contains a specific string ("ng_COI"), which indicates that the mitogenome is a COI spike species.  
# use grepl() to find mitogenome names that are COI spike names.  grepl() returns either TRUE or FALSE
# and use if_else() to set the values for when the grepl output is TRUE or FALSE
idx_meta_genomecov$COI_spike <- if_else(grepl("ng_COI",
                                              idx_meta_genomecov$mitogenome),
                                        "COI_Spike", "ArcDyn_Species") # this regex works only for the mitogenome dataset
idx_meta_genomecov$COI_Species <- if_else(grepl("_COI$",
                                                idx_meta_genomecov$mitogenome),
                                          "COI_Spike", "ArcDyn_Species") # this regex works for both the mitogenome and barcode datasets
```

copy value of a protax.genus column into protax.consensus column,
conditional on whether protax.species is NA

```{r}
protax_all <- protax_all %>% mutate(protax_consensus = ifelse(!is.na(protax.species), protax.species, protax.genus))
```

conditional select columns see below for more complex criteria
<http://www.rebeccabarter.com/blog/2019-01-23_scoped-verbs/#select_if>

```{r}
# remove columns whose sums == 0
# the criterion returns a vector of matching columns that the bracket uses to select, but it requires that all columns can be 
x[ , colSums(x) != 0]

set.seed(123)
dat <- data.frame(var1 = runif(10), var2 = rnorm(10), var3 = rlnorm(10), var4 = "notNumeric", var5 =0, var6 = FALSE )
View(dat)

dat %>% dplyr::select(where(is.logical) | where(is.numeric))
dat %>% dplyr::select_if(is.character)
```

remove columns that match some criteria note that negate() can be
removed to get the complementary set of columns
<https://stackoverflow.com/questions/10608060/excluding-columns-from-a-dataframe-based-on-column-sums>

```{r}
set.seed(123)
dat <- data.frame(var1 = runif(10), var2 = rnorm(10), var3 = rlnorm(10), var4 = "notNumeric", var5 =0, var6 = FALSE )
View(dat)
# remove cols that are numeric and sum < 15
dat %>% 
  select_if(negate(function(col) is.numeric(col) && sum(col) < 15))
```

# select columns using multiple criteria, conditional select

# this is the solution i need

<https://community.rstudio.com/t/select-if-with-multiple-conditions-redux/18833>

```{r}
testdf <- data_frame(varone = c(1,1,1),
                     vartwo = c(1,2,0),
                     varthree = c(3,3,3))

testdf %>% map_lgl(., ~var(.x) != 0) # logical vector
which(map_lgl(testdf, ~var(.x) != 0)) # return only the TRUE one(s) (cannot pipe testdf, but when inside select() command, can run)
testdf %>% select(which(map_lgl(., ~var(.x) != 0))) # returns vector

testdf %>% 
  select(which(map_lgl(., ~var(.x) != 0)), 
         varthree
         ) # variance != 0 or "three"

testdf %>% 
  select(which(map_lgl(., ~vegan::specnumber(.x, MARGIN=2) > 2)), 
         contains("one")
         ) 

testdf %>% 
  select(which(map_lgl(., ~sum(.x) > 8)), 
         ends_with("one")
         ) 


set.seed(123)
dat <- data.frame(var1 = runif(10), var2 = rnorm(10), var3 = rlnorm(10), var4 = "notNumeric", var5 =0, var6 = FALSE, var7 = 0)
dat

# use multiple select() criteria in two ways
# inside one function (is.numeric && specnumber) and
# multiple select criteria: df %>% select(var1, var2:3)
dat %>% 
  select(which(map_lgl(., ~is.numeric(.x) && 
                         vegan::specnumber(.x, MARGIN=2) > 2)), 
         var6:var7
         ) 

# multiple conditions in one function
# the ~fun(.x) shortcut is used instead of an anonymous function
dat %>% 
  select(which(map_lgl(., ~(is.numeric(.x) && sum(.x) > 8))), 
         ends_with("6")
         ) 
# parens around the ~ function is optional, apparently
dat %>% 
  select(which(map_lgl(., ~is.numeric(.x) && sum(.x) > 8)), 
         ends_with("6")
         )
```

```{r}
set.seed(1)
dta <- data.frame(observation = 1:20,
                  valueA = runif(n = 20),
                  valueB = runif(n = 20),
                  valueC = runif(n = 20),
                  valueD = runif(n = 20))
dta[2:5,3] <- NA
dta[2:10,4] <- NA
dta[7:20,5] <- NA
dta

# https://stackoverflow.com/questions/34852112/conditionally-selecting-columns-in-dplyr-where-certain-proportion-of-values-is-n
dta %>% select(which(colMeans(is.na(.)) < 0.5)) %>% head

is.na(dta)
colSums(is.na(dta))
colSums(is.na(dta))/nrow(dta)
colMeans(is.na(dta)) < 0.5 # valueD col > 0.5

which(colMeans(is.na(dta)) < 0.5) # all cols but valueD
```

remove (or keep) a set of columns with names that don't match (or match)
a vector of names
<https://stackoverflow.com/questions/39858468/remove-multiple-matching-columns-from-multiple-character-string>

e.g. "3781\|3751" or species is the vector with the column names. grepl
finds the columns that match this vector within names(df), and
select\_if() uses the logical vector to select. The ! finds the columns
that don't match the vector of names.

```{r}
select_if(df, !grepl("3781|3751", names(df)) )
select_if(df, !grepl(paste(species, collapse = "|"), names(df)) )
# species is a vector of strings with species codes

# note that there are other 
```

conditional mutation using case\_when(). Used for multiple if and else
if statements

```{r conditional mutation using case_when}
# the ~ is used for the output when the conditional is met

x <- 1:50
case_when(
  x %% 35 == 0 ~ "fizz buzz", # %% == modulo. modulo of x/5 == 0 (x is a multiple of 5)
  x %% 5 == 0 ~ "fizz",
  x %% 7 == 0 ~ "buzz",
  TRUE ~ as.character(x)
)

# Like an if statement, the arguments are evaluated in order, so you must
# proceed from the most specific to the most general. This won't work:
case_when(
  TRUE ~ as.character(x),
  x %%  5 == 0 ~ "fizz",  # %% == modulo. modulo of x/5 == 0 (x is a multiple of 5)
  x %%  7 == 0 ~ "buzz",  # x is a multiple of 7
  x %% 35 == 0 ~ "fizz buzz"  # x is a multiple of 35
)

# the returned value can be a function
case_when(
  TRUE ~ as.numeric(x), # %% == modulo. 
  x %% 5 == 0 ~ x * 1.1 + 2, # modulo of x/5 == 0 (x is a multiple of 5)
  x %%  7 == 0 ~ x + 100,  # x is a multiple of 7
  )

# set NA, set to NA, set cell to NA
# All RHS values need to be of the same type. Inconsistent types will throw an error.
# This applies also to NA values used in RHS: NA is logical, use
# typed values like NA_real_, NA_complex, NA_character_, NA_integer_ as appropriate.
case_when(
  x %% 35 == 0 ~ NA_character_,
  x %% 5 == 0 ~ "fizz",
  x %% 7 == 0 ~ "buzz",
  TRUE ~ as.character(x)
)
case_when(
  x %% 35 == 0 ~ 35,
  x %% 5 == 0 ~ 5,
  x %% 7 == 0 ~ 7,
  TRUE ~ NA_real_
)


# case_when is particularly useful inside mutate when you want to
# create a new variable that relies on a complex combination of existing
# variables
starwars %>%
  select(name:mass, gender, species) %>%
  mutate(
    type = case_when(
      height > 200 | mass > 200 ~ "large",
      species == "Droid"        ~ "robot",
      TRUE                      ~  "other"
    )
  )

# here, i'm recoding values inside the input vector instead of making a new vector. Thus, i use "mitogenome" on the LHS of the ==
# the 'TRUE ~ as.character(mitogenome)' line preserves the other values in the vector as their original values (which were characters)
idx_meta_genomecov <-  idx_meta_genomecov %>%
    mutate(
        mitogenome = case_when(
            mitogenome == "1-2_Lepidoptera_Bombycidae_Bombyx_mori_10ng_COI" ~ "Lepidoptera_Bombycidae_Bombyx_mori_COI",
            mitogenome == "2-1_Coleoptera_Elateridae_40ng_COI" ~ "Coleoptera_Elateridae_COI",
            mitogenome == "3-1_Coleoptera_Mordellidae_20ng_COI" ~ "Coleoptera_Mordellidae_COI",
            TRUE ~ as.character(mitogenome)
        )
    )

# here i'm recoding values in the input_amount column depending on the value in the mitogenome column
# the 'TRUE ~ as.numeric(input_amount)' line keeps the existing values in input_amount when the mitogenome value != the three spike species names
mocks_idx_meta_genomecov_EF_test <- mocks_idx_meta_genomecov_EF %>%
    mutate(input_amount = case_when(
        mitogenome == "Lepidoptera_Bombycidae_Bombyx_mori_COI" ~ 0.2,
        mitogenome == "Coleoptera_Mordellidae_COI" ~ 0.4,
        mitogenome == "Coleoptera_Elateridae_COI" ~ 0.8,
        TRUE ~ input_amount
    )
)

# replace NA values with 0 in the mockgradient experiment
# two conditions
mocks_idx_meta_genomecov_GH_test <- mocks_idx_meta_genomecov_GH_test %>%
    mutate(input_amount = case_when(
        experiment == "mockgradient" & is.na(input_amount) ~ 0,
        TRUE ~ input_amount
    )
)

# detect a substring and replace with that substring
kafa_envTest <- kafa_env %>%
  mutate(subPlotLoc = subPlotID) %>%
  mutate(subPlotLoc = str_replace(subPlotLoc, "ctr", "_ctr")) %>%
  mutate(subPlotLoc = str_replace(subPlotLoc, "__ctr", "_ctr")) %>%
  mutate(
      subPlotLoc = case_when(
          str_detect(subPlotLoc, "_ctr") == TRUE ~ "ctr",
          str_detect(subPlotLoc, "_0") == TRUE ~ "0",
          str_detect(subPlotLoc, "_120") == TRUE ~ "120",
          str_detect(subPlotLoc, "_240") == TRUE ~ "240",
          TRUE ~ "MISSING"
  )
) %>%
  select(day, plotID, subPlotType, subPlotID, subPlotLoc, forestTypeCorrect, everything())


# for non-COI_spike species, and non-inputDNA species, set input_amount to 0
# negation of %in% is !(mitogenome %in% c("a", "b", "c", ...)), meaning mitogenome not in this vector
# or just put ! in front of columnname
# # negation of %in% is !mitogenome %in% c("a", "b", "c", ...), meaning mitogenome not in this vector
# str_detect(mitogenome, "_COI$") == FALSE,  meaning mitogenome names that do not have "_COI" at the end (non-spike species)

mocks_idx_meta_genomecov_EF_test <- mocks_idx_meta_genomecov_EF_test %>%
    mutate(input_amount = case_when(
        str_detect(mitogenome, "_COI$") == FALSE & !(mitogenome %in% c("Diptera_Syrphidae_Helophilus_lapponicus_BOLD:ACE4226", "Diptera_Syrphidae_Parasyrphus_tarsatus_BOLD:AAC1834", "Diptera_Syrphidae_Syrphus_torvus_BOLD:AAC6088", "Diptera_Syrphidae_Helophilus_groenlandicus_BOLD:AAB1982", "Diptera_Muscidae_Spilogona_micans_BOLD:AAG1686", "Diptera_Tachinidae_Peleteria_aenea_BOLD:AAZ5252", "Diptera_Muscidae_Spilogona_monacantha_BOLD:ACA4207", "Diptera_Anthomyiidae_Zaphne_occidentalis_BOLD:ABZ1244", "Diptera_Muscidae_Spilogona_almqvistii_BOLD:AAM9104", "Diptera_Scathophagidae_Gonarcticus_arcticus_BOLD:AAM7340", "Diptera_Muscidae_Drymeia_segnis_BOLD:AAD7664", "Diptera_Muscidae_Limnophora_groenlandica_BOLD:AAC6873", "Diptera_Syrphidae_Platycheirus_groenlandicus_BOLD:AAZ4195", "Diptera_Muscidae_Lophosceles_minimus_BOLD:ACM5032", "Diptera_Anthomyiidae_Eutrichota_tunicata_BOLD:AAG2440", "Diptera_Scathophagidae_Scathophaga_apicalis_BOLD:AAV1117", "Diptera_Muscidae_Spilogona_megastoma_BOLD:AAP9046", "Diptera_Scathophagidae_Scathophaga_nigripalpis_BOLD:ACR5253", "Diptera_Muscidae_Spilogona_novaesibiriae_BOLD:AAM9110", "Araneae_Lycosidae_Pardosa_glacialis_BOLD:AAA9651")) ~ 0,
        TRUE ~ as.numeric(input_amount)
    )
)

# set a value to NA (NA_real_, NA_complex, NA_character_, NA_integer_)
env_data_mitogenome <- env_data_mitogenome %>%
    mutate(
        PC = case_when(
            sp == "spike1" ~ NA_real_,
            sp == "spike2" ~ NA_real_,
            sp == "spike3" ~ NA_real_,
            TRUE ~ PC
        )
    )

# the TRUE ~ as.character(mitogenome) and TRUE ~ as.numeric(PC) keep all the non-matched cases
# starts_with: use str_starts in filter
#
mockdf <- left_join(mock_design, species, by = c("input_sp" = "sp")) %>%
    select(sample, experiment, run, input_amount, mitogenome, BOLD, Order, Family, Genus, Species_BOLD, mitogenome) %>%
    filter(experiment != "negctrl" & run == 2 & str_starts(sample, "PlateGH")) %>%
    mutate(
        mitogenome = case_when(
            input_amount == 0.2 ~ "Lepidoptera_Bombycidae_Bombyx_mori_COI",
            input_amount == 0.4 ~ "Coleoptera_Mordellidae_COI",
            input_amount == 0.8 ~ "Coleoptera_Elateridae_COI",
            TRUE ~ as.character(mitogenome)
        )
    ) %>%
    select(-run) %>%
    arrange(sample, input_amount)

# filter rows with grepl, filter grepl
# filter with stringr
# https://sebastiansauer.github.io/dplyr_filter/
# filter(grepl(pattern, x)) # x is the vector or string
# filter(str_detect(x, pattern)) # reverse of grepl syntax;  x is the vector or string
pits_arth <- pits_all %>% filter(grepl("Arthropoda", Blast_taxonomy))
pits_arth_str <- pits_all %>% filter(str_detect(Blast_taxonomy, "Arthropoda"))
mtcars %>%
  filter(!str_detect(rowname, "\\d"))  # negate
mtcars %>%
  filter(str_detect(rowname, "^L"))
mtcars %>%
  filter(rowname %in% c("Merc", "Toyota")) # full matches
mtcars %>%
  filter(str_detect(rowname, "Merc|Toy")) # partial matches
mtcars %>%
 filter(mtcars, !cyl %in% c(4, 6)) # filter in rows with cyl values not equal to 4 or 6
mtcars %>%
  filter(str_detect(rowname, "Merc") | str_detect(rowname, "Toy")) # alt syntax
mtcars %>%
  filter(!is.na(mpg)) # to filter out rows with NA in mpg
mtcars %>%
  filter(!is.na(mpg) & !is.na(hp))  # to filter out rows with NA in mpg or hp
mtcars %>%
  na.omit %>% nrow # base R shortcut to filter out rows with NA in any column

```

```{r recode variables using base R}
# http://rprogramming.net/recode-data-in-r/
assay$Strep_outcomeLW[assay$Strep_outcome=="Win"] <- 1
assay$Strep_outcomeLW[assay$Strep_outcome=="Loss"] <- 0
assay$Strep_outcomeLW[assay$Strep_outcome=="Draw"] <- NA_character_
assay$Strep_outcomeLW <- as.numeric(assay$Strep_outcomeLW)

# Replace the data in a field based on equal to some value
SchoolData$Grade[SchoolData$Grade==5] <- "Grade Five"

# Or replace based on greater than or equal to some value
SchoolData$Grade[SchoolData$Grade<=5] <- "Grade Five or Less"

# Or replace based on equal to some text
SchoolData$Grade[SchoolData$Grade=="Five"] <- "Grade Five"

# Or replace only missing data
# Note that ==NA does not work!
SchoolData$Grade[is.na(SchoolData$Grade)] <- "Missing Grade"

# Replace data based on the values in more than one field
SchoolData$SchoolType[SchoolData$Grade<=5 & SchoolStatus=="OPEN"] <- "Elementary School"

# Recode into a new field in R
# First create the new field
StudentData$NewGrade <- NA
# Then recode the old field into the new one for the specified rows
SchoolData$NewGrade[SchoolData$Grade==5] <- 5

# Recode into A New Field Using Data From An Existing field And Criteria from Another Field
# First create the new field
StudentData$NewGrade <- NA
# Then recode the old field into the new one for the specified rows
SchoolData$NewGrade[SchoolData$SchoolType=="Elementary"] <- SchoolData$Grade[SchoolData$SchoolType=="Elementary"]
```

Replace all values \> 0 to 1. Convert a dataframe to incidence
(presence/absence, presence absence, 0/1)

```{r Replace values with another value}
comm4inext[comm4inext > 0] <- 1 # change to binary dataframe
comm4inext[comm4inext <= 2] <- 0  # replace small numbers with 0s
```

```{r recode variables}
# Recode values with named arguments
# Only works replacing a character with a character, not a number
x <- sample(c("a", "b", "c"), 10, replace = TRUE)
x
recode(x, a = "Apple")
x
recode(x, a = "Apple", .default = NA_character_)
x
recode(x, a = "Apple", b = "Banana")
x <- recode(x, a = "Apple", b = "Banana")
x

# Named arguments also work with numeric values
x <- c(1:5, NA)
x
recode(x, `2` = 20L, `4` = 40L)

# Note that if the replacements are not compatible with .x,
# unmatched values are replaced by NA and a warning is issued.
recode(x, `2` = "b", `4` = "d")

# If you don't name the arguments, recode() matches by position
recode(x, "a", "b", "c")
recode(x, "a", "b", "c", .default = "other")
recode(x, "a", "b", "c", .default = "other", .missing = "missing")

# Supply default with levels() for factors
x <- factor(c("a", "b", "c"))
recode(x, a = "Apple", .default = levels(x))

# Use recode_factor() to create factors with levels ordered as they
# appear in the recode call. The levels in .default and .missing
# come last.
x <- c(1:4, NA)
recode_factor(x, `1` = "z", `2` = "y", `3` = "x")
recode_factor(x, `1` = "z", `2` = "y", .default = "D")
recode_factor(x, `1` = "z", `2` = "y", .default = "D", .missing = "M")

# When the input vector is a compatible vector (character vector or
# factor), it is reused as default.
recode_factor(letters[1:3], b = "z", c = "y")
recode_factor(factor(letters[1:3]), b = "z", c = "y")
```

```{r filter rows by string value in the field}
assay1 <- assay1 %>% filter(!str_detect(Invasion.score, 'discount|discount plate'))
# one way to filter by string rather than numeric value
```

```{r tally, count, and summarise}
# tally() is short-hand for summarise()
mtcars %>% tally()
# count() is a short-hand for group_by() + tally()
mtcars %>% count(cyl)
otus16S %>% count(Taxonomy_v_1.0_short)
otus16S %>% count(Taxonomy_v_1.0)

# by two variables
mtcars %>% count(cyl, mpg)
assay3 %>% count(Invader, Ps.genotype)

# add_tally() is short-hand for mutate()
mtcars %>% add_tally()
# add_count() is a short-hand for group_by() + add_tally()
mtcars %>% add_count(cyl)

# count and tally are designed so that you can call
# them repeatedly, each time rolling up a level of detail
starwars
species <- starwars %>% count(species, homeworld, sort = TRUE)
species
species %>% count(species, sort = TRUE)

# example from spikepipe dataset
env_data_mitogenome2 <- left_join(env_data_mitogenome, species)
env_data_mitogenome2 <- env_data_mitogenome2 %>%
    dplyr::filter(PC >= 0.1) %>%
    count(mitogenome) # count up how many times each mitogenome shows up

# add_count() is useful for groupwise filtering
# e.g.: show only species that have a single member
starwars %>%
  add_count(species) %>%
  filter(n == 1)

# summarise_at for multiple variables.  here, for each distinct value of Full_name_of_the_sample, I provide the sum of three variables: mapped_reads, sum_coverage, elaterid_mordellid_sum. Total is 4 columns.
sum_idx_meta_genomecov <- idx_meta_genomecov %>%
    group_by(Full_name_of_the_sample) %>%
    summarise_at(vars(mapped_reads, sum_coverage, elaterid_mordellid_sum), list(~sum(as.numeric(.)))) %>% # i don't think i need the ~ in front of sum
    arrange(mapped_reads)

# deprecation warning:  previous to dplyr 0.8.0, used funs(mean, median) but now use list(mean, median) instead
# BUG ALERT:  check that the columns that you want to sum are numeric.  when you transpose a table, the columns often turn into factors. In the future, try this:  list(sum(as.numeric(as.character(.)))))  # because to convert factors to numeric, first need to convert to character

## see next code chunk for a more comprehensive explanation of how to use multiple functions

qseqSumEven <- qseq %>%
    filter(!(otuID %in% c("HAP222", "HAP264"))) %>%
    filter(mock == "even") %>%
    group_by(soupVol, otuID) %>%
    summarise_at(vars(inputCOICopyNum, inputGDNA, mtagNum, readNum, mtagNumSpikeCorr, readNumSpikeCorr), list(median)) %>%
    arrange(otuID, inputCOICopyNum)

# https://stackoverflow.com/questions/27528907/how-to-convert-data-frame-column-from-factor-to-numeric
# to convert multiple columns
indx <- sapply(df, is.factor) # note that this logical vector could be generated in other ways that select only the cols that you want
df[indx] <- lapply(df[indx], function(x) as.numeric(as.character(x)))


# to apply different functions to different variables
    df <- tribble(
      ~category,   ~x,  ~y,  ~z,
          'a',      4,   6,   8,
          'a',      7,   3,   0,
          'a',      7,   9,   0,
          'b',      2,   8,   8,
          'b',      5,   1,   8,
          'b',      8,   0,   1,
          'c',      2,   1,   1,
          'c',      3,   8,   0,
          'c',      1,   9,   1
     )

df %>% group_by(category) %>% summarise(
      mean_x=mean(x),
      median_y=median(y),
      first_z=first(z)
    )

# a complex summarise example, using a group_by_at() to group by multiple variables
# note that i include a logical expression to sum number of column elements matching a condition (here, > 0)
# i can include mutate(), filter, sum(), first(), and n_distinct() functions while making new variables
sum_idx_meta_genomecov2 <- idx_meta_genomecov %>%
    filter(pctcov >= 0.10) %>%
    mutate(Year = year(Date)) %>%
    group_by_at(vars(EI_RUN, Year, Trap)) %>%
    summarise(
        weeks = n_distinct(Date),
        sum_mapped_reads=sum(mapped_reads),
        species_obs_mapped_reads=sum(mapped_reads > 0),  # sum() a logical:  mapped_read > 0
        sum_mapped_reads_COI_corr=sum(mapped_reads_COI_corr),
        sum_mapped_reads_COI_lysis_corr=sum(mapped_reads_COI_lysis_corr),
        species_obs_COI_lysis_corr=sum(mapped_reads_COI_lysis_corr > 0),
        Bombycid10_spike=first(Lepidoptera_Bombycidae_Bombyx_mori_10ng_COI),
        Mordellid20_spike=first(Coleoptera_Mordellidae_20ng_COI),
        Elaterid40_spike=first(Coleoptera_Elateridae_40ng_COI),
        lysis_buffer_multipler=first(lysis_buffer_proportion)
        ) %>%
    mutate(species_obs_mapped_reads_perweek = round(species_obs_mapped_reads/weeks, 1)) %>%
    arrange(Year, Trap)

# another conditional count (countif) example
# https://stackoverflow.com/questions/37377819/countif-equivalent-in-dplyr-summarise
# input
memberorders=data.frame(MemID=c('A','A','B','B','B','C','C','D'),
                        week = c(1,2,1,4,5,1,4,1),
                        value = c(10,20,10,10,2,5,30,3))
# sum value conditional on week <=2 or week <=4
# sum week conditional on week <=2 or week <=4
memberorders %>%
    group_by(MemID) %>%
    summarise(sum2 = sum(value[week<=2]),
              sum4 = sum(value[week<=4]),
              count2 = sum(week<=2),
              count4 = sum(week<=4)
              )

# extract a single column from a dataframe/tibble as a vector
column_name <- df %>% dplyr::pull(column_name)


```

summarise\_at()

```{r}
library(tidyverse)
by_species <- iris %>%
  group_by(Species)

# The _at() variants directly support strings:
starwars %>%
  summarise_at(c("height", "mass"), mean, na.rm = TRUE)

# You can also supply selection helpers to _at() functions but you have
# to quote them with vars():
starwars %>%
  summarise_at(vars(height:mass), mean, na.rm = TRUE)

# The _if() variants apply a predicate function (a function that
# returns TRUE or FALSE) to determine the relevant subset of
# columns. Here we apply mean() to the numeric columns:
starwars %>%
  summarise_if(is.numeric, mean, na.rm = TRUE)

# If you want to apply multiple transformations, pass a list of
# functions. When there are multiple functions, they create new
# variables instead of modifying the variables in place:
by_species %>%
  summarise_all(list(min, max))

# Note how the new variables include the function name, in order to
# keep things distinct. Passing purrr-style lambdas often creates
# better default names:
by_species %>%
  summarise_all(list(~min(.), ~max(.)))

# When that's not good enough, you can also supply the names explicitly:
by_species %>%
  summarise_all(list(min = min, max = max))

# When there's only one function in the list, it modifies existing
# variables in place. Give it a name to create new variables instead:
by_species %>% summarise_all(list(med = median))
by_species %>% summarise_all(list(Q3 = quantile), probs = 0.75)
```

Nice examples of summarise from \~/src/2019-bias-manuscript

The key sample metadata variables are the Mixture type (or experiment),
the number of species in the mixture, and the species in each mixture.

```{r setup, include=FALSE}
# Load package after downloading (recommended)
devtools::load_all("~/src/2019-bias-manuscript")
```

```{r}
data("brooks2015_sample_data")
data("brooks2015_counts")
data("brooks2015_species_info")
```

```{r}
sam <- brooks2015_sample_data
print(sam, n = 10)
```

There are 80 samples for each of the three mixture types:

```{r}
sam %>%
    group_by(Mixture_type) %>%
    summarise(Num_samples = n())
```

We'll ultimately just be using the 71 of 80 that contain two or more
species,

```{r}
sam %>%
    group_by(Mixture_type) %>%
    filter(Num_species > 1) %>%
    summarise(Num_samples = n())
```

Distinct compositions:

```{r}
sam %>%
    select(Mixture_type, Num_species, Species_list) %>%
    distinct() %>%
    group_by(Mixture_type) %>%
    filter(Num_species > 1) %>%
    summarise(Num_samples = n())
```

## Count data

The count data includes reads from Brooks2015's above- and
below-threshold tables, specifying reads that were classified above or
below a 97% sequence identity threshold to 16S sequences in their
database.

```{r}
print(brooks2015_counts, n = 10)
```

The vast majority of reads were classified above threshold,

```{r}
brooks2015_counts %>%
    group_by(Table) %>%
    summarise(Sum = sum(Count)) %>%
    mutate(Proportion = Sum / sum(Sum))
```

In preparing this dataframe, we have grouped all reads classified to
non-mock taxa as "Other". The vast majority of above and below threshold
reads were classified to the mock taxa,

```{r}
brooks2015_counts %>%
    group_by(Table, Taxon != "Other") %>%
    summarise(Sum = sum(Count)) %>%
    mutate(Proportion = Sum / sum(Sum))
```

We will follow Brooks2015 in using the above threshold reads only, and
will restrict to just the mock taxa. We will also create a new dataframe
that will be our main data frame for our analysis going forward.

```{r}
main <- brooks2015_counts %>%
    filter(Table == "above", Taxon != "Other") %>%
    select(Sample, Taxon, Count)
```

Let's add the sample data,

```{r}
main <- main %>%
    left_join(sam, by = "Sample")
```

and a column for whether the species is expected to be in the sample.

```{r}
main <- main %>%
    mutate(
        Expected = str_detect(Species_list, Taxon)
        )
```

Loads more dplyr, tidyr, and ggplot2 examples in
\~/src/2019-bias-manuscript/analysis/\*.Rmd files

```{r}
example <- as_tibble(iris) %>% nest(-Species)
as_tibble(chickwts) %>% nest(weight)

if (require("gapminder")) {
  gapminder %>%
    group_by(country, continent) %>%
    nest()

  gapminder %>%
    nest(-country, -continent)
}
```

some row and column selections using base R

```{r row and col selections using base R}
d <- read.csv(file.path("~/src/bds-files/chapter-08-r", "Dataset_S1.txt"))
colnames(d)[12] <- "percent.GC"
d$cent <- d$start >= 25800000 & d$end <= 29700000 # logical test, returns TRUE or FALSE
d$diversity <- d$Pi / (10*1000)
d$position <- (d$end + d$start) / 2


d[d$Pi > 16 & d$percent.GC > 80, c("start", "end", "Pi", "depth")]
# rows are selected with logical tests
# columns are selected in a vector, in any order i want
#
d$percent.GC[d$Pi > 16]
summary(d$Pi[d$cent]) # where d$cent = TRUE
summary(d$Pi[!d$cent]) # where d$cent = FALSE (note the !)

common_repclass <- c("LINE", "SINE", "LTR", "DNA", "Simple_repeat")
reps[reps$repClass %in% common_repclass, ] # use %in% to keep rows matching one of the values in common_repclass
common_repclass <- names(sort(table(reps$repClass), decreasing = TRUE)[1:5]) # programmatic version of the vector above
# Buffalo, Vince. Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools (Kindle Locations 5053-5055). O'Reilly Media. Kindle Edition.
```

new book on data visualization by Claus Wilke that uses ggplot2
<https://github.com/clauswilke/dataviz>

```{r ggplot2 using Buffalo Dataset_S1.txt}
# setwd("~/src/bds-files/chapter-08-r")
library(readr)
library(ggplot2)
d <- read_csv("~/src/bds-files/chapter-08-r/Dataset_S1.txt")
d$position <- (d$end + d$start) / 2
d$diversity <- d$Pi / (10*1000)

theme <- theme_bw() + 
  theme(plot.background = element_blank(), # remove background
        panel.grid.minor = element_blank(), # remove minor gridlines
        panel.grid.major = element_blank(), # remove major gridlines
        axis.line = element_line(color = 'black')) # thicker axis lines

# put the aesthetic mappings in aes(), which itself is probably best placed inside the ggplot2 call
ggplot(d) +
    geom_point(aes(x=position, y=diversity))
ggplot(d, aes(x=position, y=diversity)) +
    geom_point() +
    theme + # theme from above
    scale_color_grey()

ggplot(d, aes(x=position, y=diversity, color = cent)) + geom_point() + theme + scale_color_grey()
ggplot(d, aes(x=position, y=diversity, color = cent)) + geom_point(alpha=0.1) + theme + scale_color_grey()
ggplot(d, aes(x=position, y=diversity, color = cent)) + geom_point(alpha=0.4) + theme + scale_color_grey()

ggplot(d, aes(x=depth, y=total.SNPs)) + geom_point() + geom_smooth(color = "grey") + theme + scale_color_grey()
ggplot(d, aes(x=percent.GC, y=depth)) + geom_point() + geom_smooth(color = "grey") + theme + scale_color_grey()

d$GC.binned <- cut(d$percent.GC, 5) # create 5 bins out of the continuous variable d$percent.GC
d$GC.binned <- cut(d$percent.GC, c( 0, 25, 50, 75, 100)) # create bins with these breaks. the risk is that some values might fall outside my defined ranges, and these values are given NA
any(is.na(cut(d$percent.GC, c( 0, 25, 50, 75, 100))))  # this checks for NA values if i use these breaks to cut

d$GC.binned <- cut(d$percent.GC, 5) # create 5 bins out of the continuous variable d$percent.GC
ggplot(d, aes(x=depth)) + geom_density(aes(linetype=GC.binned), alpha=0.5) + theme
# note that i have two aes() statements.  I could also put linetype=GC.binned inside the ggplot(aes()) statement
ggplot(d, aes(x=depth, linetype=GC.binned)) + geom_density(alpha=0.5) + theme


ggplot(d, aes(x=GC.binned)) + geom_bar() + theme + theme(axis.text.x=element_text(size=5.5))
ggplot(d, aes(x=percent.GC)) + geom_bar() + theme + theme(axis.text.x=element_text(size=8)) # using ggplot2 default bins

# different bin widths
# scale_x_continuous sets limits on x range
ggplot(d, aes(x=Pi)) + geom_bar(binwidth = 1) + scale_x_continuous(limits=c(0.01, 80))
ggplot(d, aes(x=Pi)) + geom_bar(binwidth = .05) + scale_x_continuous(limits=c(0.01, 80))

# check if some values are in a vector
c(4,3,-1) %in% c(1,3,4,8)

setwd("~/src/bds-files/chapter-08-r")
reps <- read.delim("chrX_rmsk.txt.gz", header = TRUE)
head(reps, 3)

common_repclass <- c("LINE", "SINE", "LTR", "DNA", "Simple_repeat")
reps[reps$repClass %in% common_repclass, ]

## motif example
mtfs <- read.delim("motif_recombrates.txt", header=TRUE)
rpts <- read.delim("motif_repeats.txt", header=TRUE)
mtfs$pos <- paste(mtfs$chr, mtfs$motif_start, sep="-")
rpts$pos <- paste(rpts$chr, rpts$motif_start, sep="-")
mtfs$repeat_name <- rpts$name[match(mtfs$pos, rpts$pos)] # match() is equiv to mtfs$pos %in% rpts$pos.  put the rpts$name value in mtfs$repeat_name column if mtfs$post matches a value in rpts$pos
table(mtfs$pos %in% rpts$pos)

# one graph
p <- ggplot(mtfs, aes(x=dist, y=recom)) + geom_point(size=1, color="grey") + geom_smooth(aes(color = motif), method='loess', se=FALSE, span=1/10) + theme
p
gsave(p, "mtfs-01")

# facet by motif
p <- ggplot(mtfs, aes(x=dist, y=recom)) + 
    geom_point(size=1, color="grey") + 
    geom_smooth(method='loess', se=FALSE, span=1/10) + 
    facet_wrap(~ motif) + 
    theme
p
gsave(p, "mtfs-02")

# facet by repeat_name X motif
# y-axis and x-axis are held constant across facets
p <- ggplot(mtfs, aes(x=dist, y=recom)) + geom_point(size=1, color="grey") + geom_smooth(method='loess', se=FALSE, span=1/16) + facet_grid(repeat_name ~ motif) + theme
p
gsave(p, "mtfs-03")

# free y-axis
p <- ggplot(mtfs, aes(x=dist, y=recom)) + 
    geom_point(size=1, color="grey") + 
    geom_smooth(method='loess', se=FALSE, span=1/16) + 
    facet_wrap( ~ motif, scales="free_y") + 
    theme
p
gsave(p, "mtfs-04")

# facet_grid

p <- ggplot(mpg, aes(displ, cty)) + geom_point()
p
# Use vars() to supply variables from the dataset:
p + facet_grid(rows = vars(drv))
p + facet_grid(cols = vars(cyl))
p + facet_grid(vars(drv), vars(cyl))

# The historical formula interface is also available:

p + facet_grid(. ~ cyl)
p + facet_grid(drv ~ .)
p + facet_grid(drv ~ cyl)


# To change plot order of facet grid,
# change the order of variable levels with factor()

# If you combine a facetted dataset with a dataset that lacks those
# faceting variables, the data will be repeated across the missing
# combinations:
df <- data.frame(displ = mean(mpg$displ), cty = mean(mpg$cty))
p +
  facet_grid(cols = vars(cyl)) +
  geom_point(data = df, colour = "red", size = 2)

# Free scales -------------------------------------------------------
# You can also choose whether the scales should be constant
# across all panels (the default), or whether they should be allowed
# to vary
mt <- ggplot(mtcars, aes(mpg, wt, colour = factor(cyl))) +
  geom_point()

mt + facet_grid(. ~ cyl, scales = "free")

# If scales and space are free, then the mapping between position
# and values in the data will be the same across all panels. This
# is particularly useful for categorical axes
ggplot(mpg, aes(drv, model)) +
  geom_point() +
  facet_grid(manufacturer ~ ., scales = "free", space = "free") +
  theme(strip.text.y = element_text(angle = 0))

# Margins ----------------------------------------------------------

# Margins can be specified logically (all yes or all no) or for specific
# variables as (character) variable names
mg <- ggplot(mtcars, aes(x = mpg, y = wt)) + geom_point()
mg + facet_grid(vs + am ~ gear, margins = TRUE)
mg + facet_grid(vs + am ~ gear, margins = "am")
# when margins are made over "vs", since the facets for "am" vary
# within the values of "vs", the marginal facet for "vs" is also
# a margin over "am".
mg + facet_grid(vs + am ~ gear, margins = "vs")
```

A complicated strip plot created for the microbiome screening paper
<http://www.sthda.com/english/wiki/ggplot2-essentials>
<http://www.sthda.com/english/wiki/ggplot2-stripchart-jitter-quick-start-guide-r-software-and-data-visualization>
<http://www.sthda.com/english/wiki/ggplot2-point-shapes>
<http://www.sthda.com/english/wiki/ggplot2-title-main-axis-and-legend-titles>
<http://www.sthda.com/english/wiki/ggplot2-legend-easy-steps-to-change-the-position-and-the-appearance-of-a-graph-legend-in-r-software>
<http://www.sthda.com/english/wiki/ggplot2-axis-scales-and-transformations>
<http://www.sthda.com/english/wiki/ggplot2-themes-and-background-colors-the-3-elements>

```{r complicated strip plot using ggplot2}
colorvec <- brewer.pal(3,"RdBu")
colorvec <- c(colorvec[3], colorvec[1], colorvec[2])

p <- ggplot(assay3, aes(x = Ps.genotype:Invader, y = Invasion.score)) # assay3 is from the microbiome screening paper

# truncated version
(q <- p + ylim(0, 2.5)
	+ geom_boxplot(size = .5, outlier.shape = NA, width = 0.3, aes(fill = assay3$Invader))
	+ geom_jitter(width = 0.05, cex = .8, aes(shape = assay3$Inv.strain_shape))
	+ scale_shape_manual(values = c(16, 2))
	+ labs(y = expression("Growth score:  Mean colony area " ~ (cm^{2})))
	# + scale_fill_brewer(type = "qual", palette = "Set3")  # automatic color set, not used
	+ scale_fill_manual(values = colorvec) # hand-chosen color set
  + theme(axis.title.y = element_text(size = 14), # hand-coded theme
  				axis.title.x = element_blank(),
  				legend.position='none',
  				panel.background = element_rect(fill = "white"),
  				panel.grid.major.y = element_line(colour = "grey", size = 0.5),
  				panel.grid.minor.y = element_line(colour = "grey", size = 0.1),
  				panel.border = element_rect(colour = "black", fill = NA)
  				)
	)

```

```{r viridis}
library(ggplot2)
library(viridis)

# scale_color_viridis(): Change the color of points, lines and texts
# scale_fill_viridis(): Change the fill color of areas (box plot, bar plot, etc)

# ripped from the pages of ggplot2
p <- ggplot(mtcars, aes(wt, mpg))
p + geom_point(size=4, aes(colour = factor(cyl))) +
    scale_color_viridis(discrete=TRUE) +
    theme_bw()

# ripped from the pages of ggplot2
dsub <- subset(diamonds, x > 5 & x < 6 & y > 5 & y < 6)
dsub$diff <- with(dsub, sqrt(abs(x-y))* sign(x-y))
d <- ggplot(dsub, aes(x, y, colour=diff)) + geom_point()
d + scale_color_viridis() + theme_bw()

ggplot(data.frame(x = rnorm(10000), y = rnorm(10000)), aes(x = x, y = y)) +
  geom_hex() + coord_fixed() +
  scale_fill_viridis() + theme_bw()


# from the main viridis example
dat <- data.frame(x = rnorm(10000), y = rnorm(10000))

ggplot(dat, aes(x = x, y = y)) +
  geom_hex() + coord_fixed() +
  scale_fill_viridis() + theme_bw()

library(ggplot2)
library(MASS)
library(gridExtra)

data("geyser", package="MASS")

ggplot(geyser, aes(x = duration, y = waiting)) +
  xlim(0.5, 6) + ylim(40, 110) +
  stat_density2d(aes(fill = ..level..), geom="polygon") +
  theme_bw() +
  theme(panel.grid=element_blank()) -> gg

grid.arrange(
  gg + scale_fill_viridis(option="A") + labs(x="Virdis A", y=NULL),
  gg + scale_fill_viridis(option="B") + labs(x="Virdis B", y=NULL),
  gg + scale_fill_viridis(option="C") + labs(x="Virdis C", y=NULL),
  gg + scale_fill_viridis(option="D") + labs(x="Virdis D", y=NULL),
  gg + scale_fill_viridis(option="E") + labs(x="Virdis E", y=NULL),
  ncol=3, nrow=2
)

```

ggplot2 use pronouns
<https://stackoverflow.com/questions/45088454/how-do-i-access-the-data-frame-that-has-been-passed-to-ggplot/45088522>

```{r}
mtcars %>% {
  ggplot(., aes(mpg, hp)) + 
  labs(title = paste("N =", nrow(.))) + 
  geom_point()
}
# Note that when wrapping the whole ggplot call in {...} curly braces, you must use the . dot pronoun for the data argument in ggplot(., ...). Then you can call back that object using the . pronoun anywhere in the call.
```

loops (use lapply or map instead of loops, but here is a snippet about
how to set the loop length)

```{r}
df <- data.frame(
a = rnorm(10),
b = rnorm(10),
c = rnorm(10),
d = rnorm(10)
 )
# Replace the 1:ncol(df) sequence
for (i in seq_along(df)) {
  print(median(df[[i]]))
}
# Change the value of df
df <- data.frame()
# Repeat for loop to verify there is no error
for (i in seq_along(df)) {
  print(median(df[[i]]))
}

# If use ncol(df), it gives an error
df <- data.frame()
# Repeat for loop to verify there is no error
for (i in ncol(df)) {
  print(median(df[[i]]))
}
```

lists, lapply, sapply

```{r lists lapply sapply}
adh <- list( chr =" 2L", start = 14615555L, end = 14618902L, name =" Adh")
is.list(adh)
adh
adh[1]
adh$chr
adh[[1]]


ll <- list( a = rnorm( 6, mean = 1), b = rnorm( 6, mean = 4), c = rnorm( 6, mean = 6))
ll # three vectors in this list.
# if i want to calculate the means of these three vectors, i could write a loop, but that takes effort
# use lapply
lapply(ll, mean)  # apply mean to each of the elements in the list and return as a list (thus, the in l in lapply)
# apply lapply in parallel
library(parallel)
mclapply(ll, mean) # multicore lapply

# lapply passes the first argument to the function
lapply(ll, mean) # ll's vectors are passed to mean
# if I want to pass additional arguments, i
# e.g.
ll$a[3] <- NA
lapply(ll, mean)  # because one of the elements is NA.
lapply(ll, mean, na.rm=TRUE)
# or write a function and pass that
meanRemoveNA <- function(x) {
    mean(x, na.rm = TRUE)
}

meanRemoveNA(ll$a)
lapply(ll, meanRemoveNA)

meanRemoveNAVerbose <- function(x, warn = TRUE) {
    if (any(is.na(x)) && warn) {
        warning("removing some NA values")
    }
    mean(x, na.rm = TRUE)
}

lapply(ll, meanRemoveNAVerbose)


# sapply, same as lapply but simplifies results into vectors, matrices, or arrays
lapply(ll, meanRemoveNA)
out <- sapply(ll, meanRemoveNA)
out[2] # named variables

# mapply is a multivariate version of lapply, can take multiple arguments

ind_1 <- list(loci_1 = c("T", "T"), loci_2 = c("T", "G"), loci_3 = c("C", "G"))
ind_2 <- list(loci_1 = c("A", "A"), loci_2 = c("G", "G"), loci_3 = c("C", "G"))

mapply(function(a, b) length(intersect(a, b)), ind_1, ind_2)  # run intersect(ind_1, ind_2)
mapply(function(a, b) length(intersect(a, b)), ind_1, ind_2, SIMPLIFY = FALSE)  # SIMPLIFY = FALSE stops some of the simplification of output, if needed
intersect(ind_1$loci_1, ind_2$loci_1)
intersect(ind_1$loci_2, ind_2$loci_2)
intersect(ind_1$loci_3, ind_2$loci_3)

```

split-apply-combine. this is the original version of group\_by %\>%
function in dplyr. i won't go through the base R version of this:
split() then lapply() then unsplit using rbind(). do all at once with
do.call()

```{r split apply combine}
do.call(rbind, lapply(split(d$depth, d$GC.binned), summary))
# calculate summaries for each level of GC.binned and then combine the summaries using rbind()

# tapply is a convenient version of this.  calculate mean of depth for each level of GC.binned
tapply(d$depth, d$GC.binned, mean)  # tapply(input vector, binning vector, function)

library(dplyr)
mtfs_df <- tbl_df(mtfs)
mtfs_df %>% group_by(chr)
mtfs_df %>% 
    group_by(chr) %>% 
    summarise(max_recom = max(recom), 
              mean_recom = mean(recom), 
              num = n()
              ) %>%
    arrange(desc(max_recom))
```

working with strings

<https://stringr.tidyverse.org>
<https://stringr.tidyverse.org/articles/regular-expressions.html> \#
regex in R (slightly different from regex elsewhere) String operations:
add or remove a string from a string in column

```{r}
# remove substring (and the substring is a regex) from a string inside a column (use mutate_at)
fastq_read_counts_PlatesEF <- fastq_read_counts_PlatesEF %>%
    tidyr::separate(file, into = c(NA, "gpfs", "home", "b042", "greenland", "plates", "platessub", "fastqc", "BWA", "sample", "fastqfile"), sep = "/", remove = TRUE) %>%
    mutate_at("sample", str_remove, "_[ACGT]{9}-[ACGT]{6}") %>% # remove index sequence from end of string, e.g. _TAGGTTAGG-GCCAAT
    dplyr::select(sample, tot_num_seqs = num_seqs) %>%
    dplyr::group_by(sample) %>%
    summarise(tot_num_seqs = sum(tot_num_seqs))

# remove ME substring and convert input_amount type to numeric
# ME100 to 100, and convert to numeric
mocks_idx_meta_genomecov_test <- mocks_idx_meta_genomecov_test %>%
    mutate_at("input_amount", str_remove, "M[E,G]") %>% # remove ME or MG
    hablar::convert(num(input_amount)) # hablar package

# remove "BOLD:etc.
mockdf2 <- mockdf %>% 
    mutate_at("Species_BOLD", str_remove, "_BOLD:\\w+") %>% 
    filter(sample == "PlateGH_ME100") %>% 
    unite(species, c("Genus", "Species_BOLD")) %>% 
    mutate_at("species", str_replace, "_", " ") %>% 
    select(species)
# write_csv(mockdf2, "mock_species.csv")


# concatenate a substring to an existing string in a column
fastq_read_counts_PlatesAB2$sample <- str_c("Plate", fastq_read_counts_PlatesAB2$sample)
str_c("Plate", fastq_read_counts_PlatesAB2$sample)

# str_remove multiple patterns. in the last mutate(), i want to remove either "body" or "leg" from a string. 
df_env <- df_env %>% 
    mutate(
        bodypart = case_when(
            grepl("body", sample) == TRUE ~ "body",
            grepl("leg", sample) == TRUE ~ "leg"
        )
    ) %>%
    mutate(
        bodypartcolor = if_else(bodypart == "body", 1, 2)
    ) %>% 
    mutate(
        evenness = str_remove(sample, "body|leg")
        )


# another example
# If every row doesn't split into the same number of pieces, use
# the extra and fill arguments to control what happens
df <- data.frame(x = c("a", "a b", "a b c", NA))
df <- df %>% separate(x, c("a", "b"), remove = FALSE)
# The same behaviour drops the c but no warnings
df <- df %>% separate(x, c("a", "b"), extra = "drop", fill = "right")
# Another option:
df <- df %>% separate(x, c("a", "b"), extra = "merge", fill = "left")
# Or you can keep all three
df <- df %>% separate(x, c("a", "b", "c"))

```

```{r working with strings}
nchar(c("AGCTAG", "ATA", "GATCTGAG", ""))
# grep(pattern, x)

re_sites <- c(" CTGCAG", "CGATCG", "CAGCTG", "CCCACA")
grep("CAG", re_sites) # returns positions
grep("CT[CG]", re_sites)

chrs <- c("chrom6", "chr2", "chr6", "chr4", "chr1", "chr16", "chrom8")

grep("[^\\d]6", chrs, perl = TRUE) # perl=TRUE allows PCRE syntax, returns positions of matches

# [^\\d] = any non-numeric character (thus omits, e.g. 16).  \\ because first backslash escapes the R interpretation of \, so \\d in R == \d

chrs[grep("[^\\d]6", chrs, perl = TRUE)]   #[\\d] = any non-numeric character (thus omits, e.g. 16).  \\ because first backslash escapes the R interpretation of \, so \\d in R == \d

regexpr("[^\\d]6", chrs, perl = TRUE) # also returns the length of the matching string.  -1 means no match

# to use this to extract the matches
pos <- regexpr("\\d+", chrs, perl = TRUE) # returns only the non-numeric characters

# substitute replacement for pattern in x
# sub(pattern, replacement, x)
sub(pattern ="Watson", replacement ="Watson, Franklin,", x =" Watson and Crick discovered DNA's structure.")

sub("gene=(\\w+)", "\\1", "gene=LEAFY", perl = TRUE)  # substitute the part after the = sign with just the part after = sign.  so gene=LEAFY is replaced by LEAFY.  \w is alphanumeric characters and underscores

sub(">[^ ]+ *(.*)", "\\1", ">1 length = 301354135 type = dna")  # match all text after the first space

sub(" *[chrom]+(\\d+|X|Y|M) *", "chr\\1", c("chr19", "chrY"), perl = TRUE)


# paste together multiple strings
paste("chr", c(1:22, "X", "Y"), sep ="")


# strsplit
leafy <- "gene = LEAFY; locus = 2159208; gene_model = AT5G61850.1"
leafylist <- strsplit(leafy, split = ";") # split leafy into 3 pieces

# ifelse
# ifelse(test, yes, no)
x <- c(-3, 1, -5, 2)
ifelse(x < 0, -1, 1)

sessionInfo()


# programmatic file loading
hs_files <- list.files("hotspots", pattern ="hotspots.*\\.bed", full.names = TRUE)  # list all hostspots*.bed in directory hotspots.  NB two backslashes \\ to escape the . before bed
hs_files
bedcols <- c("chr", "start", "end")
loadFile <- function(x) read.delim(x, header=FALSE, col.names = bedcols)
hs <- lapply(hs_files, loadFile) # laod all the files listed in hs_files into one list:  hs
names(hs) <- list.files("hotspots", pattern ="hotspots.*\\.bed")
hsd <- do.call(rbind, hs)
rownames(hsd) <- NULL


loadFile <- function(x) {
    # read in a BED file, extract the chromosome name from the file,
    # and add it as a column
    df <- read.delim(x, header = FALSE, col.names = bedcols)
    df$chr_name <- sub("hotspots_([^\\.] +)\\.bed", "\\1", basename(x))
    df$file <- x
    df
    }
hs <- lapply(hs_files, loadFile)
head(hs[[1]])


# load and summarise files one by one, if i can't load all into memory
#
loadAndSummarizeFile <- function(x) {
    df <- read.table(x, header = FALSE, col.names = bedcols)
    data.frame(chr = unique(df$chr), n=nrow(df), mean_len = mean(df$end - df$start))
    }

library(parallel)
hs <- mclapply(hs_files, loadAndSummarizeFile)
hs[[1]]
hsd <- do.call(rbind, hs)
hsd

# extract first letter of a string
substr(samplemetadata$ArcDyn_Plate_name, 1, 1) # extract first character
# Anteater becomes A, Bushdog becomes B, etc.


# stringr::str_match
strings <- c(" 219 733 8965", "329-293-8753 ", "banana", "595 794 7569",
  "387 287 6718", "apple", "233.398.9187  ", "482 952 3315",
  "239 923 8115 and 842 566 4692", "Work: 579-499-7527", "$1000",
  "Home: 543.355.3679")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"

str_extract(strings, phone)
str_match(strings, phone)

# string::str_replace;  replace matched patterns in a string (including removing substrings from a string)
fruits <- c("one apple", "two pears", "three bananas")
str_replace(fruits, "[aeiou]", "-") # replace first vowel with a dash
str_replace_all(fruits, "[aeiou]", "-") # replace all vowels with a dash
str_replace_all(fruits, "[aeiou]", toupper) # replace all vowels with capital versions of themselves
str_replace_all(fruits, "b", NA_character_) # replace any string containing a "b" with NA

str_replace(idx_meta_genomecov_ABA2B2EFGH$EI_RUN, "idx_meta_genomecov_", "") # to extract AB, A2B2, EF, GH from idx_meta_genomecov_AB, idx_meta_genomecov_A2B2, idx_meta_genomecov_EF, idx_meta_genomecov_GH

library(tidyverse)
shopping_list <- c("apples x4", "bag of flour", "bag of sugar", "milk x2")
str_extract(shopping_list, "\\d")
str_extract(shopping_list, "[a-z]+")
str_extract(shopping_list, "[a-z]{1,4}")
str_extract(shopping_list, "\\b[a-z]{1,4}\\b")

# Extract all matches
str_extract_all(shopping_list, "[a-z]+")
str_extract_all(shopping_list, "\\b[a-z]+\\b")
str_extract_all(shopping_list, "\\d")

# Simplify results into character matrix
str_extract_all(shopping_list, "\\b[a-z]+\\b", simplify = TRUE)
str_extract_all(shopping_list, "\\d", simplify = TRUE)

# Extract all words
str_extract_all("This is, suprisingly, a sentence.", boundary("word"))

# extract substrings from multiple columns and concatenate
otus.test$Taxonomy_v_1.0_short <- str_c(str_sub(otus.test$Family, 1, 4), str_sub(otus.test$Genus, 1, 4), str_sub(otus.test$Species, 1, 4), sep = "_")

# use str_extract and str_c in a pipe with mutate
species <- idx_meta_genomecov_ABA2B2EFGH %>%
    dplyr::select(mitogenome, COI_Species, COI_length = mt_length) %>%
    dplyr::filter(COI_Species != "COI_Spike") %>% # remove COI_spike species
    dplyr::select(-COI_Species) %>% # remove column indicating COI_spike species
    distinct(mitogenome, .keep_all = TRUE) %>% # keep only distinct values of mitogenome, should return 406
    mutate(BOLD = mitogenome) %>% # copy mitogenome into new column called BOLD
    mutate(BOLD = str_extract(BOLD, "[A-Z]{3}[0-9]{4}$")) %>% # extract only the BOLD number from the mitogenome name (here, the mitogenome name is really the Barcode name:  Araneae_Linyphiidae_Hilaira_vexatrix_BOLD:AAG5689)
    mutate(BOLD = str_c("BOLD:", BOLD)) # add "BOLD:" as prefix. Have to do it this way because some of the BOLD numbers are not prefixed by "BOLD:" Hymenoptera_Braconidae_Dolichogenidea_cf_sicaria_AAA3918

```

```{r paste together a complex string using str_c and str_pad}
# create variable from Date (e.g. 1997-07-01) and Trap (A, B, C)
# stringr::str_pad(x, width = 2, side = "left", pad = "0")  # to ensure that the date always has two digits, e.g. 07.
idx_meta_genomecov$DateTrap <- str_c("Date", year(idx_meta_genomecov$Date), str_pad(month(idx_meta_genomecov$Date), 2, side = "left", pad = "0"), str_pad(day(idx_meta_genomecov$Date), 2, side = "left", pad = "0"), idx_meta_genomecov$Trap, sep = "_")
```

```{r write.table options}
# write.table with better defaults
# don't write rownames and don't use quotes
write.table(mtfs, file = "hotspots_motifs.txt", quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)

# to write a gzipped file, use gzfile
write.table(mtfs, file = gzfile("hotspots_motifs.txt.gz"), quote = FALSE, sep = "\t", row.names = FALSE, col.names = TRUE)
```

writing a function

```{r programming tools in R}
# breakpoints
foo <- function(x) {
    browser()  # inspects which functions call other functions
    a <- 2
    y <- x + a
    return( y)
    }

foo(2)  # opens the code browser, and i can then step through and watch the values of variables change

# drop me into a debugging session
options(error=recover)  # if there's an error, i get dropped into a debugging session
bar <- function(x) x + "1"
bar(2)  # opens a debugging session
options(error=NULL)

# a function with two outputs, put the outputs into a list
# https://stackoverflow.com/questions/1826519/how-to-assign-from-a-function-which-returns-more-than-one-value

func2<-function(input) {
   a<-input+1
   b<-input+2
   output<-list(a,b)
   return(output)
}

output<-func2(5)

# see outputs
for (i in output) {
   print(i)
}
# [1] 6
# [1] 7
```

tidyr vignette Nested data

```{r tidyr unnest}
library(tidyr)
library(dplyr)
library(purrr)

df1 <- tibble(
  g = c(1, 2, 3),
  data = list(
    tibble(x = 1, y = 2),
    tibble(x = 4:5, y = 6:7),
    tibble(x = 10)
  )
)

df1

df2 <- tribble(
  ~g, ~x, ~y,
   1,  1,  2,
   2,  4,  6,
   2,  5,  7,
   3, 10,  NA
)
df2 %>% nest(data = c(x, y))

df2 %>% group_by(g) %>% nest()
df1 %>% unnest(data)

# Nested data is a great fit for problems where you have one of _something_ for each group. A common place this arises is when you’re fitting multiple models.

mtcars_nested <- mtcars %>% 
  group_by(cyl) %>% 
  nest()

mtcars_nested

# Once you have a list of data frames, it’s very natural to produce a list of models:
mtcars_nested <- mtcars_nested %>% 
  mutate(model = map(data, function(df) lm(mpg ~ wt, data = df)))
mtcars_nested

#And then you could even produce a list of predictions:
mtcars_nested <- mtcars_nested %>% 
  mutate(model = map(model, predict))
mtcars_nested  


```

```{r tidyr}
df <- tibble(
    x = 1:3,
    y = c("a", "d,e,f", "g,h")
)

df

df %>%
    transform(y = strsplit(y, ","))

df %>%
    transform(y = strsplit(y, ",")) %>%
    unnest(y)

# Or just
df %>%
    unnest(y = strsplit(y, ","))

# It also works if you have a column that contains other data frames!
df <- tibble(
    x = 1:2,
    y = list(
        tibble(z = 1),
        tibble(z = 3:4)
    )
)

df

df %>% unnest(y)

# You can also unnest multiple columns simultaneously
df <- tibble(
    a = list(c("a", "b"), "c"),
    b = list(1:2, 3),
    c = c(11, 22)
)

df

df %>% unnest(a, b)
# If you omit the column names, it'll unnest all list-cols
df %>% unnest()

# Nest and unnest are inverses
df <- data.frame(x = c(1, 1, 2), y = 3:1)
df
df %>% nest(y)
df %>% nest(y) %>% unnest()

# If you have a named list-column, you may want to supply .id
df <- tibble(
    x = 1:2,
    y = list(a = 1, b = 3:4)
)
unnest(df, .id = "name")
```

tidyr::separate\_rows. If a variable contains observations with multiple
delimited values, this separates the values and places each one in its
own row.

```{r tidyr separate rows}
df <- data.frame(
  x = 1:3,
  y = c("a", "d,e,f", "g,h"),
  z = c("1", "2,3,4", "5,6"),
  stringsAsFactors = FALSE
)
df
separate_rows(df, y, z, convert = TRUE)

# tidyr::unite
df <- expand_grid(x = c("a", NA), y = c("b", NA))
df

df %>% unite("z", x:y, remove = FALSE)
# To remove missing values:
df %>% unite("z", x:y, na.rm = TRUE, remove = FALSE)

# Separate is almost the complement of unite
df %>%
  unite("xy", x:y) %>%
  separate(xy, c("x", "y"))
# (but note `x` and `y` contain now "NA" not NA)
```

tidyr::gather. Gather takes multiple columns and collapses into
key-value pairs, duplicating all other columns as needed. You use
gather() when you notice that you have columns that are not variables.

```{r tidyr gather}
# From http://stackoverflow.com/questions/1181060
stocks <- tibble(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)

stocks
# X, Y, Z are column headings, but they are also "stock names", so we want X,Y,Z to be fields in a new column (the key) called "stock", and the values should be the values under X, Y, Z, which will be put in a new column called "price".  We don't want to gather column time, so we -time
gather(stocks, stock, price, -time)
    # gather(data, key = "key", value = "value", ..., na.rm = FALSE, convert = FALSE, factor_key = FALSE)
    # thus, key and value are new names for the new column headings
stocks %>% gather(stock, price, -time)

# get first observation for each Species in iris data -- base R
mini_iris <- iris[c(1, 51, 101), ]; mini_iris
# gather Sepal.Length, Sepal.Width, Petal.Length, Petal.Width
gather(mini_iris, key = flower_att, value = measurement,
       Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) # list the columns that i want to be gathered
# same result but less verbose
gather(mini_iris, key = flower_att, value = measurement, -Species) # list the columns that i don't want to be gathered, and the other ones are used by default

# repeat iris example using dplyr and the pipe operator
library(dplyr)
mini_iris <-
  iris %>%
  group_by(Species) %>%
  slice(1) # take the first row of each Species group
mini_iris %>% gather(key = flower_att, value = measurement, -Species)

```

tidyr::spread. Spread a key-value pair across multiple columns

```{r tidyr spread}
library(dplyr)
stocks <- data.frame(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)

stocksm <- stocks %>% gather(stock, price, -time); stocksm
stocksm %>% spread(stock, price)
    # spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE, sep = NULL)
    # key = stock, value = price. key is the column that will be spread across column headings.
    # value is the column that will fill in the
stocksm %>% spread(time, price)

# Spread and gather are complements
df <- data.frame(x = c("a", "b"), y = c(3, 4), z = c(5, 6))
df %>% spread(x, y) %>% gather(x, y, a:b, na.rm = TRUE)

# Use 'convert = TRUE' to produce variables of mixed type
df <- data.frame(row = rep(c(1, 51), each = 3),
                 var = c("Sepal.Length", "Species", "Species_num"),
                 value = c(5.1, "setosa", 1, 7.0, "versicolor", 2))
df %>% spread(var, value) %>% str
df %>% spread(var, value, convert = TRUE) %>% str

```

Bioconductor - <http://bioconductor.org/install/> \# installation
instructions

```{r Bioconductor}
install.packages("BiocManager")
library(BiocManager)
BiocManager::install(c("GenomicRanges", "Biobase", "IRanges", "AnnotationDbi", "dada2", "phyloseq", "GenomicAlignments", "Organism.dplyr")) # install Bioconductor packages
BiocManager::install() # update installed Bioconductor packages
BiocManager::version() # version of Bioconductor in use
BiocManager::valid() # identify packages that are out of date or unexpected versions. Creates a command for updating out of date bioconductor packages
v <- BiocManager::valid()
names(v)
avail <- BiocManager::available()
length(avail)
BiocManager::available("BSgenome.Hsapiens")

## biocLite.R is deprecated
# source("http://bioconductor.org/biocLite.R") # to install BiocInstaller # run this a lot
# library(BiocInstaller)
# biocLite() # install core Bioconductor packages
# biocLite(pkgs = c("GenomicRanges", "Biobase", "IRanges", "AnnotationDbi", "dada2", "phyloseq"), lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
# biocValid()  # to check on validity and cross-compatibility of bioconductor packages
# biocLite("BiocUpgrade") # to use latest version of Bioconductor for my version of R
# biocLite("phyloseq") # install phyloseq
```

IRanges

```{r IRanges}
library(IRanges)
rng <- IRanges(start = 4, end = 13) # 1-based range system (closed intervals, starting at 1). in other words, positions 4 and 13 are included, and the width of the range is 13-4+1 = 10
rng
# IRanges object with 1 range and 0 metadata columns:
#           start       end     width
#       <integer> <integer> <integer>
#   [1]         4        13        10

rng <- IRanges(end = 5, width = 5) # width instead of end
rng
# IRanges object with 1 range and 0 metadata columns:
#           start       end     width
#       <integer> <integer> <integer>
#   [1]         1         5         5

x <- IRanges(start = c(4, 7, 2, 20), end = c(13, 7, 5, 23))
names(x) <- letters[1:4] # name range objects
x
str(x)
end(x) <- end(x) + 4
x
range(x)
x[2:3] # subset the ranges object
start(x)
start(x) < 5
# start(x) <- 5 # input 5 into all starts
x
x[start(x) < 5] # subset by starts
x[width(x) > 8] # subset by widths
x['a']

a <-IRanges ( start = 7 , width = 4 )
b <-IRanges ( start = 2 , end = 5 )
c(a, b)

# mathematical operations
x <- IRanges(start = c(40, 80), end = c(67, 114))
x
x + 4L # extend ranges by 4 in both directions (4L sets 4 to be an integer, which uses less mem space).  shorter than equivalent as.integer(4)
x - 10L # shorten ranges by 10 at both ends

y <- IRanges(start = c(4, 6, 10, 12), width = 13)
y
restrict(y, 5, 10) # restrict each range to be from 5 to 10 max. if already smaller, then the range is smaller

x <- IRanges(start = c(40, 80), end = c(67, 114))
x
flank(x, width = 7) # new *upstream* range that flanks the existing range
flank(x, width = 7, start = FALSE) # new downstream range

# reduce() merges overlapping ranges
# create a bunch of random overlapping ranges
set.seed ( 0 ) # set the random number generator seed
sample(seq_len ( 50 ), 20) # sample 20 numbers from a vector that goes from 1-50
alns <-IRanges ( start = sample ( seq_len ( 50 ), 20 ), width = 5 ) # NB highest possible start position is 50, so highest possible end position is 50+5=55
head ( alns , 4 )
reduce(alns)
# gaps finds the gaps between all the ranges
gaps(alns)
gaps(alns, start=1, end=60) # to set start and end positions of the whole sequence

# set operations. create two overlapping range
a <-IRanges ( start = 4 , end = 13 )
b <-IRanges ( start = 12 , end = 17 )
intersect ( a , b ) # they overlap in one position
setdiff ( a , b ) # the part of range a that is not in range b
setdiff ( b , a ) # the part of range b that is not in range a
union ( b , a ) # IRanges of length 1 start end width [1] 4 17 14
union ( a , b ) # IRanges of length 1 start end width [1] 4 17 14
help(psetdiff)
psetdiff(a, b)

# find overlaps between ranges
# create two sets of ranges, query and subject
qry <-IRanges ( start = c ( 1 , 26 , 19 , 11 , 21 , 7 ), end = c ( 16 , 30 , 19 , 15 , 24 , 8 ), names=letters[1:6])
sbj <-IRanges ( start = c ( 1 , 19 , 10 ), end = c ( 5 , 29 , 16 ), names = letters [ 24 : 26 ])
qry
sbj

hts <- findOverlaps(qry, sbj) # list of which query ranges overlap with which subject ranges.
hts

names ( qry )[ queryHits ( hts )] # [1] "a" "a" "b" "c" "d" "e"
names ( sbj )[ subjectHits ( hts )] # [1] "x" "z" "y" "y" "z" "y"

hts <- findOverlaps(qry, sbj, type = "within") # list of which query ranges overlap ENTIRELY with which subject ranges.
hts

hts <- findOverlaps(qry, sbj, select = "first") # list of which query ranges overlap with which subject ranges, reporting only the first
hts # these list the subject ranges overlapped by qry ranges 1,2,3,4,5,6 in that order
hts <- findOverlaps(qry, sbj, select = "all") # default behaviour
hts


# create Nested Containment List object to speed up overlap calculations
sbj_it <- NCList(sbj)
sbj_it
class(sbj_it)
hts <- findOverlaps(qry, sbj_it)
hts

# accessor functions
as.matrix(hts)
countQueryHits(hts) # how many hits for each query range
setNames(countQueryHits(hts), names(qry)) # set the names of the queries
countSubjectHits(hts) # count hits per subject range
setNames(countSubjectHits(hts), names(sbj))
ranges(qry)
countOverlaps(qry, sbj) # same as setNames(countQueryHits(hts), names(qry))
subsetByOverlaps(qry, sbj)

# finding nearest ranges and calculating distance
qry <-IRanges ( start = 6 , end = 13 , name = 'query' )
sbj <-IRanges ( start = c ( 2 , 4 , 18 , 19 ), end = c ( 4 , 5 , 21 , 24 ), names = 1 : 4 )
qry
sbj
nearest(qry, sbj) # nearest sbj range to qry, including overlaps
precede(qry, sbj) # find ranges that the query precedes
follow(qry, sbj) # find ranges that the query follows
qry2 <- IRanges(start = c(6,7), width = 3)
nearest(qry2, sbj) # vectorized

# create some random ranges
qry <-IRanges ( sample ( seq_len ( 1000 ), 5 ), width = 10 )
sbj <-IRanges ( sample ( seq_len ( 1000 ), 5 ), width = 10 )
qry
sbj
distanceToNearest(qry, sbj) # distance of each qry to the nearest sbj
distance(qry, sbj) # distance of the first range in qry to the first range in sbj, second to second, third to third, and so on

# run-length encoding to save storage
x <-as.integer ( c ( 4 , 4 , 4 , 3 , 3 , 2 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0, 0, 0, 0, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4))
xrle <-Rle ( x )
xrle
as.vector(xrle)
xrle + 4L
as.vector(xrle+4L)
xrle > 3 # runs > 3
xrle[xrle > 3]
sum(xrle)
summary(xrle)

# coverage
rngs <-IRanges ( start = sample ( seq_len (60), 10 ), width = 7 ) # note this is a random sample
names(rngs)[9] <- "A" # label one range
rngs
rngs_cov <- coverage(rngs) # coverage of a set of ranges, returned in run-length encoding
rngs_cov
rngs_cov > 3 # where is coverage > 3? answer: position 34 and 36 in this dataset
# logical-Rle of length 66 with 5 runs
#   Lengths:    33     1     1     1    30
#   Values : FALSE  TRUE FALSE  TRUE FALSE
rngs_cov[as.vector(rngs_cov) > 3]
rngs_cov[rngs['A']] # coverage in the range labelled A
mean(rngs_cov[rngs['A']])

# extract ranges that have some minimal or maximal coverage
min_cov2 <- slice(rngs_cov, lower = 2) # ranges where the coverage ≥ 2
min_cov2
ranges(min_cov2)
viewMeans(min_cov2) # mean coverages for each of the extracted ranges
viewMaxs(min_cov2) # max coverages within each of the extracted ranges
viewApply(min_cov2, median)

# create a moving 5-bp window
length(rngs_cov)
bwidth <- 5L
end <-bwidth * floor ( length ( rngs_cov ) / bwidth ) # calculate an end value that is an integer and is a multiple of bwidth
windows <-IRanges ( start = seq ( 1 , end , bwidth ) , width = bwidth )
head(windows)
cov_by_wnd <-Views( rngs_cov , windows )  # View coverage by 5-bp window
head(cov_by_wnd)
viewMeans(cov_by_wnd)

# GenomicRanges extends IRanges by adding sequence name and strand information to IRanges' range information
library ( GenomicRanges )
gr <-GRanges ( seqname = c ( "chr1" , "chr1" , "chr2" , "chr3" ), ranges=IRanges(start=5:8, width=10), strand=c("+", "-", "-", "+"))
gr
# add metadata
gr <-GRanges ( seqname = c ( "chr1" , "chr1" , "chr2" , "chr3" ),
               ranges=IRanges(start=5:8, width=10),
               strand=c("+", "-", "-", "+"), gc=round(runif(4), 3)
               )
gr
# add genome seq lengths
seqlens <-c ( chr1 = 152 , chr2 = 432 , chr3 = 903 )
gr <-GRanges ( seqname = c ( "chr1" , "chr1" , "chr2" , "chr3" ),
               ranges=IRanges(start=5:8, width=10),
               strand=c("+", "-", "-", "+"),
               gc=round(runif(4), 3),
               seqlengths=seqlens)
# seqlengths(gr) <- seqlens # alternative to seqlengths=seqlens
gr # doesn't show sequence lens of the chromosomes, but the information is in the object

start(gr) # starts of the ranges
end(gr) # ends of the ranges
seqnames(gr)
strand(gr)
ranges(gr)
length(gr)
start(gr) > 7
gr[start(gr) > 7]
table(seqnames(gr))
gr[seqnames(gr) == "chr1"]
mcols(gr)
mcols(gr)$gc
gr$gc
mean(mcols(gr[seqnames(gr) == "chr1"])$gc) # calculate mean of gc of sequences with name "chr1"

# GenomicRanges lists
# can make a list of genomic ranges
gr1 <-GRanges ( c ( "chr1" , "chr2" ), IRanges ( start = c ( 32 , 95 ), width = c ( 24 , 123 )))
gr2 <-GRanges ( c ( "chr8" , "chr2" ), IRanges ( start = c ( 27 , 12 ), width = c ( 42 , 34 )))
grl <-GRangesList ( gr1 , gr2 )
grl
unlist(grl)
doubled_grl <- list(grl, grl)
doubled_grl
doubled_grl[2]
seqnames(grl)
```

```{r}
# genomicFeatures
library(GenomicFeatures)
# BiocManager::install("TxDb.Mmusculus.UCSC.mm10.ensGene") # install transcriptome annotation data, which is downloaded
library(TxDb.Mmusculus.UCSC.mm10.ensGene)
txdb <- TxDb.Mmusculus.UCSC.mm10.ensGene # short alias to make it easier to refer to
txdb # contains an sqlite database
mm_genes <- genes(txdb) # pull out all genes from txdb
head(mm_genes) # list of the genes and their chromosomes, ranges, strands, and metadata
length(mm_genes) # 39017 genes
exons(txdb)
exons_by_tx <- exonsBy(txdb, by = "tx") # exons grouped by transcript
exons_by_tx
exons_by_gn <- exonsBy(txdb, by = "gene") # exons grouped by gene
exons_by_gn

# GenomicFeatures also provides functions for extracting subsets of features that overlap a specific chromosome or range. We can limit our queries to use a subset of chromosomes by setting which sequences our transcriptDb should query using the following approach:

seqlevels(txdb) <- "chr1"
seqlevels(txdb) # shows only chr1
chr1_exons <-exonsBy ( txdb , "tx" )
chr1_exons # shows exons by transcript but only for chr1
all ( unlist ( seqnames ( chr1_exons )) == "chr1" )  # shows that chr1 is the only chromosome
txdb <- restoreSeqlevels(txdb) # restore all chromosome levels

# To extract feature data that only overlaps a specific region, use the following family of functions: transcriptsByOverlaps() , exonsByOverlaps() , and cdsByOverlaps() (see help(transcriptByOverlaps() for more information). For example, say a QTL study has identified a quantitative trait loci in the region roughly on chromosome 8, from 123,260,562 to 123,557,264. Our coordinates are rough, so we’ll add 10kbp. Recall that with IRanges , we grow ranges by a fixed number of bases by adding that number of bases to the object (from “Basic Range Operations: Arithmetic, Transformations, and Set Operations ” ); the same method is used to resize GRanges objects. So we can get all genes within this expanded region with:

qtl_region <-GRanges ( "chr8" , IRanges ( 123260562 , 123557264 ))
qtl_region_expanded <-qtl_region + 10e3
transcriptsByOverlaps(txdb, qtl_region_expanded)

```

rtracklayer imports and exports annotation data in a wide variety of
formats

```{r}
library(rtracklayer)
setwd("/Users/Negorashi2011/src/bds-files/chapter-09-working-with-range-data")
mm_gtf <-import ('Mus_musculus.GRCm38.75_chr1.gtf.gz')
mcols ( mm_gtf ) # metadata columns
colnames ( mcols ( mm_gtf )) # column names of the metadata
names ( mcols ( mm_gtf )) # column names of the metadata

mm_gtf

# write 5 random pseudogenes
set.seed ( 0 )
pseudogene_i <- which (mm_gtf$gene_biotype == "pseudogene" & mm_gtf$type == "gene")
pseudogene_i
pseudogene_sample <-sample (pseudogene_i, 5)
export ( mm_gtf [ pseudogene_sample ], con = "five_random_pseudogene.gtf" , format="GTF")
# con means connection from which data is loaded or to which data is saved, e.g. a filename

# write bed format
bed_data <- mm_gtf[pseudogene_sample] # pseudogene_sample is a list of pseudogene numbers
bed_data
mcols(bed_data) <- NULL # clear out metadata columns
bed_data
export(bed_data, con="five_random_pseudogene.bed", format = "BED")

# retrieving promoter regions:  flanks and promoters
table(mm_gtf$gene_biotype) # these are the different gene types
chr1_pcg <- mm_gtf[mm_gtf$type == "gene" & mm_gtf$gene_biotype == "protein_coding"]
chr1_pcg
summary(width(chr1_pcg)) # should have roughly the right seq lengths
length(chr1_pcg) # 1240 genes
chr1_pcg_3kb_up <- promoters(chr1_pcg, upstream = 3000, downstream = 0)
chr1_pcg_3kb_up2 <- flank(chr1_pcg, width = 3000) # default is to flank upstream:  start=TRUE, both=FALSE
identical(chr1_pcg_3kb_up, chr1_pcg_3kb_up2) # TRUE

# get the gene sequences corresponding to the flanking regions
BiocManager::install("BSgenome") # BioStrings
# BiocManager::install("BSgenome.Mmusculus.UCSC.mm10") # 712MB, so don't download


# gaps
gr2 <-GRanges ( c ( "chr1" , "chr2" ), IRanges ( start = c ( 4 , 12 ), width = 6 ), strand=c("+", "-"), seqlengths=c(chr1=21, chr2=41))
gr2 # so we can see what these ranges look like
seqinfo(gr2)
gaps(gr2) # * indicates ambiguous strands
# create a genomicRange object without strand info
gr3 <- gr2
strand(gr3) <- "*"
gr3
gaps(gr3) # all strands
gaps(gr3)[strand(gaps(gr3)) == "*"] # filter in gaps where strand is ambiguous

chrom_grngs <- as(seqinfo(txdb), "GRanges") # coerce txdb to GRanges object
seqinfo(txdb)
chrom_grngs # length of chromosomes are turned into ranges
# head(chrom_grngs, 2)
collapsed_tx <- reduce(transcripts(txdb))
transcripts(txdb)
collapsed_tx
strand(collapsed_tx) <- "*"
intergenic <- setdiff(chrom_grngs, collapsed_tx)
intergenic
```

Calculating coverage of GRanges objects

```{r}
# generate fake 150 bp reads on a chromomosome
set.seed(0)
chr19_len <- seqlengths(txdb)["chr19"]
chr19_len
start_pos <- sample(1:(chr19_len-150), 2047719, replace = TRUE)
# 2047719 == (5*61431566)/150 # to achieve 5X coverage of a 61431566 bp genome with 150bp reads
reads <- GRanges("chr19", IRanges(start = start_pos, width = 150))
reads

# now calculate coverage.  this is how i could have calculated the pct_coverage using the shorter version of genomecov output in spikepipe
cov_reads <- coverage(reads)
mean(cov_reads)
cov_reads # run_length_encoding
table(cov_reads==0) # one way
sum(runLength(cov_reads)[runValue(cov_reads) == 0]) # sum the runLengths values that have 0 coverage (runValue ==0)
runLength(cov_reads) # top half of cov_reads
runValue(cov_reads) # bottom half of cov_reads
runLength(cov_reads)[runValue(cov_reads) == 0] # the run lengths where coverage == 0, which i sum to get total length of zero coverage intervals
class(cov_reads) # SimpleRleList

sum(runLength(cov_reads)[runValue(cov_reads) == 0]) / chr19_len # 0.006628047 proportion of chr19 that has 0 coverage

```

```{bash}
setwd("~/src/bds-files/chapter-10-sequence")
# sickle and seqtk to trim low-quality bases off ends of reads
sickle se -f untreated1_chr4.fq -t sanger -o untreated1_chr4_sickle.fq
seqtk trimfq untreated1_chr4.fq > untreated1_chr4_trimfq.fq
```

```{r}
# BiocManager::install("qrqc")
library(qrqc)
setwd("~/src/bds-files/chapter-10-sequence")

# read in filenames
fqfiles <- c(none="untreated1_chr4.fq",
             sickle="untreated1_chr4_sickle.fq",
             seqtk="untreated1_chr4_trimfq.fq")

# use readSeqFile on each fastq and store in a list
seq_info <- lapply(fqfiles, function(file) {
    readSeqFile(file, hash = FALSE, kmer = FALSE)
})

# extract qualities from each file
# extract qualities as a dataframe
quals <- mapply(function(sfq, name) {
    qs <- getQual(sfq)
    qs$trimmer <- name
    qs
}, seq_info, names(fqfiles), SIMPLIFY=FALSE)

d <- do.call(rbind, quals)

p1 <- ggplot(d) + geom_line(aes(x=position, y=mean, linetype=trimmer))
p1 <- p1 + ylab("mean quality (Sanger)") + theme_bw()
print(p1)

p2 <- qualPlot(seq_info, quartile.color=NULL, mean.color=NULL) + theme_bw()
p2 <- p2 + scale_y_continuous("quality (sanger)")
print(p2)
```

purrr::map

```{r purrr map}
library(tidyverse)
# basic stuff.  use map to apply a function to all columns of a tibble
df <-  tibble(
    a = rnorm(10),
    b = rnorm(10),
    c = rnorm(10),
    d = rnorm(10)
); df

df_mean <- df %>% map_dbl(mean)
df_mean # numeric double vector
df_mean[1] # name and value
df_mean[[1]] # value only
df %>% map_dbl(median)
df %>% map_dbl(sd)

base::split() # divide into groups and reassemble. split divides the data in the vector x into the groups defined by f. The replacement forms replace values corresponding to such a division. unsplit reverses the effect of split.

models <- mtcars %>%
    split(mtcars$cyl) %>%  # .$cyl is equiv to mtcars$cyl
    map(~lm(mpg ~ wt, data = .)) # . is used as a pronoun, for each value of cyl
models[["4"]]  # output is a list of 3 models.  "4" refers to the list element 4

models <- mtcars %>%
    split(.$cyl) %>%  # .$cyl must mean mtcars$cyl
    map(~lm(mpg ~ wt, data = .)) # . is used as a pronoun, for each value of cyl
                                # this is a one-sided formula (nothing to the left of ~)
models[["4"]]  # output is a list of 3 models.  "4" refers to the list element 4

# to extract elements, like R-squared, run summary() on the elements and then pipe to extract $r.squared

models %>%
    map(summary) %>%
    map_dbl(~.$r.squared) # ~ one-sided formula, . is the pronoun for the list element of map(summary) output, $r.squared is the element

models %>%
    map(summary) %>%
    map_dbl("r.squared") # map() shortcut, use a string to extract an element

mtcars %>% map(mean) # calculate the mean of every column in mtcars

iris %>% map(unique) %>% map(length) # calculate the number of unique items in each column of iris

# map2 and pmap:  map over multiple arguments
mu <- list(5, 10, -3) # these are the means to be used for rnorm.

# if i set n = 5, the input number is then used for mean =
mu %>% map(rnorm, n = 5) %>%  # for just one element of mu:  rnorm(mu[[1]], n = 5)
    str()

# but what if we also want to vary the standard deviation of rnorm?
# rnorm(n, mean = 0, sd = 1) # the default parms

mu <- list(5, 10, -3) # these are the means to be used by rnorm for mean
sigma <- list(1, 5, 10) # these are the sigmas to be used by rnorm for sd
map2(mu, sigma, rnorm, n = 5) # mean = mu, sd = sigma
# generates
# rnorm(5, 1, n=5) # the order of mu and sigma in map2() is the order of the parameter input into rnorm
# rnorm(10, 5, n=5)
# rnorm(-3, 10, n=5)

# pmap for 3+ parameters
mu <- list(5, 10, -3) # these are the means to be used by rnorm for mean
sigma <- list(1, 5, 10) # these are the sigmas to be used by rnorm for sd
n <- list(1, 3, 5) # a list of lists
args <- list(n = n, mean = mu, sd = sigma) # name the arguments
args %>% pmap(rnorm) %>% str()
# generates
# rnorm(mean = 5, sd = 1, n = 1) # for first of 3 commands

# keep()
iris %>% keep(is.factor) %>% str() # keep the elements that are TRUE
iris %>% discard(is.factor) %>% str() # keep the elements that are FALSE
x <- list(1:5, letters, list(10)); x
x %>% some(is_character)
x %>% every(is_character)
x %>% every(is_vector)

# reduce().  keep iterating a function that takes two inputs (e.g. full_join takes two tables) and iterate it until all inputs have been run
# here's a list of 3 tables
dfs <- list(
  age = tibble(name = "John", age = 30),
  sex = tibble(name = c("John", "Mary"), sex = c("M", "F")),
  trt = tibble(name = "Mary", treatment = "A")
)
# full_join them until there's only one table left
dfs %>% reduce(full_join)

# here are three vectorx
vs <- list(
  c(1, 3, 5, 6, 10),
  c(1, 2, 3, 7, 8, 10),
  c(1, 2, 3, 4, 8, 9, 10)
)
# intersect them all
vs %>% reduce(intersect)

# accumulate() keeps all interim results

x <- sample(10)
x
x %>% accumulate(`+`)

```

resample columns of a dataframe with replacement

```{r}
library(tidyverse)
library(janitor)

# create a function that resamples columns of a dataframe with replacement
# uses base R sample() and janitor::clean_names()
boottab <- function(TaxoSimple){
    OTU <- TaxoSimple %>% select(OTU_)
    TaxoSimple_noOTU <- TaxoSimple %>% select(-OTU_)
    TaxoSimpleBoot <- TaxoSimple_noOTU[ , sample(ncol(TaxoSimple_noOTU), size=ncol(TaxoSimple_noOTU), replace=TRUE)]
    TaxoSimpleBoot <- clean_names(TaxoSimpleBoot, case = c("screaming_snake"))
    TaxoSimpleBoot <- bind_cols(OTU, TaxoSimpleBoot)
    TaxoSimpleBoot
}
# number of bootstraps
numBoots <- 99 

# create a list with TaxoSimple repeated nBoot times
TaxoSimpleList <- list(TaxoSimple)[rep(1, numBoots)]
# TaxoSimpleList is now a list of 99 copies of TaxoSimple

# apply your bootstrap function to each of the 99 elements
TaxoSimpleBootList <- map(TaxoSimpleList, boottab) # list output, or
TaxoSimpleBootDf <- map_dfr(TaxoSimpleList, boottab, .id = "Bootnum") # dataframe output
```

how to use map() excellent tutorials
<https://jennybc.github.io/purrr-tutorial/bk00_vectors-and-lists.html>
<https://jennybc.github.io/purrr-tutorial/bk01_base-functions.html>

```{r}
library(repurrrsive)
data(got_chars) # a list, with each sublist a character
tibble::tibble(
  name = map_chr(got_chars[23:25], "name"),
  id = map_int(got_chars[23:25], "id")
)

# useful for working on list-cols
library(tidyverse)
library(gapminder)
library(broom)

gapminder %>% 
  ggplot(aes(year, lifeExp, group = country)) +
    geom_line(alpha = 1/3)

gapminder %>%
  ggplot(aes(year, lifeExp, group = country)) +
  geom_line(stat = "smooth", method = "lm",
            alpha = 1/3, se = FALSE, colour = "black")
# create a list-col with nest()
gap_nested <- gapminder %>%
  group_by(country) %>%
  nest()
gapminder
gap_nested # the data for each country is a data frame, on which i can now operate separately (kind of like group_by but for dataframes)
gap_nested[[2]][[1]]
gap_nested$data[[1]]
colname(gap_nested)
# fit a model to each country
gap_fits <- gap_nested %>%
  mutate(fit = map(data, ~ lm(lifeExp ~ year, data = .x))) # the first 'data' is the list-col, which is called data.  the second 'data' from 'data = .x' is the lm() function's argument
gap_fits %>% tail(3)
canada <- which(gap_fits$country == "Canada")
summary(gap_fits$fit[[canada]])

# create a new column with the rquared extracted from the lm model
# this part requires me to read more carefully.  there is a good tutorial pdf from jcbryan:  2018-09_purrr-latinr_Jenny_Bryan.pdf, which i have
gap_fits %>% 
  mutate(rsq = map_dbl(fit, ~ summary(.x)[["r.squared"]])) %>%
  arrange(rsq)

# using broom::coef()
gap_fits %>%
  mutate(coef = map(fit, tidy)) %>%
  unnest(coef)
```

using list-cols and map2.\
<https://jennybc.github.io/purrr-tutorial/ls12_different-sized-samples.html>
interesting problem: apply sample\_n to a dataset where the n sampled
differs per level (here, Species). You could imagine writing a loop to
do this.

```{r}
library(purrr)
library(tidyr)
set.seed(4561)

nested_iris <- iris %>%
    group_by(Species) %>%   # prep for work by Species
    nest() %>%              # --> one row per Speciesm creates a list-col
    ungroup() %>%          # no more need for group_by
    mutate(n = c(2, 5, 3)) %>%  # adds a sample size column called n
    mutate(samp = map2(data, n, sample_n)) %>% # adds col called 'samp', which applies the function `sample_n(n, data)` (e.g. sample_n(2,data)) where 'data' is the list-col.  The output samp is also a list-col.
    select(-data) %>% # remove the original data list-col
    unnest(samp) # unnest

```

tapply versus dplyr

```{r}
data(iris)
# a common way to generate a summary table in R is to use the tapply() function, which is one of the base R apply() functions
# mean petal length by species
tapply(iris$Petal.Length, Species, mean)
# apply the mean() function to Petal.Length for each species separately.  It's not easy to read.

# But you can just use the group_by() and summarise() functions as we did above, with a bit more typing but more coherence.

# this is exactly like the tapply() function above
iris %>% 
    group_by(Species) %>% 
    summarise(
       Petal.Length_mean =  mean(Petal.Length)
       )

# this is a more complicated version that tapply() can't give you.  the vars() function is a column selector in the tidyverse that is used with the variant summarise_at().
iris %>% 
    group_by(Species) %>% 
    summarise_at(
        vars(Petal.Length, Petal.Width, Sepal.Width, Sepal.Length), mean
        )
```

# set cells to 0 if the value is \< 1% of the colSum (OTU read sum)

```{r}
# pits_working_otus is the dataframe
# sum(x)/100 removes cells that are less than 1% of the sum of the column x
pits_working_otus <- map_dfc(pits_working_otus, function(x) {x[x < sum(x)/100] <- 0; x}) # map_dfc does a colbind
# pitsdf <- do.call(cbind, pitslist)
# pitsdf <- as_tibble(pitsdf)
pits_working_otus <- as_tibble(pits_working_otus, .name_repair = "universal")
```

keep only OTUs with \>=5 occurrences

```{r}
OTU2 <- OTU[, colSums(OTU) >= 5] # reduces from 76 to 47 OTUs
```

<http://rprogramming.net/recode-data-in-r/> Recode All Columns in a Data
Frame in One Step

The examples I've shown you so far allow you to recode one variable at a
time. Sometimes you might want to recode all of your variables. You can
do this by just repeating your recode command over and over but this can
be a lot of typing if you have hundreds of variables.

To show how this works, lets get some data to practice with. We can use
the ChickWeight data set that is included in the base R installation.
Don't be alarmed if you see <promise> in your Global Environment when
you run the below line. This is just R's way of saying it will load the
data when you need it.

```{r}
# Create practice data with two columns called V1 and V2
data <- as.data.frame(matrix(0, ncol = 2, nrow = 5))
data$V1 <- c(1, 0, 0, 1, 1)
data$V2 <- c(0, 0, 1, 0, 0)

# Let’s recode the number 1 in the practice data frame to 777. This code uses both function() and apply() but teaching about these is beyond the scope of this article
# Recode 1 to 777 wherever 1 appears in the practice data frame
data <- apply(data, 2, function(x) {x[x == 1] <- 777; x})


# You can use the recode command from the car library to recode multiple values at the same time across all the variables in the dataset using the above method.
# Create practice data with two columns called V1 and V2
data <- as.data.frame(matrix(0, ncol = 2, nrow = 5))
data$V1 <- c(1, 0, 0, 1, 1)
data$V2 <- c(0, 0, 1, 0, 0)

# Recode 1 to 777 wherever 1 appears in the practice data frame
# Recode 0 to 888 wherever 0 appears in the practice data frame
library(car)
data <- apply(data, 2, function(x) {x <- recode(x,"1=777; 0=888"); x})


```

metacoder 0.2.0

```{r metacoder}
library(metacoder)

print(hmp_otus) # standard OTU table:  every *column* is a sample
print(hmp_samples) # every row is a sample
hmp_otu <- hmp_otus # to see hmp_otus
hmp_sample <- hmp_samples # to see hmp_samples

# We can parse the taxonomic information in the abundance matrix using a parser from taxa.
# This reads the taxonomic information that is in column 2 (lineage).
obj <- parse_tax_data(hmp_otus, class_cols = "lineage", class_sep = ";",
                      class_key = c(tax_rank = "info", tax_name = "taxon_name"),
                      class_regex = "^(.+)__(.+)$")

# This returns a taxmap object. The taxmap class is designed to store any number of tables, lists, or vectors associated with taxonomic information and facilitate manipulating the data in a cohesive way. Here is what that object looks like:

print(obj) # or click on the object in the Environment pane

# Removing low-abundance counts
#set all counts with less than 5 reads to zero:
obj$data$tax_data <- zero_low_counts(obj, "tax_data", min_count = 5)
no_reads <- rowSums(obj$data$tax_data[, hmp_samples$sample_id]) == 0
sum(no_reads)
obj <- filter_obs(obj, "tax_data", ! no_reads, drop_taxa = TRUE)

print(obj)

# accounting for un-even sampling
obj$data$tax_data <- calc_obs_props(obj, "tax_data")

print(obj)

# Getting per-taxon information
# Currently, we have values for the abundance of each OTU, not each taxon. To get information on the taxa, we can sum the abundance per-taxon like so:
obj$data$tax_abund <- calc_taxon_abund(obj, "tax_data",
                                       cols = hmp_samples$sample_id)
print(obj)
# Note that there is now an additional table with one row per taxon.
# We can also easily calculate the number of samples have reads for each taxon:
obj$data$tax_occ <- calc_n_samples(obj, "tax_abund", groups = hmp_samples$body_site)
print(obj)

# Plotting taxonomic data
# Now that we have per-taxon information, we can plot the information using heat trees. The code below plots the number of “Nose” samples that have reads for each taxon. It also plots the number of OTUs assigned to each taxon in the overall dataset.

# heat_tree(obj,
#           node_label = taxon_names,
#           node_size = n_obs,
#           node_color = Nose,
#           node_size_axis_label = "OTU count",
#           node_color_axis_label = "Samples with reads")

heat_tree(obj,
          node_label = obj$taxon_names(),
          node_size = obj$n_obs(),
          node_color = obj$data$tax_occ$Nose,
          node_size_axis_label = "OTU count",
          node_color_axis_label = "Samples with reads")


# Comparing two treatments/groups
obj$data$diff_table <- compare_groups(obj, dataset = "tax_abund",
                                      cols = hmp_samples$sample_id,
                                      groups = hmp_samples$sex)
print(obj$data$diff_table)
# note column called log2_median_ratio

heat_tree(obj,
          node_label = taxon_names,
          node_size = n_obs,
          node_color = log2_median_ratio,
          node_color_interval = c(-2, 2),
          node_color_range = c("cyan", "gray", "tan"),
          node_size_axis_label = "OTU count",
          node_color_axis_label = "Log 2 ratio of median proportions")

obj$data$diff_table$wilcox_p_value <-
    p.adjust(obj$data$diff_table$wilcox_p_value, method = "fdr")
hist(obj$data$diff_table$wilcox_p_value)

# Comparing any number of treatments/groups
obj$data$diff_table <- compare_groups(obj, dataset = "tax_abund",
                                      cols = hmp_samples$sample_id,
                                      groups = hmp_samples$body_site)
print(obj$data$diff_table)
heat_tree_matrix(obj,
                 dataset = "diff_table",
                 node_size = n_obs,
                 node_label = taxon_names,
                 node_color = log2_median_ratio,
                 node_color_range = diverging_palette(),
                 node_color_trans = "linear",
                 node_color_interval = c(-3, 3),
                 edge_color_interval = c(-3, 3),
                 node_size_axis_label = "Number of OTUs",
                 node_color_axis_label = "Log2 ratio median proportions")
```

```{r notify me when a function finishes}
library(beepr)
beep(sound = 1) # play a sound.  place after the end of a long function to inform me that something has finished
# alternatively, have a long voice message on macOS
system("say Just finished!")

# https://stackoverflow.com/questions/3365657/is-there-a-way-to-make-r-beep-play-a-sound-at-the-end-of-a-script
# brew install terminal-notifier
# then make this function
notify <- function(msgString='Message from R', titleString='Message from R', speakIt=FALSE) {
    cmd <- paste('terminal-notifier -message ', '"', msgString, '"  -title "', titleString, '"', sep='')
    system(cmd)

    if (speakIt) {
        system(paste('say', msgString))
    }
}

notify("R is done", "Message from R", speakIt=TRUE)

```

```{r make a histogram with numbers at top of each column}
##### calculate distribution of read numbers per OTU to set minimum number
otureads <- c(colSums(community_now)) # list of the reads per OTU
sum(otureads) ## 1,086,199 reads total (this is after removing some of the sites, so this is almost but not all the reads in the original dataset)
otureads[otureads>5000] <- 5000 # to make the histogram readable
otuhist <- hist(otureads, breaks = 100)
text(otuhist$mids, otuhist$counts, cex = 0.5, otuhist$counts, adj = c(.5, -.5), col = "blue3")
```

Change the order of columns in a table

```{r change order of columns}
assay2_table <-  with(assay2, table(Strep_outcome, medium2)) # creates 2X2 table in the Tabitha Innocent Streptomyces competition experiment
assay2_table <- assay2_table[, c(2,1)] # switch the order of columns 1 and 2

leechotu4 <- leechotu4 %>% dplyr::select(final.taxonomy, total_reads, incidence, everything()) # reorder columns.  the everything() includes all the remaining columns. seems to ignore cols that are already named
```

Change the name of a column. Use dplyr::select. E.g. "species =
mitogenome" changes the name of the 'mitogenome' column to 'species'
while selecting the column. Another method is to use dplyr::rename,
which keeps all columns by default

```{r}
env_data <- soupdata %>%
    dplyr::select(sample = DateTrapEIRUN, species = mitogenome, mapped_reads_mitogenome = mapped_reads, pct_coverage_mitogenome = pct_coverage)

# Renaming -----------------------------------------
# * select() keeps only the variables you specify
select(iris, petal_length = Petal.Length)  # orig colname is Petal.Length

# * rename() keeps all variables
rename(iris, petal_length = Petal.Length)  # orig colname is Petal.Length
```

Add an italicised word to the main title for a plot

```{r add italicised word to main title for a plot}
# formatted for publication
par(mfrow=c(1,2))
barplot(assay2_table, beside=T, main = expression(paste("Competitive outcome for ", italic('Streptomyces'), " strain S2")), names.arg = c("Ps2-infused media", "Control media"), col=colorvec, ylim = c(0, 80))
legend("top",c("win", "loss", "draw"),fill=colorvec)
barplot(assay8_table, beside=T, main = expression(paste("Competitive outcome for ", italic('Streptomyces'), " strain S8")), names.arg = c("Ps1-infused media", "Control media"), ylab="Frequency", col=colorvec, ylim = c(0, 80))
legend("top",c("win", "loss", "draw"),fill=colorvec)
par(mfrow=c(1,1))
```

```{r devtools}
library(devtools)
devtools::install_github("shaunpwilkinson/insect")
```

```{r bold}
library(bold)
library(seqinr)
pardosa_groenlandia <- bold_seq(taxon = "Pardosa groenlandica")
write.fasta(pardosa_groenlandia, names = "", file.out = "Pardosa_groenlandica.fas", open = "w", nbchar = 180, as.string = FALSE)
```

```{r seqinr}
library(seqinr)
library(conflicted)
conflict_prefer("count", "dplyr")
## Read 3 sequences from a FASTA file:
ortho <- read.fasta(file = system.file("sequences/ortho.fasta", package =
"seqinr"))

## Select only third codon positions:
ortho3 <- lapply(ortho, function(x) x[seq(from = 3, to = length(x), by = 3)])

## Write the 3 modified sequences to a file:
fname <- tempfile(pattern = "ortho3", tmpdir = tempdir(), fileext = "fasta")
#write.fasta(sequences = ortho3, names = names(ortho3), nbchar = 80, file.out = "ortho3.fasta")
write.fasta(sequences = ortho3, names = names(ortho3), nbchar = 80, file.out = fname)

## Read them again from the same file and check that sequences are preserved:
ortho3bis <- read.fasta(fname, set.attributes = FALSE)
stopifnot(identical(ortho3bis, ortho3))
```

RStudio git push and pull The easiest way i've found is to 1. set up
local git for a folder in R 2. add the folder to Github Desktop 3. push
the Publish button 4. the Push and Pull buttons in RStudio's Git tab
should become active

BY COMMAND Set up github remote on github.com/dougwyu/
<https://www.r-bloggers.com/rstudio-and-github/> The repo should be
completely empty: no readme, no license Following instructions assume
that i've set up ssh RSA key access to github already open an R Project
in RStudio, open Terminal using Tools -\> Shell..

```{bash}
git remote add origin https://github.com/dougwyu/2014_2015_2016_Auke_qPCR.git  # or whatever repo one wants for this particular project
git config remote.origin.url git@github.com:dougwyu/2014_2015_2016_Auke_qPCR.git
git pull -u origin master
git push -u origin master
```

Click on Git tab. Pull and Push buttons should now be active. If
problems, try restarting RStudio.

```{bash}
git remote add origin https://github.com/dougwyu/screening_Innocent_et_al.git  # or whatever repo one wants for this particular project
git config remote.origin.url git@github.com:dougwyu/screening_Innocent_et_al.git
git pull -u origin master
git push -u origin master
```

Click on Git tab. Pull and Push buttons should now be active. If
problems, try restarting RStudio.

```{bash}
git remote add origin https://github.com/dougwyu/VietnamLaosLeeches.git  # or whatever repo one wants for this particular project
git config remote.origin.url git@github.com:dougwyu/VietnamLaosLeeches.git
git pull -u origin master
git push -u origin master
```

# to update the pathname to git in RStudio, follow these instructions

<https://stackoverflow.com/questions/14011998/setting-path-to-git-executable-in-rstudio-under-osx-mountain-lion>
\# So: in RStudio, in the Git executable settings, click on 'Browse';
when the save dialog box opens, hit Command+Shift+Period. This makes
/usr/ visible. Navigate to the git executable file (/usr/local/bin/git),
select it, and Save. This git file is an alias to the latest git, which
is in usr/local/Cellar/git/ Transpose a dataframe, using sjmisc::sjmisc.
This one automatically puts the row names into the colnames, and
colnames into rownames (or into a new column)

```{r}
library(sjmisc)
data(mtcars)
mtcars
t_mtcars <- rotate_df(mtcars)
x <- mtcars[1:3, 1:4]
x
rotate_df(x)
rotate_df(x, rn = "property")

# use values in 1. column as column name
rotate_df(x, cn = TRUE)
rotate_df(x, rn = "property", cn = TRUE)

# also works on list-results
library(purrr)

dat <- mtcars[1:3, 1:4]
tmp <- purrr::map(dat, function(x) {
  sdev <- stats::sd(x, na.rm = TRUE)
  ulsdev <- mean(x, na.rm = TRUE) + c(-sdev, sdev)
  names(ulsdev) <- c("lower_sd", "upper_sd")
  ulsdev
})
tmp
as.data.frame(tmp)
rotate_df(tmp)

tmp <- purrr::map_df(dat, function(x) {
  sdev <- stats::sd(x, na.rm = TRUE)
  ulsdev <- mean(x, na.rm = TRUE) + c(-sdev, sdev)
  names(ulsdev) <- c("lower_sd", "upper_sd")
  ulsdev
})
tmp
rotate_df(tmp)
```

Transpose a dataframe, using data.table::transpose package

```{r}
library(data.table)
library(tidyverse)

# this way works
# Transpose a dataframe, using data.table::transpose package
# kafaMBC is the OTU table
kafaMBC_t <- transpose(kafaMBC)
kafaMBC_t$fields <- colnames(kafaMBC)
kafaMBC_t <- kafaMBC_t %>% select(fields, everything())
colnames(kafaMBC_t) <- kafaMBC_t[1, ] # move first row to column names
kafaMBC_t <- kafaMBC_t %>% slice(-1) # remove first row



# get data
data("mtcars")
mtcars <- mtcars
mtcars <- mtcars %>% tibble::rownames_to_column(var = "car_model")

# transpose
t_mtcars <- data.table::transpose(mtcars) # transposes a data frame

# get row and colnames in order
# get row and colnames in order
  colnames(t_mtcars) <- rownames(mtcars)
  rownames(t_mtcars) <- colnames(mtcars)


  # this can be difficult
t_mtcars$car_meas <- colnames(mtcars) # label rows
colnames(t_mtcars) <- t_mtcars[1,] #
t_mtcars <- t_mtcars %>% slice(-1)
t_mtcars <- dplyr::select(car_model, Mazda )
t_mtcars <- t_mtcars %>% rownames_to_column(var = "car_meas")
t_mtcars[1,]




```

```{r treeio and ggtree for reading jplace files}
# https://bioconductor.org/packages/devel/bioc/vignettes/treeio/inst/doc/Importer.html#parsing-epa-and-pplacer-output
library(treeio)
library(ggtree)
tree <- read.jplace("MIDORI_lrRNA.final_epa.jplace")
print(jp)
ggplot(tree, aes(x, y)) + geom_tree() + theme_tree()
ggplot(tree, aes(x, y), ladderize = FALSE) + geom_tree() + theme_tree()
ggplot(tree, aes(x, y), branch.length = "none") + geom_tree() + theme_tree()
ggplot(tree, aes(x, y)) + geom_tree() + theme_tree2()
ggplot(tree, aes(x, y)) + geom_tree() + geom_treescale()
ggplot(tree, aes(x, y)) + geom_tree() + geom_treescale() + geom_tiplab(size = 1)
gzoom(tree, grep("AY012154", tree@placements[["name"]]))

library(ape)
data(chiroptera)
library(ggtree)
gzoom(chiroptera, grep("Plecotus", chiroptera$tip.label))
```

pheatmap <http://slowkow.com/notes/heatmap-tutorial/>
<https://sahirbhatnagar.com/heatmap> <https://www.biostars.org/p/66079/>
<https://github.com/raivokolde/pheatmap/blob/master/R/pheatmap.r>

# Generate heat maps from tabular data with the R package "pheatmap"

SP:BITS©2013

This is an example use of **pheatmap** with kmean clustering and
plotting of each cluster as separate heatmap. The code below is made
redundant to examplify different ways to use 'pheatmap'. The data was
purposely taken from a R-dataset to ease distribution but similar
results will be obtained any other multicolumn dataframe.

Load libraries and read data in

```{r}
library("pheatmap")
library("RColorBrewer")
## if not installed, quickly add it as follows:
#source("http://bioconductor.org/biocLite.R")
#biocLite(c("RColorBrewer", "pheatmap"))

data(USArrests)
data <- USArrests
head(data)

# save all results to current dir
basedir <- getwd()
```

Plot full heatmap clustered by rows and columns by euclidean distance

```{r, fig.keep='first'}
# ```{r, fig.keep='none'} to avoid printing blank figure markup-output
# ```{r, fig.keep='first'} to print only the first figure

# store the heatmap into an object in order to to recover kmean information

# decide of the number of k-mean clusters to build
# use clValid or other functions to better define this number (out of scope here!)
maxclust <- 5

# create color palette
col.pal <- brewer.pal(9, "Blues")

# define metrics for clustering
drows1 <- "euclidean"
dcols1 <- "euclidean"

# prepare path and file.name for output
filename <- "my.pheatmap.pdf"
outfile <- paste(basedir, filename, sep="/")

# create heatmap
# type "?pheatmap()" for more help
hm.parameters <- list(data,
  color = col.pal,
  cellwidth = 15, cellheight = 12, scale = "none",
  treeheight_row = 200,
  kmeans_k = NA,
  show_rownames = T, show_colnames = T,
  main = "Full heatmap (avg, eucl, unsc)",
  clustering_method = "average",
  cluster_rows = TRUE, cluster_cols = TRUE,
  clustering_distance_rows = drows1,
  clustering_distance_cols = dcols1)

  # To draw the heat map on screen
  do.call("pheatmap", hm.parameters)

  # To draw to file
  # do.call("pheatmap", c(hm.parameters, filename=outfile))
```

Plot each cluster as heatmap

```{r, fig.keep='first'}
# plot 2-5 kmean clusters
for (maxclust in c(2:maxclust)) {
  filename <- paste("heatmap.for.", maxclust, "-clusters.pdf", sep="")
  outfile <- paste(basedir, filename, sep="/")

  main <- paste("result for ", maxclust, " clusters", sep="")
  hmx.parameters <- list(data,
    color = col.pal,
    cellwidth = 15, cellheight = 12, scale = "none",
    treeheight_row = 200,
    kmeans_k = maxclust,
    show_rownames = T, show_colnames = T,
    main = main,
    clustering_method = "average",
    cluster_rows = TRUE, cluster_cols = TRUE,
    clustering_distance_rows = drows1,
    clustering_distance_cols = dcols1)

  # To store cluster mappings and draw
  kmean.hm <- do.call("pheatmap", hmx.parameters)

  # To draw on screen
  do.call("pheatmap", hmx.parameters)

  # To draw to file
  do.call("pheatmap", c(hmx.parameters, filename=outfile))

  # add cluster number to matrix and save
  clustnum <- kmean.hm[["kmeans"]][["cluster"]]
  clustered.data <- cbind(data, clustnum)
  last <- ncol(clustered.data)-1

  # inspect data
  cat (paste("Data for max-clust= ", maxclust, "\n", sep=""))
  print(head(clustered.data))

  filename <- paste("Clustered.for.", maxclust, "-clusters.tsv", sep="")
  outfile <- paste(basedir, filename, sep="/")

  write.table(clustered.data, file=outfile, quote = FALSE, sep="\t",
              col.names = T, row.names = T)

  # plot 'maxclust' cluster detailed heatmap's
  clustered.data <- as.data.frame(clustered.data)

  for (clust in 1:maxclust){
    # sample data
    cluster <- subset(clustered.data, clustered.data$clustnum==clust)[1:last]

    # prepare output
    filename <- paste("cluster.", clust, "_of_", maxclust,".pdf", sep="")
    outfile <- paste(basedir, filename, sep="/")

    main <- paste("cluster #", clust, " of #", maxclust, sep="")

    cluster.parameters <- list(cluster,
      color = col.pal,
      cellwidth = 15, cellheight = 12,
      scale = "none",
      treeheight_row = 200,
      kmeans_k = NA,
      show_rownames = T, show_colnames = T,
      main = main,
      clustering_method = "average",
      cluster_rows = TRUE, cluster_cols = TRUE,
      clustering_distance_rows = drows1,
      clustering_distance_cols = dcols1)

      # To draw the heat map on screen
      do.call("pheatmap", cluster.parameters)

      # To draw to file
      do.call("pheatmap", c(cluster.parameters, filename=outfile))
    }
    # next maxclust value
  }
```

# end

<https://davetang.org/muse/2018/05/15/making-a-heatmap-in-r-with-the-pheatmap-package/>

```{r}
# install.packages(pheatmap)
```

```{r}
# load package
library(pheatmap)
library(RColorBrewer)
```

```{r}
# install DESeq if necessary
# source("http://bioconductor.org/biocLite.R")
# biocLite("DESeq")

# load package
library("DESeq")
```

```{r}
# load data and subset
example_file <- system.file ("extra/TagSeqExample.tab", package="DESeq")
data <- read.delim(example_file, header=T, row.names="gene")
data_subset <- as.matrix(data[rowSums(data)>50000,])

# create heatmap using pheatmap
pheatmap(data_subset)
```

```{r}
cal_z_score <- function(x){
  (x - mean(x)) / sd(x)
}

data_subset_norm <- t(apply(data_subset, 1, cal_z_score)) # 1 == margin = rows. 2 == cols
pheatmap(data_subset_norm)
```

```{r}
my_hclust_gene <- hclust(dist(data_subset), method = "complete")
# dist(data_subset)

# install if necessary
# install.packages(dendextend)

# load package
library(dendextend)

as.dendrogram(my_hclust_gene) %>%
  plot(horiz = TRUE)
```

```{r}
my_gene_col <- cutree(tree = as.dendrogram(my_hclust_gene), k = 2)

my_gene_col
```

```{r}
my_gene_col <- data.frame(cluster = ifelse(test = my_gene_col == 1, yes = "cluster 1", no = "cluster 2"))

head(my_gene_col)
```

```{r}
set.seed(1984)
my_random <- as.factor(sample(x = 1:2, size = nrow(my_gene_col), replace = TRUE))
my_gene_col$random <- my_random

head(my_gene_col)

```

```{r}
my_sample_col <- data.frame(sample = rep(c("tumour", "normal"), c(4,2)))
row.names(my_sample_col) <- colnames(data_subset)

my_sample_col

pheatmap(data_subset,
    color = brewer.pal(9, "YlOrRd"),
	annotation_row = my_gene_col,
	annotation_col = my_sample_col)

pheatmap(data_subset_norm,
    color = brewer.pal(9, "YlOrRd"),
	annotation_row = my_gene_col,
	annotation_col = my_sample_col)
```

```{r}
pheatmap(data_subset,
    color = brewer.pal(9, "YlOrRd"),
         annotation_row = my_gene_col,
         annotation_col = my_sample_col,
         cutree_rows = 2,
         cutree_cols = 2)

pheatmap(data_subset_norm,
    color = brewer.pal(9, "YlOrRd"),
         annotation_row = my_gene_col,
         annotation_col = my_sample_col,
         cutree_rows = 4,
         cutree_cols = 2)
```

```{r}
# use silent = TRUE to suppress the plot
my_heatmap <- pheatmap(data_subset, silent = TRUE)

# results are stored as a list
class(my_heatmap)

names(my_heatmap)

my_heatmap$tree_row %>%
  as.dendrogram() %>%
  plot(horiz = TRUE)
```

pheatmap help example

```{r}
# Create test matrix
test = matrix(rnorm(200), 20, 10)
test[1:10, seq(1, 10, 2)] = test[1:10, seq(1, 10, 2)] + 3
test[11:20, seq(2, 10, 2)] = test[11:20, seq(2, 10, 2)] + 2
test[15:20, seq(2, 10, 2)] = test[15:20, seq(2, 10, 2)] + 4
colnames(test) = paste("Test", 1:10, sep = "")
rownames(test) = paste("Gene", 1:20, sep = "")

# Draw heatmaps
pheatmap(test)
pheatmap(test, kmeans_k = 2)
pheatmap(test, scale = "row", clustering_distance_rows = "correlation")
pheatmap(test, color = colorRampPalette(c("navy", "white", "firebrick3"))(50))
pheatmap(test, cluster_row = FALSE)
pheatmap(test, legend = FALSE)

# Show text within cells
pheatmap(test, display_numbers = TRUE)
pheatmap(test, display_numbers = TRUE, number_format = "%.1e")
pheatmap(test, display_numbers = matrix(ifelse(test > 5, "*", ""), nrow(test)))
pheatmap(test, cluster_row = FALSE, legend_breaks = -1:4, legend_labels = c("0",
"1e-4", "1e-3", "1e-2", "1e-1", "1"))

# Fix cell sizes and save to file with correct size
pheatmap(test, cellwidth = 15, cellheight = 12, main = "Example heatmap")
# pheatmap(test, cellwidth = 15, cellheight = 12, fontsize = 8, filename = "test.pdf")

# Generate annotations for rows and columns
annotation_col = data.frame(
                    CellType = factor(rep(c("CT1", "CT2"), 5)),
                    Time = 1:5
                )
rownames(annotation_col) = paste("Test", 1:10, sep = "")

annotation_row = data.frame(
                    GeneClass = factor(rep(c("Path1", "Path2", "Path3"), c(10, 4, 6)))
                )
rownames(annotation_row) = paste("Gene", 1:20, sep = "")

# Display row and color annotations
pheatmap(test, annotation_col = annotation_col)
pheatmap(test, annotation_col = annotation_col, annotation_legend = FALSE)
pheatmap(test, annotation_col = annotation_col, annotation_row = annotation_row)


# Specify colors
ann_colors = list(
    Time = c("white", "firebrick"),
    CellType = c(CT1 = "#1B9E77", CT2 = "#D95F02"),
    GeneClass = c(Path1 = "#7570B3", Path2 = "#E7298A", Path3 = "#66A61E")
)

pheatmap(test, annotation_col = annotation_col, annotation_colors = ann_colors, main = "Title")
pheatmap(test, annotation_col = annotation_col, annotation_row = annotation_row,
         annotation_colors = ann_colors)
pheatmap(test, annotation_col = annotation_col, annotation_colors = ann_colors[2])

# Gaps in heatmaps
pheatmap(test, annotation_col = annotation_col, cluster_rows = FALSE, gaps_row = c(10, 14))
pheatmap(test, annotation_col = annotation_col, cluster_rows = FALSE, gaps_row = c(10, 14),
         cutree_col = 2)

# Show custom strings as row/col names
labels_row = c("", "", "", "", "", "", "", "", "", "", "", "", "", "", "",
"", "", "Il10", "Il15", "Il1b")

pheatmap(test, annotation_col = annotation_col, labels_row = labels_row)

# Specifying clustering from distance matrix
drows = dist(test, method = "minkowski")
dcols = dist(t(test), method = "minkowski")
pheatmap(test, clustering_distance_rows = drows, clustering_distance_cols = dcols)

# Modify ordering of the clusters using clustering callback option
callback = function(hc, mat){
    sv = svd(t(mat))$v[,1]
    dend = reorder(as.dendrogram(hc), wts = sv)
    as.hclust(dend)
}

pheatmap(test, clustering_callback = callback)

## Not run:
# Same using dendsort package
library(dendsort)

callback = function(hc, ...){dendsort(hc)}
pheatmap(test, clustering_callback = callback)

## End(Not run)
```

insect taxonomic assignment software

```{r}
# install.packages("insect")
library(insect)
```

files are stored in working directory

```{r}
URL <- "https://www.dropbox.com/s/aawh33hneqru6j9/metazoan_COI_marine_v3.zip?dl=1"
download.file(URL, destfile = "metazoan_COI_marine_v3.zip", mode = "wb")
unzip("metazoan_COI_marine_v3.zip")
file.remove("metazoan_COI_marine_v3.zip")
```

```{r}
S1R1 <- readFASTQ("COI_sample1_read1.fastq")
S1R2 <- readFASTQ("COI_sample1_read2.fastq")
S2 <- readFASTQ("COI_sample2.fastq")
```

```{r}
S1 <- stitch(S1R1, S1R2, up = "GGWACWGGWTGAACWGTWTAYCCYCC",
             down = "TAIACYTCIGGRTGICCRAARAAYCA")
```

```{r}
S2 <- trim(S2, up = "GGWACWGGWTGAACWGTWTAYCCYCC",
             down = "TAIACYTCIGGRTGICCRAARAAYCA")
```

```{r}
names(S1) <- paste0("Sample1_", names(S1))
names(S2) <- paste0("Sample2_", names(S2))
x <- c(S1, S2)
```

```{r}
x <- qfilter(x, minlength = 250, maxlength = 350)
```

```{r}
tree <- readRDS("classification_tree.rds")
```

```{r}
longDF <- classify(x, tree, cores = 4)
```

```{r}
taxa <- aggregate(longDF[3:10], longDF["taxID"], head, 1)
counts <- aggregate(longDF[11:12], longDF["taxID"], sum)
shortDF <- merge(taxa, counts, by = "taxID")
```

```{r}
vignette("insect-vignette")
```

modelr R for Data Science Chapter 23 Model Basics

```{r}
library(tidyverse)
library(modelr)
options(na.action = na.warn)


# a simple model
ggplot(sim1, aes(x, y)) +
    geom_point()

models <- tibble(
    a1 = runif(250, -20, 40),
    a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) +
    geom_abline(
        aes(intercept = a1, slope = a2), data = models, alpha = 1/4
    ) +
    geom_point()

# fit models using the many slope and intercept values in models df
model1 <- function(a, data){
    a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)  # generates a bunch of y's for a given intercept a[1] and slope a[2], and the x values in sim1

# now we measure the model fit of c(7, 1.5) to the real y values in sim1

measure_distance <- function(mod, data){
    diff <- data$y - model1(mod, data)
    sqrt(mean(diff^2)) # mean sum of squares, square rooted
}

measure_distance(c(7, 1.5), sim1) # computes MSS from model with slope 7 and intercept 1.5

# helper function
sim1_dist <- function(a1, a2) {
    measure_distance(c(a1, a2), sim1)
}

# apply measure_distance function to two vectors, a1 and a2, which are used to calculate y, and then the distance to the data$y is calculated
# models <- models %>% dplyr::select(-dist)

models <- models %>%
    mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

ggplot(sim1, aes(x, y)) +
    geom_point(size = 2, color = "grey30") +
    geom_abline(
        aes(intercept = a1, slope = a2, color = -dist),
        data = filter(models, rank(dist) <= 10)
    )

# instead of using random intercepts and slopes, create a regular grid

grid <- expand.grid(
  a1 = seq(-5, 20, length = 25), # length = 25 asks for a sequence of 25 digits
  a2 = seq(1, 3, length = 25)
  ) %>%
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
# equivalent of grid %>% mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
# grid is the dataframe that has a1 and a2

grid %>%
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))

ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, colour = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist),
    data = filter(grid, rank(dist) <= 10) # plot only points that are in the top 10 fitted
  )

# Newton-Raphson optimisation
measure_distance <- function(mod, data){
    diff <- data$y - model1(mod, data)
    sqrt(mean(diff^2)) # mean sum of squares, square rooted
}
best <- optim(c(0,0), measure_distance, data = sim1)
# optim(par, fn, ) # par == initial values for the parameters that will be optimised; fn == function to be minimised
best$par # optimised parameters

ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, colour = "grey30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])

# linear model lm() to optimise
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)

# Exercises
# 23.2.1.1
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2) # random deviates from the t-distribution
)
sim1a_mod <- lm(y ~ x, data = sim1a)
coef(sim1a_mod)
ggplot(sim1a, aes(x, y)) +
    geom_point(size = 2, colour = "grey30") +
    geom_abline(intercept = coef(sim1a_mod)[[1]], slope = coef(sim1a_mod)[[2]])

# 23.2.1.2
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

best <- optim(c(0,0), measure_distance, data = sim1)
best$par # optimised parameters

ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, colour = "grey30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])

# 23.2.1.3
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
best <- optim(c(10,10,10), measure_distance, data = sim1)
best$par # optimised parameters
# the final (best) parameters are dependent on the starting numbers when there are 3 parameters to estimate



# Predictions
# create evenly spaced grid where our data lie
sim1
grid <- sim1 %>%
    data_grid(x)  # sim1 has 2 variables, x and y. This returns the range of x in sim1
grid

# linear model lm() to optimise
sim1_mod <- lm(y ~ x, data = sim1)

# add predicted values
grid <- grid %>%
    add_predictions(sim1_mod) # run x through fitted model and generate y

# plot
ggplot(sim1, aes(x = x)) + # take x from sim1 dataset (x is now avail to both geoms below)
    geom_point(aes(y = y)) + # take y from sim1 dataset (x is from sim1)
    geom_line(aes(y = pred), data = grid, colour = "red", size = 1) # take y from grid dataset (x is from sim1)

# residuals
sim1 <- sim1 %>%
    add_residuals(sim1_mod)  # use original dataset to calculate residuals from the model, because sim1 has the observed y values, and sim1_mod produces the predicted y values

# plot residuals distribution
ggplot(sim1, aes(resid)) +  # single variable
    geom_freqpoly(binwidth = 0.5)

# plot residuals by predictor
ggplot(sim1, aes(x = x, y = resid)) +
    geom_ref_line(h = 0) +  # h means horizontal, from modelr package
    geom_point() # points using aes from ggplot

# use loess instead of lm()
sim1
grid <- sim1 %>%
    data_grid(x)  # sim1 has 2 variables, x and y. This returns the range of x in sim1
grid

sim1_mod2 <- loess(y ~ x, data = sim1)
grid <- grid %>%
    add_predictions(sim1_mod2)
ggplot(sim1, aes(x = x)) + # take x from sim1 dataset (x is now avail to both geoms below)
    geom_point(aes(y = y)) + # take y from sim1 dataset (x is from sim1)
    geom_line(aes(y = pred), data = grid, colour = "red", size = 4) + # take y from grid dataset (x is from sim1)
    geom_smooth(aes(y = y), data = sim1)

# or geom_smooth() by itself
ggplot(sim1, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth()

```

gather\_predictions and spread\_predictions modelr functions that add
predictions (y's) from multiple models to a dataframe

```{r}
df <- tibble::data_frame(
  x = sort(runif(100)),
  y = 5 * x + 0.5 * x ^ 2 + 3 + rnorm(length(x))
)
plot(df)

m1 <- lm(y ~ x, data = df)
grid <- data.frame(x = seq(0, 1, length = 10))
grid %>% add_predictions(m1)

m2 <- lm(y ~ poly(x, 2), data = df)
grid %>% spread_predictions(m1, m2)  # x, m1, m2
grid %>% gather_predictions(m1, m2)  # model(m1, m2), x, predictions
```

model matrix and categorical variables

```{r}
df <-tribble ( ~ y, ~ x1, ~ x2,
               4, 2, 5,
               5, 1, 6)
model_matrix(df, y ~ x1)
# A tibble: 2 x 2
#   `(Intercept)`    x1  # intercept is just a column full of ones, x1 is the x1 values 2,1
#           <dbl> <dbl>
# 1             1     2
# 2             1     1

model_matrix(df, y ~ x1 - 1) # removes the intercept column
model_matrix(df, y ~ x1 + x2)
# A tibble: 2 x 3
#   `(Intercept)`    x1    x2
#           <dbl> <dbl> <dbl>
# 1             1     2     5
# 2             1     1     6

df <- tribble(
    ~sex, ~response,
    "male", 1,
    "female", 2,
    "male", 1
)
model_matrix(df, response ~ sex)
# # A tibble: 3 x 2
#   `(Intercept)` sexmale  # dummy variable, aliased to male
#           <dbl>   <dbl>
# 1             1       1  
# 2             1       0
# 3             1       1

ggplot(sim2) +
    geom_point(aes(x, y))

mod2 <- lm(y ~ x, data = sim2) # fit a model

grid <- sim2 %>%
    data_grid(x) %>% # generate an evenly spaced set of predictors
    add_predictions(mod2) # add predicted values from fitted model
grid

ggplot(sim2, aes(x = x)) +
    geom_point(aes(y = y)) +
    geom_point(data = grid, aes(y = pred), color = "red", size = 4) # need to say that data = grid to be able to refer to the 'pred' column

# interactions
ggplot(sim3, aes(x1, y)) +
    geom_point(aes(color = x2))

mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
summary(mod2)

# add predictions from multiple models
grid <- sim3 %>%
    data_grid(x1, x2) %>% # finds all unique values of x1 and x2 and generates all combinations
    gather_predictions(mod1, mod2)

ggplot(sim3, aes(x1, y, color = x2)) +
    geom_point() +
    geom_line(data = grid, aes(y = pred)) + # x is from the global x, y is the model prediction
    facet_wrap(~ model)

# add residuals from multiple models
sim3 <- sim3 %>%
    gather_residuals(mod1, mod2) # added to sim3, not to a new grid. why?

ggplot(sim3, aes(x1, resid, color = x2)) +
    geom_point() +
    facet_wrap(model ~ x2) # the residuals for all four levels of x1 are better in mod2 than in mod1

# interactions for 2 continuous variables
sim4 <- sim4
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

sim4 <- sim4 %>%
    gather_residuals(mod1, mod2)

grid <- sim4 %>%
    data_grid(x1 = seq_range(x1, 5),  # generate 5 evenly spaced values over x1's range
              x2 = seq_range(x2, 5),
              ) %>%
    gather_predictions(mod1, mod2)

grid
# # A tibble: 50 x 4
#    model    x1    x2   pred
#    <chr> <dbl> <dbl>  <dbl>
#  1 mod1   -1    -1    0.996
#  2 mod1   -1    -0.5 -0.395
#  3 mod1   -1     0   -1.79
#  4 mod1   -1     0.5 -3.18
#  5 mod1   -1     1   -4.57
#  6 mod1   -0.5  -1    1.91
#  7 mod1   -0.5  -0.5  0.516
#  8 mod1   -0.5   0   -0.875
#  9 mod1   -0.5   0.5 -2.27
# 10 mod1   -0.5   1   -3.66
# # … with 40 more rows

# seq_range options
# pretty makes nicer values
seq_range(c(0.123, 0.923423), 5)
# [1] 0.1230000 0.3231057 0.5232115 0.7233173 0.9234230
seq_range(c(0.123, 0.923423), 5, pretty = TRUE)
# [1] 0.0 0.2 0.4 0.6 0.8 1.0

# trim and expand trim and expand tail values to concentrate values toward the center of the range
x1 <- rcauchy(100)
seq_range(x1, 5)
seq_range(x1, 5, trim = 0.1)
seq_range(x1, 5, trim = 0.5)
x1 <- c(1,0)
seq_range(x1, 5)
seq_range(x1, 5, expand = 0.5)

# visualise a model with two continuous variables
# use a geom_tile(), but can't see much difference
ggplot(grid, aes(x1, x2)) +
    geom_tile(aes(fill = pred)) +
    facet_wrap(~ model)
# try geom_line
ggplot(grid, aes(x = x1, y = pred, color = x2, group = x2)) +
    geom_line() +
    facet_wrap(~ model)

ggplot(grid, aes(x = x2, y = pred, color = x1, group = x1)) +
    geom_line() +
    facet_wrap(~ model)
```

Transformations and I()

```{r}
df <- tribble(  # tibble::tribble is row-wise tibble creation
    ~y, ~x,
    1, 1,
    2, 2,
    3, 3
)

# can use transformations inside equations
# e.g. sqrt(x), x^2
# transformations that involve '+, ^, *, or -' need to be wrapped in I() or R will interpret them as operations that are expanded. e.g. x^2 is interpreted first to x*x, which is the interaction of x with itself, which is just x. so x^2 becomes just x
model_matrix(y ~ x^2 + x, data = df) # wrong
model_matrix(y ~ I(x^2) + x, data = df) # I() wrapper to ensure that x^2 is interpreted as the square of x

model_matrix(y ~ poly(x, 2), data = df) # equiv of x + x^2 + x^3 (2 polynomial terms)

# alternative to poly() is splines::ns()
library(splines)
model_matrix(y ~ ns(x, 2), data = df)

# create a nonlinear dataset using tibble
sim5 <- tibble(
    x = seq(0, 3.5 * pi, length = 50),
    y = 4 * sin(x) + rnorm(length(x))
)

ggplot(sim5, aes(x, y)) +
    geom_point()

# now fit 5 models to these data
library(splines)
mod1 <- lm(y ~ ns(x, 1), data = sim5)
mod2 <- lm(y ~ ns(x, 2), data = sim5)
mod3 <- lm(y ~ ns(x, 3), data = sim5)
mod4 <- lm(y ~ ns(x, 4), data = sim5)
mod5 <- lm(y ~ ns(x, 5), data = sim5)

grid <- sim5 %>%
    data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>%
    gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = "y") # .pred = "y" calls the predictor column "y" instead of "pred"

ggplot(sim5, aes(x, y)) +
    geom_point() +
    geom_line(data = grid, color = "red") +
    facet_wrap(~ model)
```

Missing values usually missing values cause row-wise deletion To warn of
this, set an option

```{r}
# options(na.action = na.warn)
# and can set na.exclude inside a command
mod <- lm(y ~ x, data = df, na.action = na.exclude)

nobs(mod) # reports how many observations were used in a model
```

Buffalo Chapter 19. Model Building visualise data with a figure and find
the biggest pattern make observed patterns precise with a model repeat
but replace old response variable with residuals from the model

```{r}
library(tidyverse)
library(modelr)
options(na.action = na.warn)

library(nycflights13)
library(lubridate)

diamonds <- diamonds
ggplot(diamonds, aes(x = cut, y = price)) + geom_boxplot()
ggplot(diamonds, aes(x = color, y = price)) + geom_boxplot()
ggplot(diamonds, aes(x = clarity, y = price)) + geom_boxplot()

ggplot(diamonds, aes(carat, price)) +
    geom_hex(bins = 50)

diamonds2 <- diamonds %>%
    filter(carat <= 2.5) %>%
    mutate(lprice = log2(price), lcarat = log2(carat))

ggplot(diamonds2, aes(lcarat, lprice)) +
    geom_hex(bins = 50)

mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)

# now make an overlay of the fitted model mod_diamond and the data, using carat and price as the axes
grid <- diamonds2 %>%
    data_grid(carat = seq_range(carat, n = 50)) %>%
    mutate(lcarat = log2(carat)) %>%
    add_predictions(mod_diamond, var = "lprice") %>% # "lprice" instead of default "pred"
    mutate(price = 2^(lprice))
# this creates a grid that has carat, price, lcarat and lprice.  lprice is predicted by lcarat using the fitted model mod_diamond, which uses lcarat and lprice

ggplot(diamonds2, aes(x = carat, y = price)) +
    geom_hex(bins = 50) +
    geom_line(data = grid, color = "red", size = 1) # uses the global "aes(x = carat, y = price)" but now, these variables are taken from the grid dataframe, where they represent only the fitted model

diamonds2 <- diamonds2 %>%
    add_residuals(mod_diamond, "lresid") # because the model uses lprice ~ lcarat

ggplot(diamonds2, aes(x = lcarat, y = lresid)) +
    geom_hex(bins = 50) # effect of carat on price has been removed because there is no relationship between lcarat and the residuals

ggplot(diamonds2, aes(cut, lresid)) + geom_boxplot() # cut has an effect on residuals
ggplot(diamonds2, aes(color, lresid)) + geom_boxplot() # cut has an effect on residuals
ggplot(diamonds2, aes(clarity, lresid)) + geom_boxplot() # cut has an effect on residuals
# the residuals here mean price after the effect of carat has been removed. Negative residuals mean that the price is lower than expected for its size (number of carats)

# a more complicated model with multiple predictors, one continuous, three categorical
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)

# .model fills in datagrid with typical values of the non-stated predictors (not a range, just a single, typical value). This is a convenience function
grid <- diamonds2 %>%
    data_grid(cut, color, .model = mod_diamond2) %>%
    add_predictions(mod_diamond2)
grid

# because only cut has multiple values, i only make a graph with cut as x
ggplot(grid, aes(cut, pred)) +
    geom_point(aes(color = color))

diamonds2 <- diamonds2 %>%
    add_residuals(mod_diamond2, var = "lresid")

ggplot(diamonds2, aes(lcarat, lresid)) +
    geom_hex(bins = 50)

# extract diamonds with the biggest residuals, add pred prices from mod_diamond2, and sort to see if there are some underpriced (and overpriced) diamonds
diamonds2 %>%
    filter(abs(lresid) >= 1.0) %>%
    add_predictions(mod_diamond2) %>%
    mutate(pred_price = round(2^pred)) %>%
    mutate(price_diff = pred_price - price) %>%
    select(price_diff, price, pred_price, carat:table, everything()) %>%
    arrange(desc(price_diff))
```

```{r}
library(lubridate)

start <- as_date("2021-05-10")
end <- start %m+% months(8) # add 8 months 
end
```

NYCFlights

```{r}
# make a simple dataset of number of flights per day
library(nycflights13)
library(modelr)
daily <- flights %>%
    mutate(date = make_date(year, month, day)) %>%
    group_by(date) %>%
    summarise(n = n()) %>%
    mutate(wday = wday(date, label = TRUE))

ggplot(daily, aes(wday, n)) +
    geom_point()

ggplot(daily, aes(wday, n)) +
    geom_boxplot() +
    geom_jitter()

mod <- lm(n ~ wday, data = daily)

grid <- daily %>%
    data_grid(wday) %>%
    add_predictions(mod, "n")

ggplot(daily, aes(wday, n)) +
    geom_boxplot() +
    geom_point(data = grid, aes(y = n), color = "red", size = 4) # wday is already in the boxplot, so i can omit aes():  geom_point(data = grid, color = "red", size = 4)

# now that the effect of weekday has been removed, i plot resid ~ date, and the early part of the year is a bit low, and there are still major low saturdays on holidays
daily <- daily %>%
    add_residuals(mod)

ggplot(daily, aes(date, resid)) +
    geom_ref_line(h = 0) +
    geom_line()

ggplot(daily, aes(date, resid, color = wday)) +
    geom_ref_line(h = 0) +
    geom_line()

daily %>% filter(resid < -100) # some of the most negative residuals are public holidays

daily %>%
    ggplot(aes(date, resid)) +
    geom_ref_line(h = 0) +
    geom_line(color = "grey50") +
    geom_smooth(se = FALSE, span = 0.20)

# Saturdays only
daily %>%
    filter(wday == "Sat") %>%
    ggplot(aes(date, n)) +
    geom_point() +
    geom_line() +
    scale_x_date(
        NULL,
        date_breaks = "1 month",
        date_labels = "%b"
        )

# create a variable that breaks up the year into three school terms
# cut() divides a range into intervals and labels the intervals as levels of a factor
term <- function(date) {
    cut(date,
        breaks = ymd(20130101, 20130605, 20130825, 20140101),
        labels = c("spring", "summer", "fall")
        )
}
# term(daily$date)

daily <- daily %>%
    mutate(term = term(date))

daily %>%
    filter(wday == "Sat") %>%
    ggplot(aes(date, n, color = term)) +
    geom_point(alpha = 1/3) +
    geom_line() +
    scale_x_date(
        NULL,
        date_breaks = "1 month",
        date_labels = "%b"
        )

daily %>%
    ggplot(aes(wday, n, color = term)) +
    geom_boxplot()

mod1 <- lm(n ~ wday, data = daily)
mod2 <- lm(n ~ wday * term, data = daily)

# with/out_term refers to the model variable term
# including the term variable
daily %>%
    gather_residuals(without_term = mod1, with_term = mod2) %>%
    ggplot(aes(date, resid, color = model)) +
    geom_line(alpha = 0.75)

grid <- daily %>%
    data_grid(wday, term) %>%  # use the same variables as used in mod1, mod2
    add_predictions(mod1, var = "n")

daily %>%
    ggplot(aes(x = wday, y = n)) +
    geom_boxplot() +
    geom_point(data = grid, aes(y = n), color = "red", size = 4) +  # aes(y = n) is optional
    facet_wrap(~ term)

mod3 <- MASS::rlm(n ~ wday * term, data = daily) # more robust b/c it ignores outliers
daily %>%
    add_residuals(mod3) %>%
    ggplot(aes(date, resid)) +
    geom_ref_line(h = 0) +
    geom_line(alpha = 0.75)

# splines
library(splines)
mod <- MASS::rlm(n ~ wday * ns(date, 5), data = daily) # generate a spline
# ns(daily$date, 5)

daily %>%
    data_grid(wday, date = seq_range(date, n = 13)) %>% # wday & date are in the mod
    add_predictions(mod) %>%
    ggplot(aes(date, pred, color = wday)) +
    geom_line() +
    geom_point()


# Summary of the above
# Fit a model: mod
# Generate a data grid that evenly covers the ranges of all predictor variables:  data_grid()
# add_predictors(model = mod)) and add_residuals(model = mod) (or gather_predictors() and gather residuals()) to the data_grid
# ggplot geom_points(data = data) to show the data + geom_line(data = grid) to overlay the model

# example
library(tidyverse)
library(modelr)
options(na.action = na.warn)

diamonds2 <- diamonds %>%
    filter(carat <= 2.5) %>%
    mutate(lprice = log2(price), lcarat = log2(carat))

mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)

grid <- diamonds2 %>%
    data_grid(lcarat = seq_range(lcarat, n = 20)) %>%
    mutate(carat = 2^lcarat) %>%
    add_predictions(mod_diamond, var = "lprice") %>%
    mutate(price = 2^lprice)

ggplot(data = diamonds2, aes(x = carat, y = price)) +
    geom_hex(bins = 50) +
    geom_line(data = grid, color = "red", size = 1)

diamonds2 <- diamonds2 %>%
    add_residuals(model = mod_diamond, var = "lresid")

ggplot(data = diamonds2, aes(x = lcarat, y = lresid)) +
    geom_hex(bins = 50)

# remove the few really large diamonds
diamonds2 <- diamonds %>%
    filter(carat <= 2.5) %>%
    mutate(lprice = log2(price), lcarat = log2(carat))

ggplot(diamonds2, aes(lcarat, lprice)) +
    geom_hex(bins = 50)

mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)

# now make an overlay of the fitted model mod_diamond and the data, using carat and price as the axes
# the only predictor in mod_diamond is lcarat, but we want a data_grid that also has carat and price, because we want to plot the raw variables, not the transformed variables
grid <- diamonds2 %>%
    data_grid(carat = seq_range(carat, n = 50)) %>% # use seq_range() to create the evenly spaced range of this continuous variable
    mutate(lcarat = log2(carat)) %>% # calculate lcarat as input to mod_diamond
    add_predictions(mod_diamond, var = "lprice") %>% # "lprice" instead of default "pred"
    mutate(price = 2^(lprice)) # calculate price from predicted lprice
# this creates a grid that has carat, price, lcarat and lprice.  lprice is predicted by lcarat using the fitted model mod_diamond, which uses lcarat and lprice

ggplot(diamonds2, aes(x = carat, y = price)) +
    geom_hex(bins = 50) +  # show the original data
    geom_line(data = grid, color = "red", size = 1) # overlay the model, using points from grid. this can use the global "aes(x = carat, y = price)" but now, these variables are taken from the grid dataframe, where they represent only the fitted points

diamonds2 <- diamonds2 %>%
    add_residuals(mod_diamond, "lresid") # because the model uses lprice ~ lcarat

ggplot(diamonds2, aes(x = lcarat, y = lresid)) +
    geom_hex(bins = 50) # effect of carat on price has been removed because there is no relationship between lcarat and the residuals

# create more complex model with four predictors
mod_diamond2 <- lm(lprice ~ lcarat + cut + color + clarity, data = diamonds2)

# create data_grid with cut and color and one typical value for lcarat and clarity, get predicted lprice values, and back-transform to get price
# NB the predictions are lprice because the model uses lprice ~ lcarat
grid <- diamonds2 %>%
    data_grid(cut, color, .model = mod_diamond2) %>%
    add_predictions(mod_diamond2, "lprice") %>%
    mutate(price = 2^lprice, carat = 2^lcarat)

# scatterplot of predicted prices as a function of cut and color for typical values of lcarat and
ggplot(data = grid, aes(x = cut, y = price, color = color)) +
    geom_point()

ggplot(data = grid, aes(x = color, y = price, color = cut)) +
    geom_point()

# if i just want to use black for the different lines, just use 'group = cut'
ggplot(data = grid, aes(x = color, y = price, group = cut)) +
    geom_point()

```

Many models with purrr and broom

```{r}
library(tidyverse)
library(modelr)
library(broom)
library(gapminder)
gapminder # each country has 12 rows of data
# # A tibble: 1,704 x 6
#    country     continent  year lifeExp      pop gdpPercap
#    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>
#  1 Afghanistan Asia       1952    28.8  8425333      779.
#  2 Afghanistan Asia       1957    30.3  9240934      821.
#  3 Afghanistan Asia       1962    32.0 10267083      853.
#  ...

ggplot(gapminder, aes(year, lifeExp, group = country, color = continent)) +
    geom_line(alpha = 1/3)

by_country <- gapminder %>%
    group_by(country, continent) %>%
    nest()
by_country # the data for each country is now nested inside the data column (12 rows x 4 cols)
# # A tibble: 142 x 3
#    country     continent data             
#    <fct>       <fct>     <list>           
#  1 Afghanistan Asia      <tibble [12 × 4]>
#  2 Albania     Europe    <tibble [12 × 4]>
#  3 Algeria     Africa    <tibble [12 × 4]>
#  ...

# by_country$data # shows all 142 tibbles!
by_country$data[[1]] # data from Afghanistan

# create a function
country_model <- function(df) {
    lm(lifeExp ~ year, data = df)
}

# models <- map(by_country$data, country_model) # this just saves a bunch of models
# but actually, i want each country's model in the same dataframe as its original data
by_country <- by_country %>%
    mutate(model = map(by_country$data, country_model))
# model is now the fourth column, each row holding the lm() model object
by_country
# # A tibble: 142 x 4
#    country     continent data              model   
#    <fct>       <fct>     <list>            <list>  
#  1 Afghanistan Asia      <tibble [12 × 4]> <S3: lm>
#  2 Albania     Europe    <tibble [12 × 4]> <S3: lm>
#  3 Algeria     Africa    <tibble [12 × 4]> <S3: lm>
# ...

by_country %>% filter(continent == "Europe")
by_country %>% arrange(continent, country)

# now add residuals
by_country <- by_country %>%
    mutate(resid = map2(data, model, add_residuals))
# note that add_residuals() has this syntax:  add_residuals(data, model, var = "resid"), so map2 takes the first two arguments and inserts them into this command.
by_country

by_country$resid[[1]] # The content of the resids column includes the data.  I don't know why, but it's convenient
# to plot, we need to un-nest
resids <- unnest(by_country, resid) # the 'resid' in the second argument means that the resid column in by_country is unnested, and the data and model columns are just dropped
# note that if the resids column had not contained the original data, i could have instead run
# resid <- unnest(by_country, data, resid) # if i run this, i get two sets of data columns
resids
# # A tibble: 1,704 x 7
#    country     continent  year lifeExp      pop gdpPercap   resid
#    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>   <dbl>
#  1 Afghanistan Asia       1952    28.8  8425333      779. -1.11  
#  2 Afghanistan Asia       1957    30.3  9240934      821. -0.952
#  3 Afghanistan Asia       1962    32.0 10267083      853. -0.664
# ...

# here, aes(group = country) can be put into just geom_line(), so that geom_smooth uses the global aes()
resids %>% ggplot(aes(x = year, y = resid)) +
    geom_line(aes(group = country), alpha = 1/3) +
    geom_smooth(se = FALSE) +
    facet_wrap(~ continent)
# The lines are mostly just flat (i.e. year explains most of the variance in lifeExp), except for the African countries

# broom
nz <-filter ( gapminder , country == "New Zealand" )
nz %>% ggplot ( aes ( year , lifeExp )) +
    geom_line () +
    ggtitle ( "Full data = " )
nz_mod <-lm ( lifeExp ~ year , data = nz )

nz %>% add_predictions ( nz_mod ) %>%
    ggplot ( aes ( year , pred )) +
    geom_line () +
    ggtitle ( "Linear trend + " )

nz %>% add_residuals ( nz_mod ) %>%
    ggplot ( aes ( year , resid )) +
    geom_hline ( yintercept = 0 , color = "white" , size = 3 ) +
    geom_line () +
    ggtitle ( "Remaining pattern" )

nz_mod
broom::glance(nz_mod) # model summary in a single row
broom::tidy(nz_mod) # table of model coefficients and their errors and p-values
broom::augment(nz_mod, nz) # adds the fitted, se.fitted, residuals, etc. to each row in the data that was used in the model

names(by_country)
# model column holds the year lm() for each country
# for all the countries at once
glance <- by_country %>% mutate(glance = map(model, glance)) %>%
    unnest(glance, .drop = TRUE)
# creates tibble with the model results for each country
# .drop = TRUE means drop the list items from the tibble:  data, model, resid
glance

glance %>% arrange(r.squared) # all the low r.squared are in Africa
glance %>%
    ggplot(aes(continent, r.squared)) +
    geom_boxplot() +
    geom_jitter(width = 0.5)

bad_fit <- filter(glance, r.squared < 0.25)
# joint to gapminder to get the year and lifeExp data and then make the life expectancy by year graph
gapminder %>% semi_join(bad_fit, by = "country") %>%
    ggplot(aes(year, lifeExp, color = country)) +
    geom_line()

```

Make a directory (mkdir) in the file system, first checking if the
directory exists
<https://stackoverflow.com/questions/4216753/check-existence-of-directory-and-create-if-doesnt-exist>

```{r}
mainDir = "/Users/Negorashi2011/Dropbox/Working_docs/Roslin_Greenland/2018/SPIKEPIPE/Rcode"
subDir = "testdir"

# the working directory is part of the R environment and should be controlled by the user, not a script. Scripts should, ideally, not change the R environment. To address this problem, I might use options() to store a globally available directory where I wanted all of my output.
# someUniqueTag is just a programmer-defined prefix for the option name, which makes it unlikely that an option with the same name already exists. (For instance, if you were developing a package called "filer", you might use filer.mainDir and filer.subDir).
options(someUniqueTag.mainDir = mainDir)
options(someUniqueTag.subDir = "subDir")

if (!file_test("-d", file.path(mainDir, subDir))) {  # test if directory already exists "-d"
  if(file_test("-f", file.path(mainDir, subDir))) {  # test if file already exists "-f"
    stop("Path can't be created because a file with that name already exists.")
  } else {
    dir.create(file.path(mainDir, subDir)) # mkdir
  }
}

# Then, in any subsequent script that needed to manipulate a file in subDir, you might use something like:
mainDir = getOption(someUniqueTag.mainDir)
subDir = getOption(someUniqueTag.subDir)
filename = "fileToBeCreated.txt"
file.create(file.path(mainDir, subDir, filename))
```

```{r grateful}
# to install
library(devtools)
install_github("Pakillo/grateful")

# https://citationstyles.org/authors/#/finding-and-installing-styles
# Tips for browsing the GitHub CSL Style Repository:  https://github.com/citation-style-language/styles
# To quickly search the styles in the GitHub CSL style repository by file name, go to URL https://github.com/citation-style-language/styles, and press “t” to activate GitHub’s File Finder and start typing. E.g. type "ecology", and one of the surfaced options will be "ecology-letters"
# use this filename in the style = "" option

library(grateful)
grateful::cite_packages() # creates html file and *.bib file by default
grateful::cite_packages(style = "ecology-letters", out.format = "docx") # downloads the csl style
grateful::cite_packages(style = "the-american-naturalist", out.format = "md")
grateful::cite_packages(style = "ecology", out.format = "pdf")
```

Compositional analysis of OTU data
<https://albertsenlab.org/what-is-wrong-with-correlating-relative-abundance-everything/>

```{r}
library(mmtravis)
library(tidyverse)
library(magrittr)

mmt <- readRDS("~/src/mmtravis_supp/mt_example.rds")
mmt_sub <- mt_subset(mmt, minreads = 50, normalise = "TPM") # "TPM": Normalise read counts to Transcripts Per Milion (TPM).

mmt_phi <- mt_phi(mmt_sub)
mmt_cor <- mmt_sub$mtdata %>% column_to_rownames("GeneID") %>% t() %>% cor()

phi_dist <- as.dist(mmt_phi)
cor_dist <- as.dist(1 - abs(mmt_cor))

phi_clst <- hclust(phi_dist)
cor_clst <- hclust(cor_dist)

par(mfrow = c(1,2),mar = c(1,1,1,1))

image(
  mmt_phi[phi_clst$order,phi_clst$order],
  breaks    = quantile(mmt_phi,seq(0,1,0.1)),
  col       = (heat.colors(10)),
  useRaster = TRUE,
  yaxt = "n",
  xaxt = "n",
  asp  = 1,
  bty  = "n",main = expression(phi~statistic))

image(
  mmt_cor[cor_clst$order,cor_clst$order],
  breaks    = quantile(mmt_cor,seq(0,1,0.1)),
  col       = (heat.colors(10)),
  useRaster = TRUE,
  yaxt = "n",
  xaxt = "n",
  asp  = 1,
  bty  = "n",main = expression(Correlation~rho))
```

fopenmp in R To install alexpghayes/hayeslib requires fopenmp to run
under R, which is not supported using Apple's clang

Do these two steps: 1. go to <https://cran.r-project.org> \# download
and install gfortran 6.1 and clang 6.0.0 using the GUI installers 2.
open .R/Makevars in Atom and add these lines:

# The following statements are required to use the clang6 binary

CC=/usr/local/clang6/bin/clang CXX=/usr/local/clang6/bin/clang++
CXX11=/usr/local/clang6/bin/clang++ CXX14=/usr/local/clang6/bin/clang++
CXX17=/usr/local/clang6/bin/clang++ CXX1X=/usr/local/clang6/bin/clang++
LDFLAGS=-L/usr/local/clang6/lib \# End clang6 inclusion statements

corncob <https://github.com/bryandmartin/corncob/>
<https://arxiv.org/abs/1902.02776> Modeling microbial abundances and
dysbiosis with beta-binomial regression

Bryan D. Martin, Daniela Witten, Amy D. Willis (Submitted on 7 Feb 2019)
Using a sample from a population to estimate the proportion of the
population with a certain category label is a broadly important problem.
In the context of microbiome studies, this problem arises when
researchers wish to use a sample from a population of microbes to
estimate the population proportion of a particular taxon, known as the
taxon's relative abundance. In this paper, we propose a beta-binomial
model for this task. Like existing models, our model allows for a
taxon's relative abundance to be associated with covariates of interest.
However, unlike existing models, our proposal also allows for the
overdispersion in the taxon's counts to be associated with covariates of
interest. We exploit this model in order to propose tests not only for
differential relative abundance, but also for differential variability.
The latter is particularly valuable in light of speculation that
dysbiosis, the perturbation from a normal microbiome that can occur in
certain disease conditions, may manifest as a loss of stability, or
increase in variability, of the counts associated with each taxon. We
demonstrate the performance of our proposed model using a simulation
study and an application to soil microbial data.

```{r}
# install.packages("devtools")
devtools::install_github("bryandmartin/corncob", build = TRUE)
library(corncob)
# Use this to view the vignette in the corncob HTML help
help(package = "corncob", help_type = "html")
# Use this to view the vignette as an isolated HTML file
utils::browseVignettes(package = "corncob")
```

```{r HMSC-R}
library(devtools)
install_github("hmsc-r/HMSC", build_opts = c("--no-resave-data", "--no-manual"))
# BayesLogit is a CRAN package, but requires a 3.6+ version of R

# To get started with the package, we recommend to start with reading the package documentation which can be found by typing help('Hmsc-package'), following the vignettes and reading the help pages for the Hmsc, HmscRandomLevel and sampleMcmc functions. The vignettes are available in the 'vignette' folder, or can be accessed from within R by typing e.g. vignette(topic = "vignette_1_univariate", package = "Hmsc"). To see a list of vignettes, type vignette(package = "Hmsc").

library(Hmsc) # might have to restart RStudio to get the package to load (even the help files)
help('Hmsc-package')
vignette(package = "Hmsc")
vignette(topic = "vignette_1_univariate", package = "Hmsc")


# test
# Construct an ordination biplot using two chosen latent factors from a previously fitted HMSC model
etaPost = getPostEstimate(TD$m, "Eta")
lambdaPost=getPostEstimate(TD$m, "Lambda")
biPlot(TD$m, etaPost = etaPost, lambdaPost=lambdaPost, factors=c(1,2))




```

Add labels to variable names The package can also add labels to vector
values add\_value\_labels will not remove already set labels

```{r}
library(labelled)
df1 <- data_frame(s1 = c("M", "M", "F"), s2 = c(1, 1, 2)) %>%
  set_variable_labels(s1 = "Sex", s2 = "Question") %>%
  set_value_labels(s1 = c(Male = "M", Female = "F"), s2 = c(Yes = 1, No = 2))
var_label(df1$s1) <- NULL # remove the variable label

x1 <- labelled_spss(1:10, c(Good = 1, Bad = 8), na_values = c(9, 10))
var_label(x1) <- "A variable"
x1

x2 <- remove_labels(x1)
x2
x3 <- remove_labels(x1, user_na_to_na = TRUE)
x3
x4 <- remove_user_na(x1, user_na_to_na = TRUE)
x4
```

Save multiple datasets into a single RData file, checking that the
filename doesn't already exist

```{r}
mainDir = "~/Dropbox/Working_docs/Kelpie/data_20190419/"
filename = "Kelpie_input_species_20190420.RData"

if(file_test("-f", file.path(mainDir, filename))) {  # test if file already exists "-f"
    stop("File can't be saved because a file with that name already exists.")
  } else {
    save(mockdf, mergedf2003, mergedf2004, mergedf2005, file = file.path(mainDir, filename))
  }

# load(file.path(mainDir, filename))
```

sjPlot

```{r}
# load package
library(sjPlot)
library(sjmisc)
library(sjlabelled)

# sample data
data("efc")
efc <- as_factor(efc, c161sex, c172code)

m1 <- lm(barthtot ~ c160age + c12hour + c161sex + c172code, data = efc)
m2 <- lm(neg_c_7 ~ c160age + c12hour + c161sex + e17age, data = efc)

tab_model(m1)

data(mtcars)
m.mtcars <- lm(mpg ~ cyl + hp + wt, data = mtcars)
tab_model(m.mtcars)
tab_model(m1, auto.label = FALSE)
tab_model(m1, m2)

m3 <- glm(
  tot_sc_e ~ c160age + c12hour + c161sex + c172code,
  data = efc,
  family = poisson(link = "log")
)

efc$neg_c_7d <- ifelse(efc$neg_c_7 < median(efc$neg_c_7, na.rm = TRUE), 0, 1)
m4 <- glm(
  neg_c_7d ~ c161sex + barthtot + c172code,
  data = efc,
  family = binomial(link = "logit")
)

tab_model(m3, m4)
tab_model(m3, m4, transform = NULL, auto.label = FALSE)
```

dada2 tutorial
<https://astrobiomike.github.io/amplicon/dada2_workflow_ex> send line to
terminal: opt-cmd-enter (enter on the numeric keypad) send line to
terminal 2: opt-cmd-fn-Return (function return on compact keyboards)

```{bash}
cd ~/src/
curl -L -o dada2_amplicon_ex_workflow.tar.gz https://ndownloader.figshare.com/files/15072638
tar -xzvf dada2_amplicon_ex_workflow.tar.gz
rm dada2_amplicon_ex_workflow.tar.gz
cd dada2_amplicon_ex_workflow/
"ls" *_R1.fq | cut -f1 -d "_" > samples # use "ls" instead of ls because i have aliased ls to something more complicated.
cutadapt --version # 2.3

# loop through samples to remove primer sequences from ends of paired-end reads
for sample in $(cat samples)
do
    echo "On sample: $sample"

    cutadapt -a ^GTGCCAGCMGCCGCGGTAA...ATTAGAWACCCBDGTAGTCC \
    -A ^GGACTACHVGGGTWTCTAAT...TTACCGCGGCKGCTGGCAC \
    -m 215 -M 285 --discard-untrimmed \
    -o ${sample}_sub_R1_trimmed.fq.gz -p ${sample}_sub_R2_trimmed.fq.gz \
    ${sample}_sub_R1.fq ${sample}_sub_R2.fq \
    >> cutadapt_primer_trimming_stats.txt 2>&1
done

# view output stats
paste samples <(grep "passing" cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")") <(grep "filtered" cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")")
```

open all\_R\_commands.R in the \~/src/dada2\_amplicon\_ex\_workflow/
folder

```{r}
setwd("~/src/dada2_amplicon_ex_workflow")
```

```{r delete lrRNA sequences suggested by sativa}
library(readr)
library(tidyverse)
MIDORI_lrRNA <- read_delim("~/src/screenforbio-mbc-dougwyu/MIDORI_lrRNA_sativa/MIDORI_lrRNA.mis", "\t", escape_double = FALSE, trim_ws = TRUE, skip = 5, col_names = c("SeqID", "MislabeledLevel","OriginalLabel","ProposedLabel","Confidence","OriginalTaxonomyPath","ProposedTaxonomyPath","PerRankConfidence"))
# the key is to skip the first set of lines
names(MIDORI_lrRNA)
MIDORI_lrRNA_sep <- MIDORI_lrRNA %>%
    tidyr::separate(PerRankConfidence, sep = ";", into = c("domain_conf", "phylum_conf", "class_conf", "order_conf", "family_conf", "genus_conf", "species_conf"), remove = FALSE)

# save file with SeqIDs of all sequences where sativa disagrees with its taxonomic designation at the ranks of class, order, family, or subtribe. To do this, i choose all rows where MislabeledLevel != Genus or Species. These are sequences to delete.
MIDORI_locus.mis_to_delete <- MIDORI_lrRNA_sep %>%
    filter(!MislabeledLevel %in% c("Genus", "Species")) %>%
    select(SeqID)
write_tsv(MIDORI_locus.mis_to_delete, "~/src/screenforbio-mbc-dougwyu/MIDORI_lrRNA_sativa/MIDORI_lrRNA.mis_to_delete", col_names = FALSE)

# substitute in sequences that have a sativa Confidence ≥ 0.99
MIDORI_lrRNA_sub <- MIDORI_lrRNA %>%
    filter(MislabeledLevel %in% c("Genus", "Species")) %>%
    filter(Confidence >= 0.998) %>%
    select(SeqID, ProposedTaxonomyPath)
Tetrapoda.final_taxonomy_sativa.txt <- read_delim("~/src/screenforbio-mbc-dougwyu/Tetrapoda.final_taxonomy_sativa.txt", "\t", escape_double = FALSE, col_names = c("SeqID", "ProposedTaxonomyPath"))

Tetrapoda.final_taxonomy_sativa.txt_full <- full_join(MIDORI_lrRNA_sub, Tetrapoda.final_taxonomy_sativa.txt, by = c("SeqID"))
Tetrapoda.final_taxonomy_sativa.txt_y <- Tetrapoda.final_taxonomy_sativa.txt_full %>%
    filter(is.na(ProposedTaxonomyPath.x)) %>%
    select(SeqID, ProposedTaxonomyPath = ProposedTaxonomyPath.y)
Tetrapoda.final_taxonomy_sativa.txt_x <- Tetrapoda.final_taxonomy_sativa.txt_full %>%
    filter(!is.na(ProposedTaxonomyPath.x)) %>%
    select(SeqID, ProposedTaxonomyPath = ProposedTaxonomyPath.x)

Tetrapoda.final_taxonomy_sativa.txt <- bind_rows(Tetrapoda.final_taxonomy_sativa.txt_x, Tetrapoda.final_taxonomy_sativa.txt_y) %>%
    arrange(SeqID)
write_tsv(Tetrapoda.final_taxonomy_sativa.txt, "~/src/screenforbio-mbc-dougwyu/Tetrapoda.final_taxonomy_sativa.txt", col_names = FALSE)



```

```{r delete srRNA sequences suggested by sativa}
library(readr)
library(tidyverse)
MIDORI_srRNA <- read_delim("~/src/screenforbio-mbc-dougwyu/MIDORI_srRNA_sativa/MIDORI_srRNA.mis", "\t", escape_double = FALSE, trim_ws = TRUE, skip = 5, col_names = c("SeqID", "MislabeledLevel","OriginalLabel","ProposedLabel","Confidence","OriginalTaxonomyPath","ProposedTaxonomyPath","PerRankConfidence"))
# the key is to skip the first set of lines
names(MIDORI_srRNA)
MIDORI_srRNA_sep <- MIDORI_srRNA %>%
    tidyr::separate(PerRankConfidence, sep = ";", into = c("domain_conf", "phylum_conf", "class_conf", "order_conf", "family_conf", "genus_conf", "species_conf"), remove = FALSE)

# save file with SeqIDs of all sequences where sativa disagrees with its taxonomic designation at the ranks of class, order, family, or subtribe. To do this, i choose all rows where MislabeledLevel != Genus or Species. These are sequences to delete.
MIDORI_locus.mis_to_delete <- MIDORI_srRNA_sep %>%
    filter(!MislabeledLevel %in% c("Genus", "Species")) %>%
    select(SeqID)
write_tsv(MIDORI_locus.mis_to_delete, "~/src/screenforbio-mbc-dougwyu/MIDORI_srRNA_sativa/MIDORI_srRNA.mis_to_delete", col_names = FALSE)

# substitute in sequences that have a sativa Confidence ≥ 0.99
MIDORI_srRNA_sub <- MIDORI_srRNA %>%
    filter(MislabeledLevel %in% c("Genus", "Species")) %>%
    filter(Confidence >= 0.998) %>%
    select(SeqID, ProposedTaxonomyPath)
Tetrapoda.final_taxonomy_sativa.txt <- read_delim("~/src/screenforbio-mbc-dougwyu/Tetrapoda.final_taxonomy_sativa.txt", "\t", escape_double = FALSE, col_names = c("SeqID", "ProposedTaxonomyPath"))

Tetrapoda.final_taxonomy_sativa.txt_full <- full_join(MIDORI_srRNA_sub, Tetrapoda.final_taxonomy_sativa.txt, by = c("SeqID"))
Tetrapoda.final_taxonomy_sativa.txt_y <- Tetrapoda.final_taxonomy_sativa.txt_full %>%
    filter(is.na(ProposedTaxonomyPath.x)) %>%
    select(SeqID, ProposedTaxonomyPath = ProposedTaxonomyPath.y)
Tetrapoda.final_taxonomy_sativa.txt_x <- Tetrapoda.final_taxonomy_sativa.txt_full %>%
    filter(!is.na(ProposedTaxonomyPath.x)) %>%
    select(SeqID, ProposedTaxonomyPath = ProposedTaxonomyPath.x)

Tetrapoda.final_taxonomy_sativa.txt <- bind_rows(Tetrapoda.final_taxonomy_sativa.txt_x, Tetrapoda.final_taxonomy_sativa.txt_y) %>%
    arrange(SeqID)
write_tsv(Tetrapoda.final_taxonomy_sativa.txt, "~/src/screenforbio-mbc-dougwyu/Tetrapoda.final_taxonomy_sativa.txt", col_names = FALSE)

```

# using tryCatch() to automatically indicate where classification() fails

# <https://rsangole.netlify.com/post/try-catch/>

```{r}
for (indx in 1:nrow(df_nested)) {
    tryCatch(
        expr = {
            df_nested[[indx, "data"]] <-  df_nested[[indx, "data"]] %>%
                convert_gear_to_factors() %>%
                transform_response_to_log()
            message("Iteration ", indx, " successful.")
        },
        error = function(e){
            message("* Caught an error on itertion ", indx)
            print(e)
        }
    )
}

# example in classification_misbehavers_finder.Rmd in github.com/dougwyu/screenforbio_mbc_ailaoshan
setwd("~/src/screenforbio-mbc-ailaoshan")
taxon <- "Tetrapoda"
mismatch<-read.table(paste0("MIDORI_",taxon,".ITIS_mismatch_sp.txt"),header=F,sep="\t", stringsAsFactors=FALSE)$V1 # read in text file of names as a character vector

outmessages <- character(length(mismatch)) # create null vector

# loop through each element of the mismatch vector, apply taxize::classification. If there is an error, the mismatch element is recorded in outmessages
for (indx in seq_along(length(mismatch))) {
    tryCatch(
        expr = {
            taxize::classification(mismatch[indx], db="col", return_id=FALSE, rows=1)
            message(mismatch[indx], " successful.")
            outmessages[indx] <- paste(mismatch[indx], " successful.")
        },
        warning = function(w){
            message("* Caught a warning on iteration ",  mismatch[indx])
            print(w)
            outmessages[indx] <- paste("* Caught a warning on ", mismatch[indx])
        },
        error = function(e){
            message("* Caught an error on iteration ",  mismatch[indx])
            print(e)
            outmessages[indx] <- paste("* Caught an error on ", mismatch[indx])
        }
    )
}

```

split a vector into n chunks
<https://stackoverflow.com/questions/3318333/split-a-vector-into-chunks-in-r>

```{r}
chunk2 <- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE))
```

```{r collapsibleTree}
library(collapsibleTree)

collapsibleTree(warpbreaks, c("wool", "tension", "breaks"))

# Data from US Forest Service DataMart
species <- read.csv(system.file("extdata/species.csv", package = "collapsibleTree"))
collapsibleTree(df = species, c("REGION", "CLASS", "NAME"), fill = "green")

# Visualizing the order in which the node colors are filled
library(RColorBrewer)
collapsibleTree(
  warpbreaks, c("wool", "tension"),
  fill = brewer.pal(9, "RdBu"),
  fillByLevel = TRUE
)
collapsibleTree(
  warpbreaks, c("wool", "tension"),
  fill = brewer.pal(9, "RdBu"),
  fillByLevel = FALSE
)

# Tooltip can be mapped to an attribute, or default to leafCount
collapsibleTree(
  warpbreaks, c("wool", "tension", "breaks"),
  tooltip = TRUE,
  attribute = "breaks"
)

# Node size can be mapped to any numeric column, or to leafCount
collapsibleTree(
  warpbreaks, c("wool", "tension", "breaks"),
  nodeSize = "breaks"
)

# collapsibleTree.Node example
data(acme, package="data.tree")
View(acme)
acme$Do(function(node) node$cost <- data.tree::Aggregate(node, attribute = "cost", aggFun = sum))
collapsibleTree(acme, nodeSize  = "cost", attribute = "cost", tooltip = TRUE)

# Emulating collapsibleTree.data.frame using collapsibleTree.Node
species <- read.csv(system.file("extdata/species.csv", package = "collapsibleTree"))
hierarchy <- c("REGION", "CLASS", "NAME")
species$pathString <- paste(
  "species",
  apply(species[,hierarchy], 1, paste, collapse = "//"),
  sep = "//"
)
df <- data.tree::as.Node(species, pathDelimiter = "//")
collapsibleTree(df)
```

```{r ednaoccupancy}
# old install instructions
# library(mvtnorm, pROC, mcmcse)
# setwd("~/Downloads")
# # https://my.usgs.gov/bitbucket/projects/USGS_WARC/repos/ednaoccupancy/browse/eDNAoccupancy_0.2.2.tar.gz
# install.packages("eDNAoccupancy_0.2.2.tar.gz", repos=NULL, type="source")

remotes::install_github("RobertDorazio/eDNAoccupancy")
library(eDNAoccupancy)
vignette("eDNAoccIntro")

# tutorial
library(eDNAoccupancy)
data(fungusDetectionData)
data(fungusSurveyData)

head(fungusDetectionData)
head(fungusSurveyData)

fungusDetections = occData(fungusDetectionData, siteColName = 'site', sampleColName = 'sample')
set.seed(0157)
fit = occModel(detectionMats=fungusDetections, niter=11000, niterInterval=5000)
posteriorSummary(fit, burnin=1000, mcError=TRUE)

## Center and scale numeric-valued covariate measurements
fungusSurveyData.sc = scaleData(fungusSurveyData)
set.seed(0157)
fit = occModel(formulaSite = ~ 1, formulaSiteAndSample = ~ frogs, formulaReplicate = ~ 1, detectionMats = fungusDetections, siteData = fungusSurveyData.sc, niter = 6000,niterInterval = 2000, siteColName = 'site' )

posteriorSummary(fit, burnin=1000, mcError=TRUE)

plotTrace(fit, c('beta..Intercept.', 'alpha..Intercept.', 'alpha.frogs', 'delta..Intercept.'), burnin=1000)

plotACF(fit, c('beta..Intercept.', 'alpha..Intercept.', 'alpha.frogs', 'delta..Intercept.'), burnin=1000)
fit = updateOccModel(fit, niter=5000, niterInterval=2000)
posteriorSummary(fit, burnin=1000, mcError=TRUE)
```

```{r ggparallel}
library(ggparallel)
data(mtcars)
head(mtcars)
ggparallel(list("gear", "cyl"), data=mtcars)
ggparallel(list("gear", "cyl"), data=mtcars, method="hammock", ratio=0.25)

require(RColorBrewer)
require(ggplot2)
cols <- c(brewer.pal(4, "Reds")[-1], brewer.pal(4, "Blues")[-1])
ggparallel(list("gear", "cyl"), ratio=0.2, data=mtcars,
           method="hammock", text.angle=0) +
  scale_fill_manual(values=cols) + scale_colour_manual(values=cols) +
  theme_bw()

## combination of common angle plot and hammock adjustment:
ggparallel(list("gear", "cyl"), data=mtcars, method="adj.angle",
           ratio=2)

## compare with method='parset'
ggparallel(list("gear", "cyl"), data=mtcars, method='parset')

## flip plot and rotate text
ggparallel(list("gear", "cyl"), data=mtcars, text.angle=0) +
  coord_flip()

## change colour scheme
ggparallel(list("gear", "cyl"), data=mtcars, text.angle=0) +
  coord_flip() +
  scale_fill_brewer(palette="Set1") +
  scale_colour_brewer(palette="Set1")

## example with more than two variables:
titanic <- as.data.frame(Titanic)
ggparallel(names(titanic)[c(1,4,2,1)], order=0, titanic, weight="Freq") +
  scale_fill_brewer(palette="Paired", guide="none") +
  scale_colour_brewer(palette="Paired", guide="none")

## Not run:
cols <- c(brewer.pal(5,"Blues")[-1], brewer.pal(3, "Oranges")[-1],
          brewer.pal(3, "Greens")[-1])
ggparallel(names(titanic)[c(1,4,2,1)], order=0, titanic, weight="Freq") +
  scale_fill_manual(values=cols, guide="none") +
  scale_colour_manual(values=cols, guide="none") + theme_bw()

## hammock plot with same width lines
ggparallel(names(titanic)[c(1,4,2,3)], titanic, weight=1, asp=0.5,
           method="hammock", ratio=0.2, order=c(0,0)) +
theme( legend.position="none") +
scale_fill_brewer(palette="Paired") +
scale_colour_brewer(palette="Paired")

## hammock plot with line widths adjusted by frequency
ggparallel(names(titanic)[c(1,4,2,3)], titanic, weight="Freq",
           asp=0.5, method="hammock", order=c(0,0), text.angle=0,
           width=0.45) +
  theme( legend.position="none")


## biological examples: genes and pathways
data(genes)
cols <- c(rep("grey80", 24), brewer.pal("YlOrRd", n = 9))
genes$chrom <- factor(genes$chrom, levels=c(paste("chr", 1:22, sep=""), "chrX", "chrY"))
ggparallel(list("path", "chrom"), text.offset=c(0.03, 0,-0.03),
           data = genes,  width=0.1, order=c(1,0), text.angle=0,
           color="white",
   factorlevels =  c(sapply(unique(genes$chrom), as.character),
     unique(genes$path))) +
   scale_fill_manual(values = cols, guide="none") +
   scale_colour_manual(values = cols, guide="none") +
   coord_flip()

## End(Not run)

data(Titanic)
titanic <- as.data.frame(Titanic)

titanic$SexSurvived <- with(titanic, interaction(Sex, Survived))
titanic$SexClassSurvived <- with(titanic, interaction(Sex,Class, Survived))

ggparallel(vars=list("Survived", "SexSurvived", "SexClassSurvived"), weight="Freq", data=titanic) +
  theme(legend.position="none") +
  scale_fill_manual(values = rep(c("Orange", "Steelblue"), 14)) +
  scale_colour_manual(values = rep(c("Orange", "Steelblue"), 14))
```

Clean, consistent column names in R
<https://www.r-bloggers.com/clean-consistent-column-names/>

```{r}
clean_names <- function(.data, unique = TRUE) {
  n <- if (is.data.frame(.data)) colnames(.data) else .data

  n <- gsub("%+", "_pct_", n)
  n <- gsub("\\$+", "_dollars_", n)
  n <- gsub("\\++", "_plus_", n)
  n <- gsub("-+", "_minus_", n)
  n <- gsub("\\*+", "_star_", n)
  n <- gsub("#+", "_count_", n)
  n <- gsub("&+", "_and_", n)
  n <- gsub("@+", "_at_", n)

  n <- gsub("[^a-zA-Z0-9_]+", "_", n)
  n <- gsub("([A-Z][a-z])", "_\\1", n)
  n <- tolower(trimws(n))
  
  n <- gsub("(^_+|_+$)", "", n)
  
  n <- gsub("_+", "_", n)
  
  if (unique) n <- make.unique(n, sep = "_")
  
  if (is.data.frame(.data)) {
    colnames(.data) <- n
    .data
  } else {
    n
  }
}


clean_names(
  c(
    "  a", "a  ", "_c",
    "a %", "a", "$a", "$$$a", "GDP ($)", "GDP (us$)",
    "a (#)", "a & b", "#", "$",
    "a_cnt",
    "Aa&Bb", "camelCasePhrases",
    "AlphaBetaGamma", "Alpha       Beta", "Beta  !!! Gamma",
    "a + b", "a - b", "a * b"
  )
)

```

you can pipe a dataframe into this function df \<-
read.csv("data-raw/source\_data") %\>% clean\_names()

unnesting dataframesˆ

```{r}
library(tidyverse)
library(repurrrsive)
users <- tibble(user = gh_users) # gh_users is a list of six github users
names(users$user[[1]])
users %>% unnest_wider(user)

discs <- tibble(disc = discog) %>% 
  unnest_wider(disc) %>% 
  mutate(date_added = as.POSIXct(strptime(date_added, "%Y-%m-%dT%H:%M:%S"))) 
```

```{r arsenal comparedf}
df1 <- data.frame(id = paste0("person", 1:3), a = c("a", "b", "c"),
                  b = c(1, 3, 4), c = c("f", "e", "d"),
                  row.names = paste0("rn", 1:3), stringsAsFactors = FALSE)
df2 <- data.frame(id = paste0("person", 3:1), a = c("c", "b", "a"),
                  b = c(1, 3, 4), d = paste0("rn", 1:3),
                  row.names = paste0("rn", c(1,3,2)), stringsAsFactors = FALSE)
summary(comparedf(df1, df2))
```

```{r BarcodingR}
library(BarcodingR)
data(TibetanMoth)
TibetanMoth<-as.DNAbin(as.character(TibetanMoth[1:20,]))
b.gap<-barcoding.gap(ref=TibetanMoth,dist="K80")
b.gap
summarize.ref(TibetanMoth)


data(pineMothITS2) 
ref<-pineMothITS2
que<-ref
spe.id<-barcoding.spe.identify2(ref,que, kmer = 1, optimization = FALSE)
spe.id
 
data(TibetanMoth)
ref<-as.DNAbin(as.character(TibetanMoth[1:5,]))
que<-as.DNAbin(as.character(TibetanMoth[50:55,]))
bsi<-barcoding.spe.identify(ref, que, method = "fuzzyId")
bsi
bsi<-barcoding.spe.identify(ref, que, method = "bpNewTraining")
bsi
bsi<-barcoding.spe.identify(ref, que, method = "Bayesian")
bsi
```

```{r metap}
library(metap)
data(validity)
print(validity)
plotp(validity)
schweder(validity)
schweder(validity, drawline = c("bh", "ls", "ab"), ls.control = list(frac = 0.5), ab.control = list(a = 0, b = 0.01))
sumlog(validity) # Fisher's method
minimump(validity) # Tippett's method
sump(validity)
can.p <- c(0.020833333, 0.590277778, 0.391666667, 0.009722222, 0.094444444)
can.p
sumlog(can.p)
minimump(can.p)
sump(can.p)
invchisq(can.p, 5)
invt(can.p, 5)

```

```{r performance}
# Utilities for computing measures to assess model quality that are not directly provided by R's 'base' or 'stats' packages.
library(lme4)
library(performance)

# intra-class correlation cofefficient. The ICC can be interpreted as “the proportion of the variance explained by the grouping structure in the population”. In other word, the ICC “can also be interpreted as the expected correlation between two randomly drawn units that are in the same group”
model <- lme4::lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
icc(model)


model <- lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
r2_nakagawa(model) # The marginal r-squared considers only the variance of the fixed effects, while the conditional r-squared takes both the fixed and random effects into account. The random effect variances are actually the mean random effect variances, thus the r-squared value is also appropriate for mixed models with random slopes or nested random effects (see Johnson 2014).
model <- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_tjur(model)
r2(model)
model <- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_nagelkerke(model)
r2(model)

m1 <- lm(mpg ~ wt + cyl, data = mtcars)
model_performance(m1)

m2 <- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
m3 <- lmer(Petal.Length ~ Sepal.Length + (1 | Species), data = iris)
compare_performance(m1, m2, m3)

data(iris)
lm1 <- lm(Sepal.Length ~ Species, data = iris)
lm2 <- lm(Sepal.Length ~ Species + Petal.Length, data = iris)
lm3 <- lm(Sepal.Length ~ Species * Petal.Length, data = iris)
compare_performance(lm1, lm2, lm3)

```

```{r zetadiv}
# from Latombe et al. (2018 Biorxiv paper)
library(zetadiv)
library(simba)

##load data
data(bird.spec.fine) 
xy <- bird.spec.fine[,1:2] # geographic coordinates of sites 
data.spec <- bird.spec.fine[,3:192] # site-by-species matrix 
data(bird.env.fine) 
data.env <- bird.env.fine[,3:9] # site-by-environment matrix

################ ##zeta decline## ################

##expected zeta 
set.seed(1) 
zeta.decline.fine.ex <- Zeta.decline.ex(data.spec, 
                                        orders = 1:50, 
                                        plot = FALSE
                                        ) 
zeta.decline.fine.ex

##non-directional nearest neighbour (NON) zeta 
set.seed(1) 
zeta.decline.fine1000.NON <- Zeta.decline.mc(data.spec, 
                                             xy, 
                                             orders = 1:50, 
                                             sam = 1000, 
                                             NON = TRUE, 
                                             plot = FALSE
                                             ) 
zeta.decline.fine1000.NON

##ALL zeta, for different numbers of combinations 
set.seed(1) 
zeta.decline.fine1000 <- Zeta.decline.mc(data.spec, 
                                         xy, 
                                         orders = 1:50, 
                                         sam = 1000, 
                                         plot = FALSE
                                         ) 
zeta.decline.fine1000

set.seed(1) 
zeta.decline.fine100 <- Zeta.decline.mc(data.spec, 
                                        xy, 
                                        orders = 1:50, 
                                        sam = 100, 
                                        plot = FALSE
                                        ) 
zeta.decline.fine100

set.seed(1) 
zeta.decline.fine10 <- Zeta.decline.mc(data.spec, 
                                       xy, 
                                       orders = 1:50, 
                                       sam = 10, 
                                       plot = FALSE
                                       ) 
zeta.decline.fine10

##Figure 2 -- This code copies code from Plot.zeta.decline, with some modification to enhance the layout 
# windows(width = 12, height = 8) 
par(mfrow = c(1,1))
par(mfrow = c(2, 4), mar=c(5,4.5,4,2)) 
##plot the zeta decline for the expected zeta
plot(zeta.decline.fine.ex$zeta.order, 
     zeta.decline.fine.ex$zeta.val, 
     xlab = "Zeta order", 
     ylab = "Zeta-diversity", 
     pch = 20, 
     ylim = c(0, 
              zeta.decline.fine.ex$zeta.val[1] +
              zeta.decline.fine.ex$zeta.val.sd[1]
              ), 
     main = "Zeta-diversity decline",
     col="red", 
     cex.lab=2, 
     cex.main=2
     )

lines(zeta.decline.fine.ex$zeta.order, zeta.decline.fine.ex$zeta.val,col="red")
lines(zeta.decline.fine.ex$zeta.order, zeta.decline.fine.ex$zeta.val + zeta.decline.fine.ex$zeta.val.sd, lty = 2)
lines(zeta.decline.fine.ex$zeta.order, zeta.decline.fine.ex$zeta.val - zeta.decline.fine.ex$zeta.val.sd, lty = 2)


##plot the zeta ratio for the expected zeta
plot(zeta.decline.fine.ex$zeta.order[1:(length(zeta.decline.fine.ex$ zeta.order) - 1)], zeta.decline.fine.ex$ratio, pch = 20, xlab = "Zeta order", ylab = "Zeta-ratio", main = "Ratio of zetadiversity\ndecline",col="red", ylim=c(0.6,1), cex.lab=2, cex.main=2)

lines(zeta.decline.fine.ex$zeta.order[1:(length(zeta.decline.fine.ex $zeta.order) - 1)], zeta.decline.fine.ex$ratio,col="red")

##plot the zeta decline in a log scale for the expected zeta
plot(zeta.decline.fine.ex$zeta.order, zeta.decline.fine.ex$zeta.val, log = "y", pch = 20, xlab = "Zeta order", ylab = "Zeta-diversity", main = "Exponential regression", col="red", cex.lab=2, cex.main=2)

lines(zeta.decline.fine.ex$zeta.order,  10^predict.lm(zeta.decline.fine.ex$zeta.exp,  data.frame(zeta.decline.fine.ex$zeta.order)), col="red")

##plot the zeta decline in a log-log scale for the expected zeta
plot(zeta.decline.fine.ex$zeta.order, zeta.decline.fine.ex$zeta.val, log = "xy", pch = 20, xlab = "Zeta order", ylab = "Zeta-diversity", main = "Power law regression",col="red",cex.lab=2,cex.main=2)

lines(zeta.decline.fine.ex$zeta.order, 10^predict.lm(zeta.decline.fine.ex$zeta.pl, data.frame(zeta.decline.fine.ex$zeta.order)),col="red")

##plot the zeta decline for the NON subsampling scheme
plot(zeta.decline.fine1000.NON$zeta.order, zeta.decline.fine1000.NON$zeta.val, xlab = "Zeta order", ylab = "Zeta-diversity", pch = 20, ylim = c(0, zeta.decline.fine1000.NON$zeta.val[1] + zeta.decline.fine1000.NON$zeta.val.sd[1]), main = "Zeta-diversity decline",col="red",cex.lab=2,cex.main=2)

lines(zeta.decline.fine1000.NON$zeta.order, zeta.decline.fine1000.NON$zeta.val,col="red")

lines(zeta.decline.fine1000.NON$zeta.order, zeta.decline.fine1000.NON$zeta.val + zeta.decline.fine1000.NON$zeta.val.sd, lty = 2)

lines(zeta.decline.fine1000.NON$zeta.order, zeta.decline.fine1000.NON$zeta.val  - zeta.decline.fine1000.NON$zeta.val.sd, lty = 2)



##plot the zeta ratio for the NON subsampling scheme
plot(zeta.decline.fine1000.NON$zeta.order[1:(length(zeta.decline.fine1000.NON$zeta.order) - 1)], zeta.decline.fine1000.NON$ratio, pch = 20, xlab = "Zeta order", ylab = "Zeta-ratio", main = "Ratio of zetadiversity\ndecline",col="red",ylim=c(0.6,1),cex.lab=2,cex.main=2)

lines(zeta.decline.fine1000.NON$zeta.order[1:(length(zeta.decline.fine1000.NON$zeta.order) - 1)], zeta.decline.fine1000.NON$ratio,col="red")

##plot the zeta decline in a log scale for the NON subsampling scheme
plot(zeta.decline.fine1000.NON$zeta.order, zeta.decline.fine1000.NON$zeta.val, log = "y", pch = 20, xlab = "Zeta order", ylab = "Zeta-diversity", main = "Exponential regression",col="red",cex.lab=2,cex.main=2)
lines(zeta.decline.fine1000.NON$zeta.order, 10^predict.lm(zeta.decline.fine1000.NON$zeta.exp, data.frame(zeta.decline.fine1000.NON$zeta.order)),col="red")

##plot the zeta decline in a log-log scale for the NON subsampling scheme
plot(zeta.decline.fine1000.NON$zeta.order, zeta.decline.fine1000.NON$zeta.val, log = "xy", pch = 20, xlab = "Zeta order", ylab = "Zeta-diversity", main = "Power law regression",col="red",cex.lab=2,cex.main=2)
lines(zeta.decline.fine1000.NON$zeta.order, 10^predict.lm(zeta.decline.fine1000.NON$zeta.pl, data.frame(zeta.decline.fine1000.NON$zeta.order)),col="red")


##Figure A1 (sensitivity analysis) 
# takes forever to run
# set.seed(1) 
# zeta.sens <- Zeta.sam.sensitivity(data.spec, order = 3, sam.seq = c(10,25,50,100,seq(250,1500,250)), reps = 100, display = TRUE, plot = TRUE, notch = TRUE)

# lots of code for figure 3 removed

################################################## 
##Multi-Site Generalised Dissimilarity Modelling##
##################################################

set.seed(1) 
zeta.ispline.fine2 <- Zeta.msgdm(data.spec, data.env, xy, 
                                 order=2, sam=1000,
                                 reg.type="ispline", 
                                 normalize="Sorensen",
                                 family=stats::binomial(link="log"), 
                                 cons.inter = -1
                                 )

set.seed(1) 
zeta.ispline.fine10 <- Zeta.msgdm(data.spec, data.env, xy, 
                                  order=10, sam=1000, 
                                  reg.type="ispline", 
                                  normalize="Sorensen",
                                  family=stats::binomial(link="log"), 
                                  cons.inter = -1
                                  )


#Figure 4 
# windows(width=10,height=4) 
layout(matrix(1:3, 1, 3), widths=c(2,2,1.5)) 

Plot.ispline(zeta.ispline.fine2, 
             data.env, 
             distance = TRUE, 
             legend = FALSE, 
             cex=2, lwd=2
             ) 

par(xpd=T) 
text(-0.15*(par("usr")[2] - par("usr")[1]), par("usr")[4]+0.14*(par("usr")[4]- par("usr")[3]), "a)", cex=3) 
par(xpd=F) 

Plot.ispline(zeta.ispline.fine10, 
             data.env, 
             distance = TRUE, 
             legend = FALSE, 
             cex=2, lwd=2
             ) 

par(xpd=T) 
text(-0.15*(par("usr")[2] - par("usr")[1]),par("usr")[4]+0.14*(par("usr")[4] - par("usr")[3]),"b)",cex=3) 
par(xpd=F) 

plot.new()
par(xpd=NA) 
legend("topleft", lty=1:(ncol(data.env)+1), pch=1:(ncol(data.env)+1), legend=c(names(data.env),"Distance"), cex=2, lwd=2, bty="n")


##variation partitioning (Figure 5) 
set.seed(1) 
zeta.varpart.ispline.fine2 <- Zeta.varpart(msgdm.mod=zeta.ispline.fine2,
                                            reg.type = "ispline",
                                            family=binomial("log")
                                            ) 

set.seed(1) 
zeta.varpart.ispline.fine10 <- Zeta.varpart(msgdm.mod=zeta.ispline.fine10,
                                             reg.type = "ispline",
                                             family=binomial("log")
                                             )


##variation partitioning (Figure 5) 
set.seed(1) 
zeta.glm.fine2 <- Zeta.msgdm(data.spec,data.env, xy, 
                             order=2, sam=1000, 
                             reg.type="glm", 
                             normalize="Sorensen", 
                             method.glm = "glm.fit2"
                             ) 
zeta.varpart.glm.fine2 <- Zeta.varpart(zeta.glm.fine2, 
                                       method.glm = "glm.fit2"
                                       ) 
set.seed(1) 
zeta.glm.fine10 <- Zeta.msgdm(data.spec,data.env, xy, 
                              order=10, 
                              sam=1000, 
                              reg.type="glm", 
                              normalize="Sorensen", 
                              method.glm = "glm.fit2"
                              ) 
zeta.varpart.glm.fine10 <- Zeta.varpart(zeta.glm.fine10, 
                                        method.glm = "glm.fit2"
                                        )

# windows(width=12,height=12)
par(mfrow=c(2,2), oma=c(0,0,0,0), mar=c(0,2,0,5), xpd=NA)

pie.neg(zeta.varpart.ispline.fine2[4:7,1], 
        density = c(4, 0, 8, -1), 
        angle = c(90, 0, 0, 0), 
        labels = c("distance", "undistinguishable", 
                   "environment", "unexplained"), 
        radius = 0.9)

text(x=par("usr")[1],y=0.85*par("usr")[4],"a)",cex=2)
text(x=(par("usr")[1] + par("usr")[2])/2, 
     y=0.85*par("usr")[4], 
     expression(paste(zeta[2])), 
     cex=2
     ) 

pie.neg(zeta.varpart.ispline.fine10[4:7,1], 
        density = c(4, 0, 8, 1), 
        angle = c(90, 0, 0, 0), 
        labels = c("distance","undistinguishable", 
                   "environment","unexplained"), 
        radius = 0.9
        )
text(x=par("usr")[1],y=0.85*par("usr")[4],"b)",cex=2)
text(x=(par("usr")[1]+par("usr")[2])/2,y=0.85*par("usr")[4],expression(paste(zeta[10])),cex=2) 

pie.neg(zeta.varpart.glm.fine2[4:7,1], 
        density = c(4, 0, 8, -1), 
        angle = c(90, 0, 0, 0), 
        labels = c("distance","undistinguishable",
                   "environment","unexplained"), 
        radius = 0.9
        )
text(x=par("usr")[1], y=0.85*par("usr")[4],"c)", cex=2)

pie.neg(zeta.varpart.glm.fine10[4:7,1], 
        density = c(4, 0, 8, -1), 
        angle = c(90, 0, 0, 0), 
        labels = c("distance","undistinguishable",
                   "environment","unexplained"), 
        radius = 0.9
        ) 
text(x=par("usr")[1], y=0.85*par("usr")[4],"d)", cex=2)

################################ ##Hierarchical scaling of zeta## ################################

zeta.scale.irreg <- list() zeta.scale.all <- matrix(NA,10,15)

for(i in 1:10){ print(i) set.seed(1) zeta.scale.irreg[[i]] <- Zeta.scale.min.dist(xy, data.spec, m = c(1:10,seq(20,60,10)), order = i, reorder = 1, normalize = FALSE, plot = FALSE, zeta.type="exact")

zeta.scale.all[i,] <- colMeans(zeta.scale.irreg[[i]]$values) }

colours <c("black","orange","red","blue","purple","darkgreen","brown","grey", "cadetblue","magenta") pch=c(16,4,15,17,18,8,4,0,2,3)

##Figure 8 pdf("figures/figure 8.pdf",width=7,height=18) par(mfrow=c(3,1),lwd=2,mar=c(5,7,4,2)) plot(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[1]]$values),ty pe="p",ylim=c(0,180),ylab="Zeta value",xlab="Grain",cex=2.5,cex.lab=3,cex.axis=2.5,pch=pch[1]) lines(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[1]]$values)) for(i in 2:10){

points(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[i]]$values), col=colours[i],cex=2.5,pch=pch[i])

lines(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[i]]$values),c ol=colours[i]) } legend(x="bottomright",c("zeta 1","zeta 2","zeta 3","zeta 4","zeta 5","zeta 6","zeta 7","zeta 8","zeta 9","zeta 10"),col=colours,pch=pch,lwd=2,cex=2.5,bty="n",pt.cex=2.5) par(xpd=NA) text(-0.15*(par("usr")[2]par("usr")[1]),par("usr")[4]+0.08*(par("usr")[4]par("usr")[3]),"a)",cex=3) par(xpd=F)

par(lwd=2) plot(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[1]]$values)colMeans(zeta.scale.irreg[[2]]$values),type="p",ylim=c(0.1,27),pch=p ch[1],ylab="Difference of zeta",xlab="Grain",cex=2.5,cex.lab=3,cex.axis=2.5,log="xy") lines(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[1]]$values)colMeans(zeta.scale.irreg[[2]]$values)) for(i in 2:9){

points(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[i]]$values)colMeans(zeta.scale.irreg[[i+1]]$values),col=colours[i],pch=pch[i],c ex=2.5)

lines(c(1:10,seq(20,60,10)),colMeans(zeta.scale.irreg[[i]]$values)colMeans(zeta.scale.irreg[[i+1]]$values),col=colours[i]) } legend(x="bottomleft",c("zeta 1 - zeta 2","zeta 2 - zeta 3","zeta 3

- zeta 4","zeta 4 - zeta 5","zeta 5 - zeta 6"),col=colours[1:9],pch=pch[1:9],lwd=2,cex=2.5,bty="n",pt.cex=2.5) legend(x="bottomright",c("zeta 6 - zeta 7","zeta 7 - zeta 8","zeta 8

- zeta 9","zeta 9 - zeta 10"),col=colours[6:9],pch=pch[6:9],lwd=2,cex=2.5,bty="n",pt.cex=2.5) par(xpd=NA) text(0.5,50,"b)",cex=3) par(xpd=F)

plot(zeta.scale.all[,1],pch=1,xlim= c(0,11),ylim=c(0,180),ylab="Zeta value",xlab="Order",cex=2.5,cex.lab=3,cex.axis=2.5,lwd=2) lines(zeta.scale.all[,1],lwd=2) text(10.5,zeta.scale.all[,1][10],c(1:10,seq(20,60,10))[1],pos=4,cex= 2) for(i in 2:15){

points(zeta.scale.all[,i],pch=i,cex=2.5,lwd=2) lines(zeta.scale.all[,i],lwd=2)

text(10.5,zeta.scale.all[,i][10],c(1:10,seq(20,60,10))[i],pos=4,cex= 2) } par(xpd=NA) text(-0.15*(par("usr")[2]par("usr")[1]),par("usr")[4]+0.08*(par("usr")[4]par("usr")[3]),"c)",cex=3) par(xpd=F)

dev.off()


library(zetadiv)
utils::data(bird.spec.coarse)
xy.bird <- bird.spec.coarse[1:2]
data.spec.bird <- bird.spec.coarse[3:193]
utils::data(bird.env.coarse)
data.env.bird <- bird.env.coarse[,3:9]

zeta.glm <- Zeta.msgdm(data.spec.bird, data.env.bird, sam = 100, order = 3)
zeta.glm
dev.new()
graphics::plot(zeta.glm$model)

zeta.ngls <- Zeta.msgdm(data.spec.bird, data.env.bird, xy.bird, sam = 100, order = 3,
    reg.type = "ngls", rescale = TRUE)
zeta.ngls

##########

utils::data(Marion.species)
xy.marion <- Marion.species[1:2]
data.spec.marion <- Marion.species[3:33]
utils::data(Marion.env)
data.env.marion <- Marion.env[3]

zeta.gam <- Zeta.msgdm(data.spec.marion, data.env.marion, sam = 100, order = 3,
    reg.type = "gam")
zeta.gam
dev.new()
graphics::plot(zeta.gam$model)

zeta.ispline <- Zeta.msgdm(data.spec.marion, data.env.marion, xy.marion, sam = 100,
    order = 3, normalize = "Jaccard", reg.type = "ispline")
zeta.ispline

```

```{r ggridges}
library(ggplot2)
library(ggridges)
library(gridExtra)

data <- data.frame(x = 1:5, y = rep(1, 5), height = c(0, 1, 3, 4, 2))
ggplot(data, aes(x, y, height = height)) + geom_ridgeline()
# for side-by-side plotting
library(gridExtra)

data <- data.frame(x = 1:5, y = rep(1, 5), height = c(0, 1, -1, 3, 2))
plot_base <- ggplot(data, aes(x, y, height = height))
grid.arrange(
  plot_base + geom_ridgeline(),
  plot_base + geom_ridgeline(min_height = -2),
  ncol = 2
)

d <- data.frame(x = rep(1:5, 3), y = c(rep(0, 5), rep(1, 5), rep(2, 5)),
                height = c(0, 1, 3, 4, 0, 1, 2, 3, 5, 4, 0, 5, 4, 4, 1))
ggplot(d, aes(x, y, height = height, group = y)) + 
  geom_ridgeline(fill = "lightblue")

ggplot(iris, aes(x = Sepal.Length, y = Species)) + geom_density_ridges()
ggplot(iris, aes(x = Sepal.Length, y = Species)) + geom_density_ridges2()

# modified dataset that represents species as a number
iris_num <- transform(iris, Species_num = as.numeric(Species))

# does not work, causes error
# ggplot(iris_num, aes(x = Sepal.Length, y = Species)) + geom_density_ridges()

# works 
ggplot(iris_num, aes(x = Sepal.Length, y = Species_num, group = Species_num)) + 
  geom_density_ridges()

# scale = 0.9, not quite touching
ggplot(iris, aes(x = Sepal.Length, y = Species)) + geom_density_ridges(scale = 0.9)

library(viridis)
d <- data.frame(x = rep(1:5, 3) + c(rep(0, 5), rep(0.3, 5), rep(0.6, 5)),
                y = c(rep(0, 5), rep(1, 5), rep(3, 5)),
                height = c(0, 1, 3, 4, 0, 1, 2, 3, 5, 4, 0, 5, 4, 4, 1))
ggplot(d, aes(x, y, height = height, group = y, fill = factor(x+y))) +
  geom_ridgeline_gradient() +
  scale_fill_viridis(discrete = TRUE, direction = -1, guide = "none")

ggplot(lincoln_weather, aes(x = `Mean Temperature [F]`, y = `Month`, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  scale_fill_viridis(name = "Temp. [F]", option = "C") +
  labs(title = 'Temperatures in Lincoln NE in 2016')
```

```{r pivottabler}
library(pivottabler)
# arguments:  qhpvt(dataFrame, rows, columns, calculations, ...)
data(bhmtrains)
View(bhmtrains)
qpvt(bhmtrains, "TOC", "TrainCategory", "n()") # TOC = Train Operating Company 
qpvt(bhmtrains, c("=", "TOC"), c("TrainCategory", "PowerType"), #"=" use names of calculations
     c("Number of Trains"="n()",
       "Maximum Speed"="max(SchedSpeedMPH, na.rm=TRUE)"))

qhpvt(GH_2003_2004_2004_all, c("=", "kelpie_out", "target"), c("Kelpie_detected"),
     c("Number"="n()",
       "Max length"="max(length_max, na.rm=TRUE)"))
qhpvt(GH_2003_2004_2004_all, c("=", "target"), c("kelpie_out", "Kelpie_detected"),
     c("Number"="n()",
       "Max length"="max(length_max, na.rm=TRUE)"))


# without qhpvt shortcut
pt <-  PivotTable$new()
pt$addData(GH_2003_2004_2005_combined_all)
pt$addColumnDataGroups("kelpie_out")
pt$addColumnDataGroups("Kelpie_detected")
pt$addRowDataGroups("target")
pt$defineCalculation(calculationName="TotalDetections", summariseExpression="n()")
pt$sortRowDataGroups(levelNumber=1, orderBy="calculation", sortOrder="desc")
pt$evaluatePivot()
GH_2003_2004_2005_combined_all_pt <- pt$asDataFrame()
pt$renderPivot()
# pt$asMatrix()
# pt

```

Two tricks here to learn. Calculate for each value within a subgroup,
its percentage of the total and create a new column. Second trick is to
add a column with the rank of a value.

```{r}
# The trick is to calculate percentage for every name within each Year. The group_by() means that when i calculate sum(Count) it is only for that Year. group_by does the work of a loop in the tidyverse.  Try it without the group_by
babynamessumm <- babynames %>% 
    group_by(Year) %>% 
    mutate(
        pct = round(Count/sum(Count)*100, 2)
    ) %>% 
    ungroup() %>% 
    filter(Name == name) # limits to only your chosen name

# where did i get this code?  I googled it with the query: "calculate percentage of total in dplyr" and it was in the first answer
# https://stackoverflow.com/questions/29549731/finding-percentage-in-a-sub-group-using-group-by-and-summarise

babynamessumm # notice that there are lines for M and F.  The problem asks us to choose the sex with the higher sum for this name In short, do we filter on "M" or "F"?

(commoner <- babynamessumm %>% 
    group_by(Gender) %>% 
    summarise(
        Gender_sum = sum(Count)
        ) %>% 
    mutate(
        rank = min_rank(desc(Gender_sum)) # again, i googled to get this
        ) %>% 
    filter(rank == 1) %>% 
    pull(Gender)) # pulls out just the vector with this column name (as opposed to select(Gender), which pulls out the whole column)

babynamessumm <- babynamessumm %>% 
    filter(Gender == commoner)

# add variable values to an axis label in ggplot
ggplot(babynamessumm, aes(Year, pct)) +
    geom_line() +
    theme_bw() +
    labs(x = paste("Year"), 
         y = paste("% of", gender, "with name", name, sep = " "))

# another example of adding new axis labels
p <- ggplot(mtcars, aes(mpg, wt, col = cyl)) + 
  geom_point()
p
p + labs(col = "Cylinders", x = "New x label", y = "New y label")
```

```{r}
climtree <- c(0.021,0.001,0.094,0.590,0.392)
p.adjust(climtree, method = "fdr")
# [1] 0.0525000 0.0050000 0.1566667 0.5900000 0.4900000

```

```{r zeta diversity}
library(zetadiv)

data(bird.spec.fine)
xy <- bird.spec.fine[,1:2] # geographic coordinates of sites
data.spec <- bird.spec.fine[,3:192] # site-by-species matrix
data(bird.env.fine)
data.env <- bird.env.fine[,3:9] # site-by-environment matrix

# zeta diversity decline with zeta order
set.seed(1)
dev.new (width = 12, height = 4)
zeta.decline.fine.ex <- Zeta.decline.ex(data.spec, orders = 1:50)
dev.new(width = 12, height = 4)
zeta.decline.fine.NON <- Zeta.decline.mc(data.spec, xy, orders = 1:50, NON = TRUE, DIR = FALSE, FPO = NULL)

# zeta diversity decline with distance
set.seed(1)
dev.new()
zeta.ddecay.lm.fine <- Zeta.ddecay(xy, data.spec, order = 2, confint.level = 0.95) # the default regression is a linear model
set.seed(1)
dev.new()
zeta.ddecay.gam.fine <- Zeta.ddecay(xy, data.spec, order = 2, reg.type="gam") # a generalised additive model is used instead of the default linear model

# MS-GDM
set.seed(1)
zeta.ispline.fine2 <- Zeta.msgdm(data.spec,data.env,xy,order=2,sam=1000,reg.type="ispline",normalize="Sorensen",family=binomial(link="log"),cons.inter = -1)

Plot.ispline(zeta.ispline.fine2, data.env, distance = TRUE,legend = FALSE)

set.seed(1)
zeta.varpart.fine2 <- Zeta.varpart(zeta.ispline.fine2)
# Zeta.varpart returns a data frame with one column containing the variation explained by each component a (the variation explained by distance alone), b (the variation explained by either distance or the environment), c (the variation explained by the environment alone) and d (the unexplained variation).
# Adjusted Rsq
# [abc]	0.576467194
# [ab]	0.113322083
# [bc]	0.568607179
# [a]	0.007860015 # a (the variation explained by distance alone)
# [b]	0.105462067 # b (the variation explained by either distance or the environment)
# [c]	0.463145112 # c (the variation explained by the environment alone)
# [d]	0.423532806 # d (the unexplained variation)

pie.neg(zeta.varpart.fine2[4:7,1], density = c(4, 0, 8, -1), angle = c(90, 0, 0, 0), labels = c("distance","undistinguishable","environment","unexplained"), radius = 0.9)
```

```{r}
#Install and load packages
if (!requireNamespace(c("BiocManager", "devtools"), quietly = FALSE)) install.packages("BiocManager")
BiocManager::install(c("ShortRead", "Biostrings", "msa"))
devtools::install_github(repo="thackmann/distanced", subdir="Distanced")
library(Distanced)

#Set parameters
system.file()
sample.filepath=file.path=system.file("extdata", "Mock1V34run130401_test_data.fq", package = "Distanced")
reference.filepath=file.path=system.file("extdata", "reference_sequences_V34_no_primers.fasta", package = "Distanced")
n_sample_max.scalar=100
replace_ambiguous_letters.character=FALSE
random_seed.character=FALSE

#Run Distanced
Distanced(sample.filepath, reference.filepath, n_sample_max.scalar, replace_ambiguous_letters.character, random_seed.character)

#If Distanced ran successfully, output will match that below
#                        Mean pairwise distance
# Uncorrected                         0.2351698
# Estimated by Distanced              0.2236499
# Actual                              0.2249940
```

```{r tidystringdist}
library(tidystringdist)
proust <- tidy_comb_all(c("Albertine", "Françoise", "Gilberte", "Odette", "Charles"))
tidy_stringdist(proust)
```

```{r RVerbalExpressions}
devtools::install_github("VerbalExpressions/RVerbalExpressions")
library(RVerbalExpressions)
# Creates regular expressions that can be used in stringr, grep, grepl, regexpr, gregexpr, regexec, sub, gsub
# https://github.com/tyluRp/RVerbalExpressions

x <- rx_start_of_line() %>% 
  rx_find('http') %>% 
  rx_maybe('s') %>% 
  rx_find('://') %>% 
  rx_maybe('www.') %>% 
  rx_anything_but(' ') %>% 
  rx_end_of_line()

# print the expression
x
#> [1] "^(http)(s)?(\\://)(www\\.)?([^ ]*)$"

# test for a match
grepl(x, "https://www.google.com")
#> [1] TRUE

# HOBO216-M1-S1-B_BDSW202056824-1a_F2308_q48_sorted.bam_idxstats.txt
x <- rx_start_of_line() %>% 
    rx_find()

```

```{r ggparallel}
library(ggparallel)
require(RColorBrewer)
require(ggplot2)
data(mtcars)

ggparallel(list("gear", "cyl"), data=mtcars)
ggparallel(list("gear", "cyl"), data=mtcars, method="hammock", ratio=0.25)

cols <- c(brewer.pal(4, "Reds")[-1], brewer.pal(4, "Blues")[-1])
ggparallel(list("gear", "cyl"), ratio=0.2, data=mtcars,
           method="hammock", text.angle=0) +
  scale_fill_manual(values=cols) + scale_colour_manual(values=cols) +
  theme_bw()

## combination of common angle plot and hammock adjustment:
ggparallel(list("gear", "cyl"), data=mtcars, method="adj.angle",
           ratio=2)

## compare with method='parset'
ggparallel(list("gear", "cyl"), data=mtcars, method='parset')

## flip plot and rotate text
ggparallel(list("gear", "cyl"), data=mtcars, text.angle=0) +
  coord_flip()

## change colour scheme
ggparallel(list("gear", "cyl"), data=mtcars, text.angle=0) +
  coord_flip() +
  scale_fill_brewer(palette="Set1") +
  scale_colour_brewer(palette="Set1")

## example with more than two variables:
titanic <- as.data.frame(Titanic)
ggparallel(names(titanic)[c(1,4,2,1)], order=0, titanic, weight="Freq") +
  scale_fill_brewer(palette="Paired", guide="none") +
  scale_colour_brewer(palette="Paired", guide="none")

## Not run: 
cols <- c(brewer.pal(5,"Blues")[-1], brewer.pal(3, "Oranges")[-1],
          brewer.pal(3, "Greens")[-1])
ggparallel(names(titanic)[c(1,4,2,1)], order=0, titanic, weight="Freq") +
  scale_fill_manual(values=cols, guide="none") +
  scale_colour_manual(values=cols, guide="none") + theme_bw()

## hammock plot with same width lines
ggparallel(names(titanic)[c(1,4,2,3)], titanic, weight=1, asp=0.5,
           method="hammock", ratio=0.2, order=c(0,0)) +
theme(legend.position="none") +
scale_fill_brewer(palette="Paired") +
scale_colour_brewer(palette="Paired")

## hammock plot with line widths adjusted by frequency
ggparallel(names(titanic)[c(1,4,2,3)], titanic, weight="Freq",
           asp=0.5, method="hammock", order=c(0,0), text.angle=0,
           width=0.45) +
  theme(legend.position="none")


## biological examples: genes and pathways
data(genes)
cols <- c(rep("grey80", 24), brewer.pal("YlOrRd", n = 9))
genes$chrom <- factor(genes$chrom, levels=c(paste("chr", 1:22, sep=""), "chrX", "chrY"))
ggparallel(list("path", "chrom"), text.offset=c(0.03, 0,-0.03),
           data = genes,  width=0.1, order=c(1,0), text.angle=0,
           color="white",
   factorlevels =  c(sapply(unique(genes$chrom), as.character),
     unique(genes$path))) +
   scale_fill_manual(values = cols, guide="none") +
   scale_colour_manual(values = cols, guide="none") +
   coord_flip()

## End(Not run)

data(Titanic)
titanic <- as.data.frame(Titanic)

titanic$SexSurvived <- with(titanic, interaction(Sex, Survived))
titanic$SexClassSurvived <- with(titanic, interaction(Sex,Class, Survived))

ggparallel(vars=list("Survived", "SexSurvived", "SexClassSurvived"), weight="Freq", data=titanic) +
  theme(legend.position="none") +
  scale_fill_manual(values = rep(c("Orange", "Steelblue"), 14)) +
  scale_colour_manual(values = rep(c("Orange", "Steelblue"), 14))
```

slopegraphs, version 1

```{r newcancer}
# devtools::install_github("ibecav/CGPfunctions")
# https://cran.r-project.org/web/packages/CGPfunctions/vignettes/Using-newggslopegraph.html
library(CGPfunctions)
data(newcancer)
newggslopegraph(newcancer, Times = Year, Measurement = Survival, Grouping = Type, Title = "newcancer slopegraph")

# show how to pivot_wider, although i don't use this format data
newcancerwide <- tidyr::pivot_wider(newcancer, Year, names_prefix = "Year", names_sep = "", names_repair = "check_unique", values_from = Survival, values_fill = NULL, values_fn = NULL)

# newggslopegraph(dataframe, Times, Measurement, Grouping,
#   Title = "No title given", SubTitle = "No subtitle given",
#   Caption = "No caption given", XTextSize = 12, YTextSize = 3,
#   TitleTextSize = 14, SubTitleTextSize = 10, CaptionTextSize = 8,
#   TitleJustify = "left", SubTitleJustify = "left",
#   CaptionJustify = "right", LineThickness = 1, LineColor = "ByGroup",
#   DataTextSize = 2.5, DataTextColor = "black",
#   DataLabelPadding = 0.05, DataLabelLineSize = 0,
#   DataLabelFillColor = "white", WiderLabels = FALSE,
#   ReverseYAxis = FALSE, ReverseXAxis = FALSE, RemoveMissing = TRUE,
#   ThemeChoice = "bw")

# the minimum command to generate a plot
newggslopegraph(newcancer, Year, Survival, Type)

# adding a title which is always recommended
newggslopegraph(newcancer, Year, Survival, Type,
  Title = "Estimates of Percent Survival Rates",
  SubTitle = NULL,
  Caption = NULL
)

# simple formatting changes
newggslopegraph(newcancer, Year, Survival, Type,
  Title = "Estimates of Percent Survival Rates",
  LineColor = "darkgray",
  LineThickness = .5,
  SubTitle = NULL,
  Caption = NULL
)

# complex formatting with recycling and wider labels see vignette for more examples
newggslopegraph(newcancer, Year, Survival, Type,
  Title = "Estimates of Percent Survival Rates",
  SubTitle = "Based on: Edward Tufte, Beautiful Evidence, 174, 176.",
  Caption = "https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0003nk",
  LineColor = c("black", "red", "grey"),
  LineThickness = .5,
  WiderLabels = TRUE
)

# not a great example but demonstrating functionality
newgdp$rGDP <- round(newgdp$GDP)

newggslopegraph(newgdp,
  Year,
  rGDP,
  Country,
  LineColor = c(rep("grey", 3), "red", rep("grey", 11)),
  DataTextSize = 3,
  DataLabelFillColor = "gray",
  DataLabelPadding = .2,
  DataLabelLineSize = .5
)
```

slopegraphs, version 2 \# this is a good tutorial, see code below under
vidyakesavan <https://github.com/vidyakesavan/slopegraph-in-R>
<http://daydreamingnumbers.com/blog/slopegraph-in-r/>

```{r vidyakesavan}
## Import libraries
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(tidyr)
library(stringr)

##Read the data file
# cloned from https://github.com/vidyakesavan/slopegraph-in-R
life <- read.csv("/Users/Negorashi2011/src/slopegraph-in-R/LifeExpectancy.csv",header=TRUE)

## Rearrange the data as Country|1990|2013 for slopegraph. Filter the life expectancy data fo
## 1990 and 2013 into 2 temp variables and then extract the values into year1990 and year2013.
## All countries are extracted to variable group.
## Create a new data frame named 'a' with the 3 variables year1990,year2013, countries
## Set the variable years which will be used for x-axis as 23 (2013-1990)

# tmp1990 <- life %>% filter(Year==1990)
# tmp2013 <- life %>% filter(Year==2013)
# year1990 <- tmp1990$Value
# year2013 <- tmp2013$Value
# group <- tmp2013$Country
# years <- 23
# a <- data.frame(year1990,year2013,group)

# tidyr version of above code
years <- 23 # for the end value of the x-axis (from 2013-1990)
a <- pivot_wider(life, names_from = Year,
            names_prefix = "year", names_sep = "", names_repair = "check_unique",
            values_from = Value, values_fill = NULL, values_fn = NULL) %>% 
  select(year1990, year2013, group = Country)

## Create the labels for the 2 ends of the slopegraph
## Left end (1990) will say COuntry,Value
## Right end(2013) will say value
## Round the values to get rid of the decimals
lab1990 <- paste(a$group, round(a$year1990),sep=",")
lab2013 <- paste(round(a$year2013), a$group, sep=",")

## Draw the initial plot

p <- ggplot(a) + 
  geom_segment(aes(x=0, xend=years, y=year1990, yend=year2013), size=0.5) +
  ggtitle("Life Expectancy At Birth, 1990 & 2013") +
  theme(plot.title = element_text(face="bold",size=20,color="blue"))
p
## Set the theme background, grids, ticks, text and borders to blank
p<-p + theme(panel.background = element_blank())
p<-p + theme(panel.grid=element_blank())
p<-p + theme(axis.ticks=element_blank())
p<-p + theme(axis.text=element_blank())
p<-p + theme(panel.border=element_blank())

## Adding extra space around the graph to accomodate the labels
p <- p + xlim(-8, (years+9))
p <- p + ylim(min(a$year2013,a$year1990),max(a$year2013,a$year1990)+7)

## Set the Y axis title
p<-p+xlab("")+ylab("Life Expectancy")+
  theme(axis.title.y = element_text(size = 15, angle = 90))


## Set the labels on the slopegraph
## x variable is repeated with value as years (right end)
## hjust of 0, aligns text to left
## Size is the font size
p<-p+geom_text(label=lab2013,y=a$year2013,x=rep.int(years,length(a$year2013)),hjust=0, vjust=0.5,size=4)
p<-p+geom_text(label=lab1990,y=a$year1990,x=rep.int(0,length(a$year2013)),hjust=1, vjust=0.5,size=4)

## Set the label for the 2 ends of the slopegraph.
## Y value needs to be slightly greater than the max value of the data, to avoid overlap. 
## This spacing needs to be adjusted in the ylim above
p<-p+geom_text(label="1990",x=0,y=(1.05*(max(a$year2013,a$year1990))),hjust=0.2,size=5)
p<-p+geom_text(label="2013",x=years,y=(1.05*(max(a$year2013,a$year1990))),hjust=0,size=5)


## Show the plot on screen
print(p)

## Add footnote
grid.newpage()
footnote <- "Data Source: UN Data, Life expectancy at birth, total (years)"
g <- arrangeGrob(p, bottom = textGrob(footnote, x = 0, hjust = -0.1, vjust=0.1, gp = gpar(fontface = "italic", fontsize = 12)))
grid.draw(g)

```

Use renv::init() to initialize renv with a new or existing project. This
will set up your project with a private library, and also make sure to
install all of the packages you're using into that library. The packages
used in your project will be recorded into a lockfile, called renv.lock.

```{r renv}
remotes::install_github("rstudio/renv")
# renv::init()
```

> renv::init()

Welcome to renv! It looks like this is your first time using renv. This
is a one-time message, briefly describing some of renv's functionality.

This package maintains a local cache of data on the filesystem at: -
'\~/Library/Application Support/renv'

This path can be customized -- see the documentation in `?paths`. renv
will also write to files within the active project folder, including: -
A folder 'renv' in the project directory, and - A lockfile called
'renv.lock' in the project directory. In particular, projects using renv
will normally use a private, per-project R library, in which new
packages will be installed. This project library is isolated from other
R libraries on your system.

In addition, renv will attempt to update files within your project,
including: - .gitignore - .Rbuildignore - .Rprofile Please read the
introduction vignette with `vignette("renv")` for more information. You
can also browse the package documentation online at
<http://rstudio.github.io/renv>.

By providing consent, you will allow renv to write and update these
files.

```{r}
library(here)
here() # gives the working directory, dependent on .Rproj or .here file  (.here takes priority over .Rproj)
dr_here() # explains why the working directory is what it is
set_here() # sets the working directory by adding a .here file
here(".R", "rstudio", "keybindings", "rstudio_bindings.json") # equivalent to file.path(".R", "rstudio", "keybindings", "rstudio_bindings.json")
# [1] "/Users/Negorashi2011/.R/rstudio/keybindings/rstudio_bindings.json"
test <- jsonlite::read_json(here(".R", "rstudio", "keybindings", "rstudio_bindings.json"))
# here() is superior to file.path() because here() always starts from the original working directory, regardless of where i am in the project subdirectories (prob useful for the spikepipe problems)
```

```{r see}
library(see)
library(ggplot2)

# Better looking points with modern theme and flat design colours
ggplot(iris, aes(x = Sepal.Width, y = Sepal.Length, color = Species)) +
  geom_point2(size=4, alpha=0.5) +
  scale_color_flat_d() +
  theme_modern()

# Violin plot with blackboard theme and material design colours
ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) + 
  geom_violindot(fill_dots = "white") + 
  scale_fill_material_d() +
  theme_blackboard()

# Lucid theme
library(bayestestR)
library(rstanarm)

model <- rstanarm::stan_glm(mpg ~ wt + gear + cyl + disp, data = mtcars)

result <- equivalence_test(model, ci = c(.89, .95))

plot(result) +
  theme_modern() +
  scale_fill_flat()

plot(result) +
  theme_lucid() +
  scale_fill_flat()


# geom_point2
normal <- ggplot(iris, aes(x = Petal.Width, y = Sepal.Length)) +
  geom_point(size = 4, alpha = 0.3) +
  theme_modern()

new <- ggplot(iris, aes(x = Petal.Width, y = Sepal.Length)) +
  geom_point2(size = 4, alpha = 0.3) +
  theme_modern()

library(patchwork)
normal + new

# geom_point2
normal <- ggplot(iris, aes(x = Petal.Width, y = Sepal.Length)) +
  geom_jitter(size = 4, alpha = 0.3) +
  theme_modern()

new <- ggplot(iris, aes(x = Petal.Width, y = Sepal.Length)) +
  geom_jitter2(size = 4, alpha = 0.3) +
  theme_modern()

normal + new

# pool ball points:  points labelled with observation name
ggplot(iris, aes(x = Petal.Width, y = Sepal.Length, color = Species )) +
  geom_poolpoint(label = rownames(iris)) +
  scale_color_flat_d() +
  theme_modern()

ggplot(iris, aes(x = Petal.Width, y = Sepal.Length, color = Species )) +
  geom_pooljitter(label = rownames(iris)) +
  scale_color_flat_d() +
  theme_modern()

ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violindot() +
  theme_modern()
```

{renv} for pulling out package dependencies setwd() to your working
folder and dependencies() will check all R files recursively, with
packages listed in the Packages column. Then some dplyr to make a vector

```{r}
library(renv)
library(tidyverse)
getwd()
pkgs <- dependencies(path = getwd())
pkgs_unique <- pkgs %>% 
  select(Package) %>% 
  distinct() %>% 
  arrange(Package) %>%  
  pull(Package) # pull a vector out of a column from a dataframe
class(pkgs_unique)
pkgs_unique
```

vegan::make.cepnames \# abbreviate botanical or zoological latin name
into an eight-character name

```{r}
make.cepnames(c("Aa maderoi", "Poa sp.", "Cladina rangiferina",
"Cladonia cornuta", "Cladonia cornuta var. groenlandica",
"Cladonia rangiformis", "Bryoerythrophyllum"))
data(BCI)
colnames(BCI) <- make.cepnames(colnames(BCI))
```

breakaway

```{r}
remotes::install_github("adw96/breakaway", force=TRUE, build_vignettes = TRUE)
library(breakaway)
data(toy_otu_table)
data(toy_metadata)

## For historical reasons we going to call them:
otu_data <- toy_otu_table
meta_data <- toy_metadata
head(meta_data)

vignette("intro-diversity-estimation")
data("toy_otu_table")
build_frequency_count_tables(toy_otu_table)
```

(see also: <https://rdrr.io/cran/phylotools/man/dat2fasta.html>) write
table to fasta file

```{r}
# https://bootstrappers.umassmed.edu/guides/main/r_writeFasta.html
# table should have a column named 'name' and a column named 'seq'
writeFasta <- function(data, filename){
  fastaLines = c()
  for (rowNum in 1:nrow(data)){
    fastaLines = c(fastaLines, as.character(paste(">", data[rowNum,"name"], sep = "")))
    fastaLines = c(fastaLines, as.character(data[rowNum,"seq"]))
  }
  fileConn <- file(filename)
  writeLines(fastaLines, fileConn)
  close(fileConn)
}

require(dplyr)
exampleData = dplyr::data_frame(name = c("seq1", "seq2", "seq3"),
                                seq = c("AAGGTTTTGCCAA", "TTTTGCCAAGGAA", "TTTAAGGTGCCAA"), 
                                other = c("meta1", "meta2", "meta3"))

# writes to working directory
writeFasta(exampleData, "example.fasta")
```

bash example from <https://www.biostars.org/p/324288/#324293> \$1 is
your name column and \$2 is your sequence column, so switch those if the
order is sequence, name

```{bash}
awk '{print ">"$1"\n"$2}' tab.tsv > seqs.fa
```

read in a fasta file and convert to a data frame

```{r}
# read in spikes fasta file (output is a list)
coispikes <- seqinr::read.fasta(file = file.path("..", "8_reference_sequences_datasets", coispikefasta), seqtype = "DNA", as.string = TRUE, forceDNAtolower = FALSE, set.attributes = FALSE, strip.desc = TRUE, whole.header = TRUE)

# use unlist() %>% enframe() to convert list to dataframe 
coispikesdf <- coispikes %>% 
  unlist(recursive = FALSE) %>% 
  enframe(name = "name", value = "seq") 

# bind_rows to combine kelpie otus and spike seqs
kelpie_otus_and_spikes <- bind_rows(coispikesdf, kelpie_otus)

# write to working directory
# writeFasta(kelpie_otus_and_spikes, paste0("kelpie_20200214_BF3BR2_derep_filtered_geneious", ".fas"))
```

<https://stackoverflow.com/questions/4227223/convert-a-list-to-a-data-frame>

```{r}
library(tidyverse)

l <- replicate(
    132,
    list(sample(letters, 20)),
    simplify = FALSE
)

l_tib <- l %>% 
    unlist(recursive = FALSE) %>% 
    enframe() %>% 
    unnest(cols = c("value")) # or unnest(cols = c(value))

l_tib
#> # A tibble: 2,640 x 2
#>     name value
#>    <int> <chr>
#> 1      1     d
#> 2      1     z
#> 3      1     l
#> 4      1     b
#> 5      1     i
#> 6      1     j
#> 7      1     g
#> 8      1     w
#> 9      1     r
#> 10     1     p
#> # ... with 2,630 more rows
```

set to NA, set NA, set a cell to NA

```{r}
library(dplyr)  

df <- data_frame(col1 = c(1, 2, 3, 0),
                 col2 = c(0, 2, 3, 4),
                 col3 = c(1, 0, 3, 0),
                 col4 = c('a', 'b', 'c', 'd'))

df <- na_if(df, 0)
# A tibble: 4 x 4
#    col1  col2  col3 col4 
#   <dbl> <dbl> <dbl> <chr>
# 1     1    NA     1 a    
# 2     2     2    NA b    
# 3     3     3     3 c    
# 4    NA     4    NA d

# if i want to use case_when, i need to choose which kind of NA. Here, period is a character vector, so some cells get set to NA_character_
idx_meta_genomecov <- idx_meta_genomecov %>% 
  mutate(
    period = case_when(
      site == "HOBO-036" & trap == "M1" & period == "S2" ~ NA_character_,
      TRUE ~ as.character(period)
    )
  )

```

```{r}
# Provide real numbers, not scientific notation.
options(scipen = 999)
```

```{r coinertia co-inertia analysis}
library(ade4)
data(doubs)
dudi1 <- dudi.pca(doubs$env, scale = TRUE, scan = FALSE, nf = 3)
dudi2 <- dudi.pca(doubs$fish, scale = FALSE, scan = FALSE, nf = 2)
coin1 <- coinertia(dudi1,dudi2, scan = FALSE, nf = 2)
coin1
summary(coin1)

if(adegraphicsLoaded()) {
  g1 <- s.arrow(coin1$l1, plab.cex = 0.7)
  g2 <- s.arrow(coin1$c1, plab.cex = 0.7)
  g3 <- s.corcircle(coin1$aX, plot = FALSE)
  g4 <- s.corcircle(coin1$aY, plot = FALSE)
  cbindADEg(g3, g4, plot = TRUE)
  g5 <- plot(coin1)
    
} else {
s.arrow(coin1$l1, clab = 0.7)
s.arrow(coin1$c1, clab = 0.7)
par(mfrow = c(1,2))
s.corcircle(coin1$aX)
s.corcircle(coin1$aY)
par(mfrow = c(1,1))
plot(coin1)
}

```

```{r Google scholar}
library(scholar)
library(ggplot2)
library(cowplot)
id <- "muZUfswAAAAJ" # my google scholar ID from the URL
ct <- get_citation_history(id)

predict_h_index(id)
ggplot(ct, aes(year, cites)) +
    geom_line() +
    geom_point() +
    theme_cowplot()

```

```{r ggeffects}
library(ggeffects)
library(sjlabelled)
library(sjmisc)
library(ggplot2)
data(efc)
names(efc)
look_for(efc, details = TRUE) # list variables and labels
get_label(efc$c172code) # "carer's level of education"
get_labels(efc$c172code, values = "n") # 1, 2, 3 are labelled low/intermediate/high levels of education
get_labels(efc$c172code, values = "p") # 1, 2, 3 are labelled low/intermediate/high levels of education
efc %>% distinct(c172code)

fit <- lm(barthtot ~ c12hour + neg_c_7 + c161sex + c172code, data = efc)
summary(fit)

ggpredict(fit, terms = "c12hour")

theme_set(theme_bw())
mydf <- ggpredict(fit, terms = "c12hour")
mydf
ggplot(mydf, aes(x, predicted)) + geom_line()

mydf <- ggpredict(fit, terms = c("c12hour", "c172code")) 
mydf
mean(efc$neg_c_7, na.rm = TRUE)
mean(efc$c161sex, na.rm = TRUE)
# up to 4 other predictors, and the 2-4th predictors are used as grouping levels
# the other two predictors are set to their means
ggplot(mydf, aes(x, predicted, colour = group)) + geom_line()
# predicted is the predicted value of the model for different values of c12hour (x), c172code (low/intermediate/high), and mean values of neg_c_7 and c161sex

mydf <- ggpredict(fit, terms = c("c12hour", "c172code", "c161sex"))
mydf
# three predictors, the second two being binned automatically, with 3rd one being a facet
efc$c161sex # values 1 and 2 are labelled
ggplot(mydf, aes(x = x, y = predicted, colour = group)) + 
  geom_line() + 
  facet_wrap(~facet) # 

# Finally, a third differentation can be defined, creating another column named panel. In such cases, you may create multiple plots (for each value in panel). ggeffects takes care of this when you use plot() and automatically creates an integrated plot with all panels in one figure.
mydf <- ggpredict(fit, terms = c("c12hour", "c172code", "c161sex", "neg_c_7"))
plot(mydf)

# Marginal effects for each model term, if term argument is NULL
mydf <- ggpredict(fit)
mydf


# 2,3,4-way interactions
data(efc)

# make categorical
efc$c161sex <- to_factor(efc$c161sex)
efc$c161sex

# fit model with interaction
fit <- lm(neg_c_7 ~ c12hour + barthtot * c161sex, data = efc)

# select only levels 30, 50 and 70 from continuous variable Barthel-Index
mydf <- ggpredict(fit, terms = c("barthtot [30,50,70]", "c161sex"))
ggplot(mydf, aes(x, predicted, colour = group)) + geom_line()
```

```{r}
library(dplyr)
library(correlation)
correlation(iris)

iris %>%
  select(starts_with("Sepal")) %>%
  correlation()

iris %>%
  select(starts_with("Sepal")) %>%
  correlation(select(iris, starts_with("Petal")))

iris %>%
  group_by(Species) %>%
  correlation()

data <- mtcars
data$cyl <- as.factor(data$cyl)
correlation(data, method = "auto")
```

```{r}
library(corrplot)
M <- cor(mtcars)
corrplot(M, method = "circle")
corrplot(M, method = "ellipse")

cor(mtcars) %>% 
  corrplot(method = "ellipse")


```

Adding predicted lines and CIs from a model
<https://aosmith.rbind.io/2018/11/16/plot-fitted-lines/>

```{r}
library(ggplot2)
library(viridis)
model1 <- lm(ArrivalDate ~ WingLength, data = df1)
# use predict() to generate fitted values + CI values
df1_preds <- predict(model1, interval = "confidence")
df1_preds <- data.frame(df1_preds) # change matrix to dataframe
df1_all <- bind_cols(df1, df1_preds) # bind orig dataset with predictions and CIs

# run ggplot and add a geom_line() (not geom_smooth()) and a geom_ribbon() for the CIs
# for ArrivalDate ~ WingLength
lmgg2 <- ggplot(df1_all, aes(x = WingLength, y = ArrivalDate)) + 
  geom_point(aes(color = Sex), size = 3) + 
  labs(x = "Wing length", y = "Arrival day") + 
  theme_bw() + 
  theme(
    panel.grid.minor = element_blank(), 
    panel.grid.major = element_blank()
  ) + 
  scale_color_viridis(discrete=TRUE) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15) +
  geom_line(aes(x = WingLength, y = fit))
lmgg2

```

ggeffects see this:
<https://aosmith.rbind.io/2018/11/16/plot-fitted-lines/> the tutorial
doesn't use ggeffects, but i think the general ideas could be useful
here

```{r}
# use ggeffects to generate predictions (this is really for complex models,
# doesn't plot datapoints)
model1pred <- ggpredict(model1, terms = "WingLength")
  # the key here is to set the terms correctly for the model. for instance, if i fit a 2-predictor model, i think i might have to set terms = both predictors. have to check

# simple version
ggplot(model1pred, aes(x, predicted)) + 
  geom_line() 

# if i want to plot data points and the fitted model, each geom gets a different dataframe, and for some reason, i need to explicitly use 'data = '
ggplot() + 
  geom_point(data = df1_all, aes(x = WingLength, y = ArrivalDate, color = Sex), size = 3) + 
  labs(x = "Wing length", y = "Arrival day") + 
  theme_cowplot() +
  scale_color_viridis(discrete=TRUE) +
  geom_line(data = model1pred, aes(x = x, y = predicted)) +
  geom_ribbon(data = model1pred, aes(x = x, ymin = conf.low, ymax = conf.high), alpha = 0.15)

```

```{r soccer power index liverpool}
library(tidyverse)
spi <- read_csv("~/Downloads/soccer-spi/spi_matches.csv")
spi <- spi %>% 
	filter(league == "Barclays Premier League" & team1 == "Liverpool")

ggplot(spi, aes(date, spi1)) + 
	geom_line()


```

```{r tidyr::expand}
library(dplyr)
library(tidyr)
# existing combinations of vs & cyl
mtcars %>% distinct(vs, cyl) %>% arrange(vs, cyl)

# All possible combinations of vs & cyl, even those that aren't
# present in the data
expand(mtcars, vs, cyl)
# adds the missing combination (vs, cyl) (1,8)

# Only combinations of vs and cyl that appear in the data
expand(mtcars, nesting(vs, cyl))

# Implicit missings ---------------------------------------------------------
df <- tibble(
  year   = c(2010, 2010, 2010, 2010, 2012, 2012, 2012),
  qtr    = c(   1,    2,    3,    4,    1,    2,    3),
  return = rnorm(7)
)
df 
df %>% expand(year, qtr)
df %>% expand(year = 2010:2012, qtr)
df %>% expand(year = full_seq(year, 1), qtr)
df %>% complete(year = full_seq(year, 1), qtr)

# Nesting -------------------------------------------------------------------
# Each person was given one of two treatments, repeated three times
# But some of the replications haven't happened yet, so we have
# incomplete data:
experiment <- tibble(
  name = rep(c("Alex", "Robert", "Sam"), c(3, 2, 1)),
  trt  = rep(c("a", "b", "a"), c(3, 2, 1)),
  rep = c(1, 2, 3, 1, 2, 1),
  measurement_1 = runif(6),
  measurement_2 = runif(6)
)

# We can figure out the complete set of data with expand()
# Each person only gets one treatment, so we nest name and trt together:
all <- experiment %>% expand(nesting(name, trt), rep)
all

# We can use anti_join to figure out which observations are missing
all %>% anti_join(experiment)

# And use right_join to add in the appropriate missing values to the
# original data
experiment %>% right_join(all)
# Or use the complete() short-hand
experiment %>% complete(nesting(name, trt), rep)

# Generate all combinations with expand():
formulas <- list(
  formula1 = Sepal.Length ~ Sepal.Width,
  formula2 = Sepal.Length ~ Sepal.Width + Petal.Width,
  formula3 = Sepal.Length ~ Sepal.Width + Petal.Width + Petal.Length
)
data <- split(iris, iris$Species)
out <- crossing(formula = formulas, data)


# expand_grid(), which is a tidyverse version of expand.grid()
expand_grid(x = 1:3, y = 1:2)
expand_grid(l1 = letters, l2 = LETTERS)
# Can also expand data frames
(df <- data.frame(x = 1:2, y = c(2, 1)))
expand_grid(df, z = 1:3)
# And matrices
expand_grid(x1 = matrix(1:4, nrow = 2), x2 = matrix(5:8, nrow = 2))
# alt syntax, wrapper around tidyr::expand_grid()
crossing(df, z = 1:3)
crossing(mtcars$vs, mtcars$cyl) 
```

insight, used by ggeffects, to pull out elements from model objects

```{r insight}
library(insight)
library(lme4)
data(sleepstudy)
sleepstudy$mygrp <- sample(1:5, size = 180, replace = TRUE)
sleepstudy$mysubgrp <- NA
sleepstudy$Weeks <- sleepstudy$Days / 7
sleepstudy$cat <- as.factor(sample(letters[1:4], nrow(sleepstudy), replace = TRUE))

for (i in 1:5) {
  filter_group <- sleepstudy$mygrp == i
  sleepstudy$mysubgrp[filter_group] <-
    sample(1:30, size = sum(filter_group), replace = TRUE)
}

model <- lmer(
  Reaction ~ Days + I(Days^2) + log1p(Weeks) + cat +
    (1 | mygrp / mysubgrp) + 
    (1 + Days | Subject),
  data = sleepstudy
)

find_response(model)
find_predictors(model)
find_random(model)
find_random_slopes(model)
find_predictors(model, effects = "all", component = "all")
find_terms(model)

model_info(model)


```

```{r ggsignif}
library(ggplot2)
library(ggsignif)

ggplot(iris, aes(x=Species, y=Sepal.Length)) + 
  geom_boxplot() +
  geom_signif(comparisons = list(c("versicolor", "virginica")), 
              map_signif_level=TRUE)

dat <- data.frame(Group = c("S1", "S1", "S2", "S2"),
                  Sub   = c("A", "B", "A", "B"),
                  Value = c(3,5,7,8))  

ggplot(dat, aes(Group, Value)) +
  geom_bar(aes(fill = Sub), stat="identity", position="dodge", width=.5) +
  geom_signif(y_position=c(5.3, 8.3), xmin=c(0.8, 1.8), xmax=c(1.2, 2.2),
              annotation=c("**", "NS"), tip_length=0) +
  geom_signif(comparisons=list(c("S1", "S2")),
              y_position = 9.3, tip_length = 0, vjust=0.2) +
  scale_fill_manual(values = c("grey80", "grey20"))

ggplot(mpg, aes(class, hwy)) +
 geom_boxplot() +
 geom_signif(comparisons = list(c("compact", "pickup"),
                                c("subcompact", "suv")))

ggplot(mpg, aes(class, hwy)) +
 geom_boxplot() +
 geom_signif(comparisons = list(c("compact", "pickup"),
                                c("subcompact", "suv")),
             map_signif_level=function(p)sprintf("p = %.2g", p))

ggplot(mpg, aes(class, hwy)) +
  geom_boxplot() +
  geom_signif(annotations = c("First", "Second"),
              y_position = c(30, 40), xmin=c(4,1), xmax=c(5,3))

```

```{r tidyr::pivot_longer}
library(tidyr)
# See vignette("pivot") for examples and explanation

# Simplest case where column names are character data
View(relig_income)
relig_income %>%
 pivot_longer(-religion, names_to = "income", values_to = "count")

# Slightly more complex case where columns have common prefix,
# and missing missings are structural so should be dropped.
billboard
billboard %>%
 pivot_longer(
   cols = starts_with("wk"),
   names_to = "week",
   names_prefix = "wk",
   values_to = "rank",
   values_drop_na = TRUE
 )

# Multiple variables stored in colum names
who %>% pivot_longer(
  cols = new_sp_m014:newrel_f65,
  names_to = c("diagnosis", "gender", "age"),
  names_pattern = "new_?(.*)_(.)(.*)",
  values_to = "count"
)

# Multiple observations per row
anscombe
anscombe %>%
 pivot_longer(
     cols = everything(),
     names_to = c(".value", "set"),
     names_pattern = "(.)(.)"
     )
```

```{r fct_inorder}
f <- factor(c("b", "b", "a", "c", "c", "c"))
f
fct_inorder(f)
fct_infreq(f)

fct_inorder(f, ordered = TRUE)

f <- factor(sample(1:10))
fct_inseq(f)
```

```{r}
df <- tibble::tribble(
  ~color,     ~a, ~b,
  "blue",      1,  2,
  "green",     6,  2,
  "purple",    3,  3,
  "red",       2,  3,
  "yellow",    5,  1
)
df$color <- factor(df$color)
fct_reorder(df$color, df$a, min)
fct_reorder2(df$color, df$a, df$b)

boxplot(Sepal.Width ~ Species, data = iris)
boxplot(Sepal.Width ~ fct_reorder(Species, Sepal.Width), data = iris)
boxplot(Sepal.Width ~ fct_reorder(Species, Sepal.Width, .desc = TRUE), data = iris)

chks <- subset(ChickWeight, as.integer(Chick) < 10)
chks <- transform(chks, Chick = fct_shuffle(Chick))

if (require("ggplot2")) {
ggplot(chks, aes(Time, weight, colour = Chick)) +
  geom_point() +
  geom_line()

# Note that lines match order in legend
ggplot(chks, aes(Time, weight, colour = fct_reorder2(Chick, Time, weight))) +
  geom_point() +
  geom_line() +
  labs(colour = "Chick")
}
```

```{r}
library(forcats)
f <- factor(c("a", "b", "c", "d"), levels = c("b", "c", "d", "a"))
f
fct_relevel(f)
fct_relevel(f, "a")
fct_relevel(f, "b", "a")

# Move to the third position
fct_relevel(f, "a", after = 2)

# Relevel to the end
fct_relevel(f, "a", after = Inf)
fct_relevel(f, "a", after = 3)

# Relevel with a function
fct_relevel(f, sort)
fct_relevel(f, sample)
fct_relevel(f, rev)
```

```{r patchwork}
library(ggplot2)
library(patchwork)
p1 <- ggplot(mtcars) + 
  geom_point(aes(mpg, disp)) + 
  ggtitle('Plot 1')

p2 <- ggplot(mtcars) + 
  geom_boxplot(aes(gear, disp, group = gear)) + 
  ggtitle('Plot 2')

p3 <- ggplot(mtcars) + 
  geom_point(aes(hp, wt, colour = mpg)) + 
  ggtitle('Plot 3')

p4 <- ggplot(mtcars) + 
  geom_bar(aes(gear)) + 
  facet_wrap(~cyl) + 
  ggtitle('Plot 4')

p1 + p2
p1 + p2 + labs(subtitle = 'This will appear in the last plot')
p1 + p2 + p3 + p4
p1 + p2 + p3 + p4 + plot_layout(nrow = 3, byrow = FALSE)
p1 / p2
p1 | (p2 / p3)
(p1 | (p2 / p3)) + 
  plot_annotation(title = 'The surprising story about mtcars')
(p1 | (p2 / p3)) + plot_layout(guides = 'collect')
```

anonymizer

```{r}
remotes::install_github("paulhendricks/anonymizer")
library(anonymizer)
library(tidyverse)

View(letters)
letters %>% head
#> [1] "a" "b" "c" "d" "e" "f"
letters %>% head %>% salt(.seed = 1)
#> [1] "gjoxfagjoxf" "gjoxfbgjoxf" "gjoxfcgjoxf" "gjoxfdgjoxf" "gjoxfegjoxf"
#> [6] "gjoxffgjoxf"
letters %>% head %>% salt(.seed = 1) %>% unsalt(.seed = 1)
#> [1] "a" "b" "c" "d" "e" "f"
letters %>% head %>% hash(.algo = "crc32")
#> [1] "c0749952" "597dc8e8" "2e7af87e" "b01e6ddd" "c7195d4b" "5e100cf1"
letters %>% head %>% salt(.seed = 1) %>% hash(.algo = "crc32")
#> [1] "b0891ad8" "361d6876" "fd41bbd3" "e0448b6b" "2b1858ce" "ad8c2a60"
letters %>% head %>% anonymize(.algo = "crc32", .seed = 1)
#> [1] "b0891ad8" "361d6876" "fd41bbd3" "e0448b6b" "2b1858ce" "ad8c2a60"


```

dplyr::coalesce
<https://stackoverflow.com/questions/14563531/combine-column-to-remove-nas>
combine multiple columns into one in R and ignore NA

```{r}
data <- data.frame('a' = c('A','B','C','D','E'),
                 'x' = c(1,2,NA,NA,NA),
                 'y' = c(NA,NA,3,NA,NA),
                 'z' = c(NA,NA,NA,4,5))

data %>% mutate(mycol = coalesce(x,y,z)) %>%
         select(a, mycol)
```

filter out rows where rowSum == 0, using a pronoun remove rows where
rowSums == 0 delete rows where sum of rows == 0

```{r}
(dat <- structure(list(`A-XXX` = c(1.51653275922944, 0.077037240321129, 
0), `fBM-XXX` = c(2.22875185527511, 0, 0), `P-XXX` = c(1.73356698481106, 
0, 0), `vBM-XXX` = c(3.00397859609183, 0, 0)), .Names = c("A-XXX", 
"fBM-XXX", "P-XXX", "vBM-XXX"), row.names = c("BATF::JUN_AHR", 
"BATF::JUN_CCR9", "BATF::JUN_IL10"), class = "data.frame"))

dat_filtered <- dat %>% filter(rowSums(.) != 0)

# more complicated one when i want to sum only sum columns, like in a combined environment and OTU table
# use select(., starts_with()) to pipe a bunch of columns into rowSums and create new variable mbsum. Then filter whole dataset on mbsum. 
fhtnmdf <- fhtnmdf %>% 
    mutate(
        mbsum = select(., starts_with(c("Acti_", "Amph_", "Aves_",
                                     "Mamm_"))) %>% 
    rowSums()
    ) %>% 
    filter(mbsum > 0) %>% 
    select(-mbsum)
```

amalgam
<https://www.biorxiv.org/content/10.1101/2020.02.27.968677v1.full.pdf>

```{r}
remotes::install_github("tpq/amalgam")

# Load package and sample data
library(amalgam) 
data(iris)

# find best amalgamation 
A <- amalgam(x = iris[ , 1:4],
             n.amalgams = 3,
             maxiter = 50,
             objective = objective.keepDist,
             # objective=objective.keepWADIST, # another distance
             # objective=objective.keepSKL, # another distance
             # objective=objective.maxRDA, # if maximizing RDA
             z = iris[ ,5], # only needed if maximizing RDA
             asSLR = FALSE, # if TRUE, n.amalgams must be even
             shrink = FALSE # toggles James−Stein type shrinkage
             )

#visualizeresults
plot(A, col = iris[ ,5])

# objective = objective.keepDist
A[["weights"]] # contribution of each of the measurements to each of the 3 amalgams
    #              [,1] [,2] [,3]
    # Sepal.Length    0    0    0
    # Sepal.Width     0    0    1
    # Petal.Length    0    1    0
    # Petal.Width     1    0    0
# objective=objective.maxRDA
A[["weights"]] 
    #              [,1] [,2] [,3]
    # Sepal.Length    0    1    0
    # Sepal.Width     1    0    0
    # Petal.Length    0    0    1
    # Petal.Width     0    0    1

# second example
set.seed(3220)
(sampleData <- randAcomp(10, 15)) # 10 samples, 15 species
rowSums(sampleData) # all rows should sum to 1, meaning that columns are species and rows are samples

out <- amalgam(sampleData, n.amalgams = 3, z = NULL,
               objective = objective.keepDist,
               weight = weight.Nto1)
out[["amalgams"]] # 10 samples, 3 amalgamated "species"
rowSums(out[["amalgams"]]) # don't sum to 1, i guess because not all species are used.
out[["weights"]] # contribution of each of the 15 species to each of the 3 amalgams; could use this to filter the original dataset, i guess
colSums(out[["weights"]])
    # 3, 4, 3 == 10.  10 of the 15 species contribute to the amalgams
print(out)

fakeClusters <- cutree(hclust(dist(sampleData)), 3)
plot(out, col = fakeClusters, center = TRUE, scale = TRUE)


# third example
simData <- randAcomp(5, 10)
result <- amalgam(simData, n.amalgams = 3, objective = objective.keepDist)
print(result)
plot(result)
```

Use c() to calculate on vectors of numbers

```{r}
median(c(22472,67629,19353,30255,48767,17074,13560,32871))
mean(c(22472,67629,19353,30255,48767,17074,13560,32871))
max(c(22472,67629,19353,30255,48767,17074,13560,32871))
hist(c(22472,67629,19353,30255,48767,17074,13560,32871))

```

paramedic: to estimate absolute abundances from relative abundances +
some absolute abundance estimates of some species

```{r}
# github.com/statdivlab/paramedic
remotes::install_github("statdivlab/paramedic")
remotes::install_github("statdivlab/paramedic", build_vignettes = TRUE, build_manual = TRUE)
```

s-jSDM Pichler

install 4.8.2 miniconda

```{bash }
rm -r miniconda # remove old version
wget https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.2-MacOSX-x86_64.sh -O miniconda.sh
bash miniconda.sh -b -p $HOME/miniconda
export PATH="$HOME/miniconda/bin:$PATH"
hash -r
rm miniconda.sh
conda config --set auto_update_conda false
```

install s-jSDM 1. run these commands in terminal. details here:
`r vignette("Dependencies", package = "sjSDM")`

```{bash}
# install miniconda from miniconda website. miniconda is agnostic to python2 and 3

conda create --name sjSDM_env python=3.7 # choose py3
# conda init bash # first time
conda activate sjSDM_env # now in py3 env
conda install pytorch torchvision cpuonly -c pytorch # cpu
```

2.  run these in R

```{r install sjSDM}
# workaround:  download file from https://github.com/TheoreticalEcology/s-jSDM/files/4553935/sjSDM_0.0.2.9000.tar.gz
# install.packages("~/Downloads/sjSDM_0.0.2.9000.tar.gz", repos=NULL)
# install.packages("~/Downloads/sjSDM_0.0.2.9000.tar.gz", repos=NULL, INSTALL_opts = "--no-test-load") # currently needs this INSTALL_opts

# install from github
remotes::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM", build_vignettes = TRUE, build_manual = TRUE, force = TRUE) # doesn't work
# remotes::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM", INSTALL_opts = "--no-test-load", force = TRUE)


# these shouldn't be needed except in the initial installation
# library(sjSDM)
# install_sjSDM(version = "cpu", conda_python_version = "3.6") 
# 
# # if this fails because it cannot find torch_optimizer, run this first:
# reticulate::conda_install(
#           packages = "torch_optimizer",
#           envname = "r-reticulate",
#           conda = "auto",
#           pip = TRUE
#         )
# # then 
# library(sjSDM)
```

sjSDM on ada GPU

```{bash}
# send to terminal:  cmd-alt-fn-return
# ssh b042@ada.uea.ac.uk
ssh ada
interactive-gpu # log onto ADA gpu nodes
source /gpfs/home/b042/scratch/sjSDM_env/bin/activate 
  # This will make your prompt appear as follows: (sjSDM_env) [b042@<gpunode>]
  # Amjad set the following in my /gpfs/home/.Rprofile
  # Sys.setenv(RETICULATE_PYTHON="/gpfs/scratch/b042/sjSDM_env/bin/python")
# to run in ada batch mode, the source command does not work. 
  # instead, add this line to the top of my R script:
  # Sys.setenv(RETICULATE_PYTHON="/gpfs/scratch/b042/sjSDM_env/bin/python")

module add R
# module add R/4.0.0
R
# to install updates
# remotes::install_github("https://github.com/TheoreticalEcology/s-jSDM", subdir = "sjSDM") # currently at sjSDM_0.1.0.9000.tar.gz
```

```{r test the installation}
library(sjSDM)
getwd() # "/gpfs/home/b042"
setwd("~/sjSDM")
getwd() # "/gpfs/home/b042/sjSDM"
# community <- simulate_SDM(env = 3L, species = 5L, sites = 100L)
community <- simulate_SDM(sites = 100, species = 10, env = 5)
Env <- community$env_weights
Occ <- community$response
Sp <- matrix(rnorm(800), 100, 2) # spatial coordinates (no effect on species occurrences)
objects()
 
model <- sjSDM(Y = Occ, 
               env = linear(data = Env, 
                            formula = ~0 + X1*X2 + X3 + X4),
               spatial = linear(data = Sp, 
                                formula = ~0 + X1:X2), 
               se = TRUE,
               device = "cpu" # "gpu"
               )
summary(model)
saveRDS(model, file = "model.RDS")
# model2 <- readRDS("model.RDS"); objects()
Rsquared(model)
Rsquared(model2)


imp <- importance(model)
print(imp)
pdf(file = "imp.pdf")
plot(imp)
dev.off()


an = anova(model, cv = FALSE)
print(an)
pdf(file = "an.pdf")
plot(an)
dev.off()

# Working with sjSDM
com = simulate_SDM(env = 3L, 
                   species = 5L, 
                   sites = 100L
                   )
model = sjSDM(Y = com$response, 
              env = com$env_weights, 
              iter = 10L,
              device = "gpu"
              )
coef(model)
summary(model)
getCov(model)

# Adding quadratic predictors and interactions
model = sjSDM(Y = com$response, 
              env = linear(data = com$env_weights, 
                           formula = ~ X1*X2,), 
              iter = 50L, 
              se = TRUE,
              device = "gpu"
              )
summary(model)
# quadratic effect without intercept
model = sjSDM(Y = com$response, 
              env = linear(data = com$env_weights, 
                           formula = ~0+ I(X1^2)), 
              iter = 50L, 
              se = TRUE,
              device = "gpu"
              )
summary(model)


# Regularisation on abiotic coefficients
# sjSDM supports l1 (lasso) and l2 (ridge) regularization: 
  # * alpha is the weighting between lasso and ridge 
  # * alpha = 0.0 corresponds to pure lasso 
  # * alpha = 1.0 corresponds to pure ridge
model = sjSDM(Y = com$response, 
              env = linear(data = com$env_weights, 
                           formula = ~0+ I(X1^2), 
                           lambda = 0.5,
                           alpha = 0.5
                           ), 
              iter = 50L)
summary(model)

# regularization on species-species associations
model = sjSDM(Y = com$response, 
              env = linear(data = com$env_weights, 
                           formula = ~0+ I(X1^2), 
                           lambda = 0.5,
                           alpha = 0.5
                           ),
              biotic = bioticStruct(lambda = 0.1, 
                                    alpha = 0.5
                                    ),
              iter = 50L,
              device = "gpu")
summary(model)

# Regularization on the inverse covariance matrix (precision) is also supported
model = sjSDM(Y = com$response, 
              env = linear(data = com$env_weights, 
                           formula = ~0+ I(X1^2), 
                           lambda = 0.5
                           ), 
              biotic = bioticStruct(lambda =0.1, 
                                    on_diag = FALSE, 
                                    inverse = TRUE
                                    ), 
              iter = 50L,
              device = "gpu"
              )
summary(model)

# Fitting a non-linear (deep neural network) model
com = simulate_SDM(env = 3L, species = 5L, sites = 100L)
X = com$env_weights
Y = com$response

# three fully connected layers with relu as activation function
model = sjSDM(Y = Y, 
              env = DNN(data = X, 
                        formula = ~., 
                        hidden = c(10L, 10L, 10L), 
                        activation = "relu"
                        ), 
              iter = 50L, 
              se = TRUE,
              device = "gpu"
              )
summary(model)

# The methods for sjSDM() work also for the non-linear model:
getCov(model) # species association matrix
pred = predict(model) # predict on fitted data
pred = predict(model, newdata = X) # predict on new data

weights = getWeights(model) # get layer weights and sigma
setWeights(model, weights)
plot(model) # does not work


# Adding a spatial term
com = simulate_SDM(env = 3L, species = 5L, sites = 100L)
X = com$env_weights
Y = com$response

XYcoords = matrix(rnorm(200), 100, 2)
plot(XYcoords)

model = sjSDM(Y, 
              env = linear(X, ~X1+X2), 
              spatial = linear(XYcoords, ~0+X1:X2))
summary(model)

# Regularization on the spatial model:
model = sjSDM(Y, 
              env = linear(X, ~X1+X2), 
              spatial = linear(XYcoords, ~0+X1:X2, lambda = 0.4)
              )
summary(model)

# We can also use a non-linear model as a spatial model:
model = sjSDM(Y, 
              env = linear(X, ~X1+X2), 
              spatial = DNN(XYcoords, ~0+X1:X2, lambda = 0.4)
              )
summary(model)

# Or for both:
model = sjSDM(Y, 
              env = DNN(X, ~X1+X2), 
              spatial = DNN(XYcoords, ~0+X1:X2, lambda = 0.4)
              )
summary(model)



# sjSDM_cv()
# simulate sparse community:
com = simulate_SDM(env = 5L, species = 25L, sites = 50L, sparse = 0.5)

# tune regularization:
tune_results = sjSDM_cv(Y = com$response,
                        env = com$env_weights, 
                        tune = "random", # random steps in tune-parameter space
                        CV = 3L, # nrow(com$response), # 3L is 3-fold CV
                        tune_steps = 5L,
                        alpha_cov = seq(0, 1, 0.1),
                        alpha_coef = seq(0, 1, 0.1),
                        lambda_cov = seq(0, 0.1, 0.001), 
                        lambda_coef = seq(0, 0.1, 0.001),
                        n_cores = NULL, # or 2L, # small models can be also run in parallel on the GPU
                        iter = 2L # we can pass arguments to sjSDM via ...
                        )

# print overall results:
tune_results

# summary (mean values over CV for each tuning step)
summary(tune_results)

# visualize tuning and best points:
(best = plot(tune_results, perf = "logLik"))
best[["lambda_coef"]]
best[["alpha_coef"]]
 
# fit model with new regularization parameters:
model = sjSDM(Y = com$response,
              env = linear(com$env_weights, 
                              lambda = best[["lambda_coef"]],
                              alpha = best[["alpha_coef"]]),
              biotic = bioticStruct(lambda = best[["lambda_cov"]],
                                    alpha = best[["alpha_cov"]])
              )

summary(model)
```

running sjSDM on the ada GPU, using

```{r}
# test code
set.seed(42)
community <- simulate_SDM(sites = 400, species = 10, env = 3)
Env <- community$env_weights
Occ <- community$response
SP <- matrix(rnorm(800, 0, 0.3), 400, 2) # spatial coordinates (no effect on species occurences)
model <- sjSDM(Y = Occ, 
               env = linear(data = Env, formula = ~X1+X2+X3), 
               spatial = linear(data = SP, formula = ~0+X1:X2), 
               se = TRUE,
               device = "gpu"
               )
summary(model)
imp=importance(model)
print(imp)

# sjSDM_cv()
# simulate sparse community:
com = simulate_SDM(env = 5L, 
                   species = 25L, 
                   sites = 100L, 
                   sparse = 0.3
                   )
summary(com)
nrow(com$response) # should be same as number of sites
# tune regularization:
tune_results = sjSDM_cv(Y = com$response,
                        env = com$env_weights, 
                        tune = "random", # random steps in tune-parameter space
                        # CV = nrow(com$response), # leave-one-out cross valid
                        CV = 3L, # 3L is 3-fold cross validation
                        tune_steps = 60L, # 25L,
                        alpha_cov = seq(0, 1, 0.1),
                        alpha_coef = seq(0, 1, 0.1),
                        lambda_cov = seq(0, 0.1, 0.001), 
                        lambda_coef = seq(0, 0.1, 0.001),
                        n_cores = 10L, # run this many sjsdm models at once. 24 would be too much, since there are only 2 GPU cards over which to distribute the sjsdm models
                        n_gpu = 2, # spread over this many GPU cards
                        iter = 150L, # 2L, 
                        # can pass arguments to sjSDM via ...
                        # device = 1 # choose the gpu card if i want
                        )
# for larger models, reduce n_cores so that i do not overload the 2 GPUs
# for leave-one-out cv, set CV = number_of_sites (here, 100) using nrow(com$response)
# https://rscs.uea.ac.uk/ada/using-ada/jobs/gpu
# each gpu node has 2 NVIDIA Quadro P5000 cards, each with 2560 GPU cores

# print overall results:
tune_results

# summary (mean values over CV for each tuning step)
summary(tune_results)
tune_results_loocv <- tune_results
tune_results_cv3 <- tune_results
ls()

# visualize tuning and best points:
# pdf(file = "best_test.pdf")
# plot(tune_results, perf = "logLik")
# dev.off()
# green points in the best plot are (close to) the best lambda and alpha values
best = plot(tune_results, perf = "logLik") # prints a pdf and saves results in best object


# fit model with new regularization paramter:
model <- sjSDM(Y = com$response, 
                env = linear(com$env_weights, 
                            lambda = best[["lambda_coef"]],
                            alpha = best[["alpha_coef"]]
                            ),
                biotic = bioticStruct(lambda = best[["lambda_cov"]],
                                      alpha = best[["alpha_cov"]]
                                      ),
                device = "gpu"
              )

summary(model)
```

remove singleton OTUs, remove single-read OTUs, remove single read OTUs

```{r}
community[community == 1] <- 0
```

ggeasy

```{r}
library(ggplot2)
library(ggeasy)

# Add legend title to a specific aesthetic
ggplot(mtcars, aes(wt, mpg, colour = cyl, size = hp)) +
  geom_point() + easy_add_legend_title(col = "Number of Cylinders")

# Add legend title to all aesthetics
ggplot(mtcars, aes(wt, mpg, colour = cyl)) +
  geom_point() + easy_add_legend_title("Number of Cylinders")

# Move legends to bottom
ggplot(mtcars, aes(wt, mpg, colour = cyl, size = hp)) +
  geom_point() + easy_move_legend("bottom")

# Make legends horizontal
ggplot(mtcars, aes(wt, mpg, colour = cyl, size = hp)) +
  geom_point() + easy_rotate_legend("horizontal")

# Justify legends to the bottom and justify to the right
ggplot(mtcars, aes(wt, mpg, colour = cyl, size = hp)) +
  geom_point() +
  easy_move_legend("bottom") +
  easy_adjust_legend("right")

# make all text larger
ggplot(mtcars, aes(mpg, hp)) +
  geom_point() +
  easy_all_text_size(22)
# also works if accidentally using easy_text_size(n)

# make the x and y axis text larger
ggplot(mtcars, aes(mpg, hp)) +
  geom_point() +
  easy_text_size(c("axis.text.x", "axis.text.y"), 22)

# make the x axis labels larger
ggplot(mtcars, aes(mpg, hp)) +
  geom_point() +
  easy_x_axis_labels_size(22)

# make the plot title larger
ggplot(mtcars, aes(mpg, hp)) +
  geom_point() +
  labs(title = "My Plot") +
  easy_plot_title_size(22)

# make the legend title larger
ggplot(mtcars, aes(mpg, hp)) +
  geom_point(aes(fill = gear)) +
  easy_plot_legend_title_size(22)

# make all the text red
ggplot(mtcars, aes(mpg, hp)) +
  geom_point(aes(fill = gear)) +
  easy_all_text_color("red")

# make all the text 45 degrees, right-justified
ggplot(mtcars, aes(mpg, hp)) +
  geom_point(aes(fill = gear)) +
  easy_change_text(what = "angle", to = 45) +
  easy_change_text(what = "hjust", to = 1)

# make just x-axis text 45 degrees, right-justified
ggplot(mtcars, aes(mpg, hp)) +
  geom_point(aes(fill = gear)) +
  easy_change_text(which = "axis.text.x", what = "angle", to = 45) +
  easy_change_text(which = "axis.text.x", what = "hjust", to = 1)

# there is more in the helpfile
```

This one-liner is if i want to get a vector that shows me when an
element is duplicated and then only keep the rows that are not
duplicates or the first row of any duplicate.

```{r}
otu_MTB_E_uniques <- otu_MTB_E[duplicated(otu_MTB_E$MTBseq) == FALSE, ]
```

{taxa}
<https://grunwaldlab.github.io/metacoder_documentation/workshop--05--plotting.html>

```{r}
library(taxa)
conflict_prefer("recode", "car")
# Read a vector of classifications
 my_taxa <- c("Mammalia;Carnivora;Felidae",
              "Mammalia;Carnivora;Felidae",
              "Mammalia;Carnivora;Ursidae")
 parse_tax_data(my_taxa, class_sep = ";")

 # Read a list of classifications
 my_taxa <- list("Mammalia;Carnivora;Felidae",
                "Mammalia;Carnivora;Felidae",
                "Mammalia;Carnivora;Ursidae")
 parse_tax_data(my_taxa, class_sep = ";")

 # Read classifications in a table in a single column
 species_data <- data.frame(tax = c("Mammalia;Carnivora;Felidae",
                                    "Mammalia;Carnivora;Felidae",
                                    "Mammalia;Carnivora;Ursidae"),
                           species_id = c("A", "B", "C"))
 parse_tax_data(species_data, class_sep = ";", class_cols = "tax")

 # Read classifications in a table in multiple columns
 species_data <- data.frame(lineage = c("Mammalia;Carnivora;Felidae",
                                        "Mammalia;Carnivora;Felidae",
                                        "Mammalia;Carnivora;Ursidae"),
                            species = c("Panthera leo",
                                        "Panthera tigris",
                                        "Ursus americanus"),
                            species_id = c("A", "B", "C"))
 parse_tax_data(species_data, class_sep = c(" ", ";"),
                class_cols = c("lineage", "species"))

 # Read classification tables with one column per rank
 species_data <- data.frame(class = c("Mammalia", "Mammalia", "Mammalia"),
                            order = c("Carnivora", "Carnivora", "Carnivora"),
                            family = c("Felidae", "Felidae", "Ursidae"),
                            genus = c("Panthera", "Panthera", "Ursus"),
                            species = c("leo", "tigris", "americanus"),
                            species_id = c("A", "B", "C"))
  parse_tax_data(species_data, class_cols = 1:5)
  parse_tax_data(species_data, class_cols = 1:5,
                 named_by_rank = TRUE) # makes `taxon_ranks()` work

 # Classifications with extra information
 my_taxa <- c("Mammalia_class_1;Carnivora_order_2;Felidae_genus_3",
              "Mammalia_class_1;Carnivora_order_2;Felidae_genus_3",
              "Mammalia_class_1;Carnivora_order_2;Ursidae_genus_3")
 parse_tax_data(my_taxa, class_sep = ";",
                class_regex = "(.+)_(.+)_([0-9]+)",
                class_key = c(my_name = "taxon_name",
                              a_rank = "taxon_rank",
                              some_num = "info"))


  # --- Parsing multiple datasets at once (advanced) ---
  # The rest is one example for how to classify multiple datasets at once.

  # Make example data with taxonomic classifications
  species_data <- data.frame(tax = c("Mammalia;Carnivora;Felidae",
                                     "Mammalia;Carnivora;Felidae",
                                     "Mammalia;Carnivora;Ursidae"),
                             species = c("Panthera leo",
                                         "Panthera tigris",
                                         "Ursus americanus"),
                             species_id = c("A", "B", "C"))

  # Make example data associated with the taxonomic data
  # Note how this does not contain classifications, but
  # does have a varaible in common with "species_data" ("id" = "species_id")
  abundance <- data.frame(id = c("A", "B", "C", "A", "B", "C"),
                          sample_id = c(1, 1, 1, 2, 2, 2),
                          counts = c(23, 4, 3, 34, 5, 13))

  # Make another related data set named by species id
  common_names <- c(A = "Lion", B = "Tiger", C = "Bear", "Oh my!")

  # Make another related data set with no names
  foods <- list(c("ungulates", "boar"),
                c("ungulates", "boar"),
                c("salmon", "fruit", "nuts"))

  # Make a taxmap object with these three datasets
  x = parse_tax_data(species_data,
                     datasets = list(counts = abundance,
                                     my_names = common_names,
                                     foods = foods),
                     mappings = c("species_id" = "id",
                                  "species_id" = "{{name}}",
                                  "{{index}}" = "{{index}}"),
                     class_cols = c("tax", "species"),
                     class_sep = c(" ", ";"))

  # Note how all the datasets have taxon ids now
  x$data

  # This allows for complex mappings between variables that other functions use
  map_data(x, my_names, foods)
  map_data(x, counts, my_names)
```

changing BLAS

```{r}
# Improved R Performance with OpenBLAS and vecLib Libraries
# 
# Every time i install a new version of R, i should do this.  The veclib library is said by CRAN to be faster but possibly not as accurate
# 
# http://blog.quadrivio.com/2015/06/improved-r-performance-with-openblas.html
# https://jeffreybreen.wordpress.com/2010/10/21/veclib-open-source-r-for-mac/
# 
# Dramatic performance improvements are available by using an alternative to the BLAS ("Basic Linear Algebra Subprograms") library that is shipped with R. Current alternatives to the R "Reference" (i.e. default) library include: 
# 
# 	•	OpenBLAS. Open-source library.
# 	•	ATLAS. Open-source library.
# 	•	Apple Accelerate vecLib. Optimized library for Macintosh. Proprietary, but included automatically with OS X.
# 	•	Intel MKL (Math Kernel Library). Proprietary and expensive, although included without charge with Revolution Open R.

# R code benchmarking

d <- 5e3

# R Reference BLAS (the original one)
system.time({ x <- matrix(rnorm(d^2),d,d); tcrossprod(x) })
#    user  system elapsed 
# 186.718   1.483 189.839 
system.time({ x <- matrix(rnorm(d^2),d,d); solve(x) })
#    user  system elapsed 
# 273.504   1.909 277.557
# Switching from the Reference R BLAS library to Apple's vecLib library is quite easy, although the official R FAQ on the subject is slightly misleading. The symbolic link given on the R FAQ page refers to an older version of R and is no longer correct. The current procedure, as of OS X 10.10.3 and R 3.2.1 is:

# Unix code
# to use vecLib (the faster one)
# in unix
# cd /Library/Frameworks/R.framework/Resources/lib
# ln -sf /System/Library/Frameworks/Accelerate.framework/Frameworks/vecLib.framework/Versions/Current/libBLAS.dylib libRblas.dylib
# Restart R session

# to return to the original and default R reference BLAS
# cd /Library/Frameworks/R.framework/Resources/lib
# ln -sf libRblas.0.dylib libRblas.dylib

# vecLib
d <- 5e3
system.time({ x <- matrix(rnorm(d^2),d,d); tcrossprod(x) })
 #   user  system elapsed 
 # 11.030   0.266   3.876
system.time({ x <- matrix(rnorm(d^2),d,d); solve(x) })
 #   user  system elapsed 
 # 23.432   0.621   6.830
```

```{r ggrepel}
library(ggrepel)
library(tidyverse)
library(patchwork)
set.seed(42)

dat <- subset(mtcars, wt > 2.75 & wt < 3.45)
dat$car <- rownames(dat)

p <- ggplot(dat, aes(wt, mpg, label = car)) +
  geom_point(color = "red")

p1 <- p + geom_text() + labs(title = "geom_text()")

p2 <- p + geom_text_repel() + labs(title = "geom_text_repel()")

p1 + p2


set.seed(42)

dat2 <- subset(mtcars, wt > 3 & wt < 4)
# Hide all of the text labels.
dat2$car <- ""
# Let's just label these items.
ix_label <- c(2,3,16)
dat2$car[ix_label] <- rownames(dat2)[ix_label]

ggplot(dat2, aes(wt, mpg, label = car)) +
  geom_point(color = ifelse(dat2$car == "", "grey50", "red")) +
  geom_text_repel()

set.seed(42)
ggplot(dat, aes(wt, mpg, label = car)) +
  geom_point(color = "red") +
  geom_text_repel(point.padding = NA)

set.seed(42)

# All labels should be to the right of 3.
x_limits <- c(3, NA)

ggplot(dat, aes(wt, mpg, label = car, color = factor(cyl))) +
  geom_vline(xintercept = x_limits, linetype = 3) +
  geom_point() +
  geom_label_repel(
    arrow = arrow(length = unit(0.03, "npc"), type = "closed", ends = "first"),
    force = 10,
    xlim  = x_limits
  ) +
  scale_color_discrete(name = "cyl")


set.seed(42)

ggplot(mtcars, aes(x = wt, y = 1, label = rownames(mtcars))) +
  geom_point(color = "red") +
  geom_text_repel(
    nudge_y      = 0.05,
    direction    = "x",
    angle        = 90,
    vjust        = 0,
    segment.size = 0.2
  ) +
  xlim(1, 6) +
  ylim(1, 0.8) +
  theme(
    axis.line.y  = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y  = element_blank(),
    axis.title.y = element_blank()
  )
```

save list of packages
<https://www.r-bloggers.com/quick-way-of-installing-all-your-old-r-libraries-on-a-new-device/>
<https://gist.github.com/tjmahr/8a1535fb7c89c4f6c59c57affaba2353>

```{r}
# I recently bought a new laptop and began installing essential software all over again, including R of course! And I wanted all the libraries that I had installed in my previous laptop. Instead of installing libraries one by one all over again, I did the following:

# Step 1: Save a list of packages installed in your old computing device (from your old device).
installed <- as.data.frame(installed.packages())
write.csv(installed, '~/Desktop/installed_previously.csv')

# This saves information on installed packages in a csv file named installed_previously.csv. Now copy or e-mail this file to your new device and access it from your working directory in R.
# Step 2: Create a list of libraries from your old list that were not already installed when you freshly download R (from your new device).
installedPreviously <- read.csv('installed_previously.csv')
baseR <- as.data.frame(installed.packages())
toInstall <- setdiff(installedPreviously, baseR)

# We now have a list of libraries that were installed in your previous computer in addition to the R packages already installed when you download R. So you now go ahead and install these libraries.
# 
# Step 3: Download this list of libraries.
install.packages(toInstall)

.libPaths()
```

<https://rstudio.github.io/thematic/index.html> Automatic theming. not
working yet as of 29 Apr 2020. needs github version of rmarkdown

```{r}
# remotes::install_github("rstudio/thematic")
# install.packages('htmltools')
# install.packages('rmarkdown')

library(thematic)
thematic_on(
  bg = "#222222", fg = "white", accent = "#0CE3AC",
  font = font_spec("Oxanium", scale = 1.25)
)

library(ggplot2)
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  geom_smooth()

ggplot(mtcars, aes(wt, mpg)) +
  geom_point(aes(color = factor(cyl))) +
  geom_smooth(color = "white")

ggplot(mtcars, aes(wt, mpg)) +
  geom_point(aes(color = cyl)) +
  geom_smooth(color = "white")

thematic_off()
```

```{r}
library(relimp)
set.seed(182)  ## an arbitrary number, just for reproducibility
x <- rnorm(100)
z <- rnorm(100)
w <- rnorm(100)
y <- 3 + (2 * x) + z + w + rnorm(100)
test <- lm(y ~ x + z + w)
print(test)
relimp(test, 2, 3)    #  compares effects of x and z
relimp(test, 2, 3:4)  #  compares effect of x with that of (z,w) combined
##
##  Data on housing and satisfaction, from Venables and Ripley
##  -- multinomial logit model
library(MASS)
library(nnet)
data(housing)
house.mult <- multinom(Sat ~ Infl + Type + Cont, weights = Freq,
  data = housing)
relimp(house.mult, set1 = 2:3, set2 = 7, response.cat = "High")
```

kable

```{r}
library(knitr)
library(kableExtra)
dt <- mtcars[1:5, 1:6]
dt
kable(dt)

dt %>%
  kable() %>%
  kable_styling()
```

Convert R to Rmd Convert R code to Rmd code Convert R script to Rmd

```{r}
library(knitr)
spin()
```

quasi-probabilities to create [0,1] interval data from within-species
abundance data from spike-in metabarcoded datasets, for sjSDM

```{r}
library(scales)
-100:100
rescale(-100:100)
rescale(runif(50))
rescale(1)

# example code from sjsdm help text, where the Y data are count data, including 0s. 
Y_pa = scales::rescale(log(Y+0.001))
Y_pa
```

calculate mean of each row
<https://dplyr.tidyverse.org/dev/articles/rowwise.html>

```{r}
df_all <- df_all %>% 
    rowwise %>% # a kind of group_by() but for each row
    mutate(
        score_mean = mean(c(score_predator, score_rabbit, score_pop))
    )


df <- tibble(x = 1:2, y = 3:4, z = 5:6)

df %>% mutate(m = mean(c(x, y, z))) # without rowwise
#> # A tibble: 2 x 4
#>       x     y     z     m
#>   <int> <int> <int> <dbl>
#> 1     1     3     5   3.5
#> 2     2     4     6   3.5

df %>% rowwise() %>% mutate(m = mean(c(x, y, z))) # with rowwise
#> # A tibble: 2 x 4
#> # Rowwise: 
#>       x     y     z     m
#>   <int> <int> <int> <dbl>
#> 1     1     3     5     3
#> 2     2     4     6     4

```

```{r time series heat map}
library(ggplot2)
library(dplyr) # easier data wrangling 
library(viridis) # colour blind friendly palette, works in B&W also
library(Interpol.T) #  will generate a large dataset on initial load
library(lubridate) # for easy date manipulation
library(ggExtra) # because remembering ggplot theme options is beyond me
library(tidyr) 
 
 
data <- data(Trentino_hourly_T,package = "Interpol.T")
 
names(h_d_t)[1:5]<- c("stationid","date","hour","temp","flag")
df <- tbl_df(h_d_t) %>%
  filter(stationid =="T0001")
 
df <- df %>% mutate(year = year(date),
                  month = month(date, label=TRUE),
                  day = day(date))
  
df$date<-ymd(df$date) # not necessary for plot but 
#useful if you want to do further work with the data
 
#cleanup
rm(list=c("h_d_t","mo_bias","Tn","Tx",
          "Th_int_list","calibration_l",
          "calibration_shape","Tm_list"))
 
 
#create plotting df
df <-df %>% select(stationid,day,hour,month,year,temp)%>%
        fill(temp) #optional - see note below
 
# Re: use of fill
# This code is for demonstrating a visualisation technique
# There are 5 missing hourly values in the dataframe.
 
# see the original plot here (from my ggplot demo earlier this year) to see the white spaces where the missing values occcur:
# https://github.com/johnmackintosh/ggplotdemo/blob/master/temp8.png 
 
# I used 'fill' from  tidyr to take the prior value for each missing value and replace the NA
# This is a quick fix for the blog post only - _do not_ do this with your real world data
 
# Should really use either use replace_NA or complete(with fill)in tidyr 
# OR 
# Look into more specialist way of replacing these missing values -e.g. imputation.
 

df <- data.frame(
  x = rep(c(2, 5, 7, 9, 12), 2),
  y = rep(c(1, 2), each = 5),
  z = factor(rep(1:5, each = 2)),
  w = rep(diff(c(0, 4, 6, 8, 10, 14)), 2)
)
ggplot(df, aes(x, y)) +
  geom_tile(aes(fill = z), colour = "grey50")

ggplot(df, aes(x, y, width = w)) +
  geom_tile(aes(fill = z), colour = "grey50")
ggplot(df, aes(xmin = x - w / 2, xmax = x + w / 2, ymin = y, ymax = y + 1)) +
  geom_rect(aes(fill = z), colour = "grey50")



 
 
statno <-unique(df$stationid)
 
 
 
######## Plotting starts here#####################
p <-ggplot(df,aes(day,hour,fill=temp))+
  geom_tile(color= "white",size=0.1) + 
  scale_fill_viridis(name="Hrly Temps C",option ="C")
p <-p + facet_grid(year~month)
p <-p + scale_y_continuous(trans = "reverse", breaks = unique(df$hour))
p <-p + scale_x_continuous(breaks =c(1,10,20,31))
p <-p + theme_minimal(base_size = 8)
p <-p + labs(title= paste("Hourly Temps - Station",statno), x="Day", y="Hour Commencing")
p <-p + theme(legend.position = "bottom")+
  theme(plot.title=element_text(size = 14))+
  theme(axis.text.y=element_text(size=6)) +
  theme(strip.background = element_rect(colour="white"))+
  theme(plot.title=element_text(hjust=0))+
  theme(axis.ticks=element_blank())+
  theme(axis.text=element_text(size=7))+
  theme(legend.title=element_text(size=8))+
  theme(legend.text=element_text(size=6))+
  removeGrid()#ggExtra
 
# you will want to expand your plot screen before this bit!
p #awesomeness
```

<http://pages.cs.wisc.edu/~songwang/disc11_split-plot.html>

```{r}
Dat <- data.frame(list(
  observation = 1:24,
  replicate = rep(c("I","II","III"),each =8),
  A = rep(rep(c("A0","A1"),each = 4),3),
  B = rep(c("B0","B1","B2","B3"),times = 6 ),
  yield = c(13.8,15.5,21.0,18.9,19.3,22.2,25.3,25.9,
            13.5,15.0,22.7,18.3,18.0,24.2,24.8,26.7,
            13.2,15.2,22.3,19.6,20.5,25.4,28.4,27.6)
  ))

with(Dat, table(A, B, replicate))



pigment <- data.frame(list(
  batch = rep(1:15,each=4),
  sample = rep(1:30,each =2),
  test = 1:60,
  y = c( 40,39,30,30,26,28,25,26,29,28,14,15,30,31,24,24, 
         19,20,17,17,33,32,26,24,23,24,32,33,34,34,29,29,
         27,27,31,31,13,16,27,24,25,23,25,27,29,29,31,32,
         19,20,29,30,23,23,25,25,39,37,26,28)
  ))
pigment$batch <- as.factor(pigment$batch)
pigment$sample <- as.factor(pigment$sample)
pigment$test <- as.factor(pigment$test)
aov.pigment <- aov(y~batch+sample+test,data=pigment)
summary(aov.pigment)
```

```{r}
library(vegan)
library(aPCoA)
data("Tasmania")
data<-data.frame(treatment=Tasmania$treatment, block=Tasmania$block)
bray<-vegdist(Tasmania$abund, method="bray")
bray
rownames(data)<-rownames(as.matrix(bray))
opar<-par(mfrow=c(1,2),
          mar=c(3.1, 3.1, 3.1, 5.1),
          mgp=c(2, 0.5, 0),
          oma=c(0, 0, 0, 4))
result<-aPCoA(bray~block, data, treatment)
par(opar)
```

```{r}
library(GGally)
library(tidyverse)
data(nasa)
nasaLate <- nasa[
   nasa$date >= as.POSIXct("1998-01-01") &
   nasa$lat >= 20 &
   nasa$lat <= 40 &
   nasa$long >= -80 &
   nasa$long <= -60
 , ]
 temp.gly <- glyphs(nasaLate, "long", "day", "lat", "surftemp", height=2.5)
 ggplot2::ggplot(temp.gly, ggplot2::aes(gx, gy, group = gid)) +
   add_ref_lines(temp.gly, color = "grey90") +
   add_ref_boxes(temp.gly, color = "grey90") +
   ggplot2::geom_path() +
   ggplot2::theme_bw() +
   ggplot2::labs(x = "", y = "")
 
 
 data(nasa)
temp.gly <- glyphs(nasa, "long", "day", "lat", "surftemp", height=2.5)
ggplot(temp.gly, ggplot2::aes(gx, gy, group = gid)) +
  add_ref_lines(temp.gly, color = "grey90") +
  add_ref_boxes(temp.gly, color = "grey90") +
  geom_path() +
  theme_bw() +
  labs(x = "", y = "")
```

Nice helper functions to copy data and paste in R-usable formats like
tribble functions and vectors

```{r}
library(tidyverse)
library(datapasta)

# select 1 2 3
1 2 3
# choose addins fiddle_selection, and you get
c(1,2,3)

Mint    Fedora  Debian  Ubuntu  OpenSUSE
# copy 'Mint    Fedora  Debian  Ubuntu  OpenSUSE' and choose addins paste as vector, and you get
c("Mint", "Fedora", "Debian", "Ubuntu", "OpenSUSE")


some stuff I typed
# select 'some stuff I typed' and choose addins fiddle selection and you get
c(some, stuff, I, typed)

# select c(some, stuff, I, typed) and choose addins toggle vector quotes, and you get
c("some","stuff","I","typed")

iris %>%
  head() %>%
  dmdclip()

```

boral

```{r}
library(boral)
library(mvabund) ## Load a dataset from the mvabund package
data(spider)
y <- spider$abun
X <- scale(spider$x)
n <- nrow(y)
p <- ncol(y)

## NOTE: The values below MUST NOT be used in a real application;
## they are only used here to make the examples run quick!!!
example_mcmc_control <- list(n.burnin = 10, n.iteration = 100, 
     n.thin = 1)

testpath <- file.path(tempdir(), "jagsboralmodel.txt")
 
 
## Example 1 - model with two latent variables, site effects, 
## 	and no environmental covariates
spiderfit_nb <- boral(y, family = "negative.binomial", 
    lv.control = list(num.lv = 2), row.eff = "fixed", 
    mcmc.control = example_mcmc_control, model.name = testpath)

summary(spiderfit_nb)

par(mfrow = c(2,2))
plot(spiderfit_nb) ## Plots used in residual analysis, 
## Used to check if assumptions such an mean-variance relationship 
## are adequately satisfied.
par(mfrow = c(1,1))
lvsplot(spiderfit_nb) ## Biplot of the latent variables, 
## which can be interpreted in the same manner as an ordination plot.

## Not run: 
## Example 2a - model with no latent variables, no site effects, 
##      and environmental covariates
spiderfit_nb <- boral(y, X = X, family = "negative.binomial", 
    mcmc.control = example_mcmc_control, model.name = testpath)

summary(spiderfit_nb) 
## The results can be compared with the default example from 
## the manyglm() function in mvabund. 

## Caterpillar plots for the coefficients
par(mfrow=c(2,3), mar = c(5,6,1,1))
sapply(colnames(spiderfit_nb$X), coefsplot, x = spiderfit_nb)


## Example 2b - suppose now, for some reason, the 28 rows were
## 	sampled such into four replications of seven sites
## Let us account for this as a fixed effect
spiderfit_nb <- boral(y, X = X, family = "negative.binomial", 
    row.eff = "fixed", row.ids = matrix(rep(1:7,each=4),ncol=1),
    mcmc.control = example_mcmc_control, model.name = testpath)

spiderfit_nb$row.coefs

## Example 2c - suppose now, for some reason, the 28 rows reflected
## 	a nested design with seven regions, each with four sub-regions
## We can account for this nesting as a random effect
spiderfit_nb <- boral(y, X = X, family = "negative.binomial", 
    row.eff = "random", 
    row.ids = cbind(1:n, rep(1:7,each=4)), 
    mcmc.control = example_mcmc_control, model.name = testpath)

spiderfit_nb$row.coefs

## Example 2d - model with environmental covariates and 
##  two structured latent variables using fake distance matrix
fakedistmat <- as.matrix(dist(1:n))
spiderfit_lvstruc <- boral(y, X = X, family = "negative.binomial", 
    lv.control = list(num.lv = 2, type = "exponential", distmat = fakedistmat), 
     mcmc.control = example_mcmc_control, model.name = testpath)


summary(spiderfit_lvstruc)

## Example 3a - Extend example 2 to demonstrate grouped covariate selection
## on the last three covariates. 
example_prior_control <- list(type = c("normal","normal","normal","uniform"), 
     ssvs.index = c(-1,-1,-1,1,2,3))
spiderfit_nb2 <- boral(y, X = X, family = "negative.binomial", 
    mcmc.control = example_mcmc_control,
    prior.control = example_prior_control, model.name = testpath)
     
summary(spiderfit_nb2) 


## Example 3b - Extend example 2 to demonstrate individual covariate selection
## on the last three covariates. 
example_prior_control <- list(type = c("normal","normal","normal","uniform"), 
    ssvs.index = c(-1,-1,-1,0,0,0))
spiderfit_nb3 <- boral(y, X = X, family = "negative.binomial", 
    mcmc.control = example_mcmc_control, prior.control = example_prior_control, 
    model.name = testpath)
summary(spiderfit_nb3) 

	
## Example 4 - model fitted to presence-absence data, no site effects, and
## two latent variables
data(tikus)
y <- tikus$abun
y[y > 0] <- 1
y <- y[1:20,] ## Consider only years 1981 and 1983
y <- y[,apply(y > 0,2,sum) > 2] ## Consider only spp with more than 2 presences
     
tikusfit <- boral(y, family = "binomial", 
    lv.control = list(num.lv = 2), mcmc.control = example_mcmc_control,
    model.name = testpath)
     
lvsplot(tikusfit, biplot = FALSE) 
## A strong location between the two sampling years 


## Example 5a - model fitted to count data, no site effects, and
## two latent variables, plus traits included to explain environmental responses
data(antTraits)
y <- antTraits$abun
X <- as.matrix(scale(antTraits$env))
## Include only traits 1, 2, and 5
traits <- as.matrix(antTraits$traits[,c(1,2,5)])
example_which_traits <- vector("list",ncol(X)+1)
for(i in 1:length(example_which_traits)) 
    example_which_traits[[i]] <- 1:ncol(traits)
## Just for fun, the regression coefficients for the second column of X,
## corresponding to the third element in the list example_which_traits,
## will be estimated separately and not regressed against traits.
example_which_traits[[3]] <- 0

fit_traits <- boral(y, X = X, traits = traits, 
    lv.control = list(num.lv = 2),
    which.traits = example_which_traits, family = "negative.binomial",
    mcmc.control = example_mcmc_control, model.name = testpath,
    save.model = TRUE)

summary(fit_traits)


## Example 5b - perform selection on trait coefficients
ssvs_traitsindex <- vector("list",ncol(X)+1)
for(i in 1:length(ssvs_traitsindex)) 
     ssvs_traitsindex[[i]] <- rep(0,ncol(traits))
ssvs_traitsindex[[3]] <- -1
fit_traits <- boral(y, X = X, traits = traits, which.traits = example_which_traits, 
    family = "negative.binomial", 
    lv.control = list(num.lv = 2), mcmc.control = example_mcmc_control, 
    save.model = TRUE, prior.control = list(ssvs.traitsindex = ssvs_traitsindex),
    model.name = testpath)

summary(fit_traits)


## Example 6 - simulate Bernoulli data, based on a model with two latent variables, 
## no site variables, with two traits and one environmental covariates 
## This example is a proof of concept that traits can used to 
## explain environmental responses 
library(mvtnorm)

n <- 100; s <- 50
X <- as.matrix(scale(1:n))
colnames(X) <- c("elevation")

traits <- cbind(rbinom(s,1,0.5), rnorm(s)) 
## one categorical and one continuous variable
colnames(traits) <- c("thorns-dummy","SLA")

simfit <- list(true.lv = rmvnorm(n, mean = rep(0,2)), 
    lv.coefs = cbind(rnorm(s), rmvnorm(s, mean = rep(0,2))), 
    traits.coefs = matrix(c(0.1,1,-0.5,1,0.5,0,-1,1), 2, byrow = TRUE))
rownames(simfit$traits.coefs) <- c("beta0","elevation")
colnames(simfit$traits.coefs) <- c("kappa0","thorns-dummy","SLA","sigma")

simy = create.life(true.lv = simfit$true.lv, lv.coefs = simfit$lv.coefs, X = X, 
    traits = traits, traits.coefs = simfit$traits.coefs, family = "binomial") 


example_which_traits <- vector("list",ncol(X)+1)
for(i in 1:length(example_which_traits)) 
    example_which_traits[[i]] <- 1:ncol(traits)
fit_traits <- boral(y = simy, X = X, traits = traits, 
    which.traits = example_which_traits, family = "binomial", 
    lv.control = list(num.lv = 2), save.model = TRUE, 
    mcmc.control = example_mcmc_control, model.name = testpath)

summary(fit_traits)
par(mfrow = c(1,1))
lvsplot(fit_traits)
```

```{r paletteer}
library(ggplot2)
library(paletteer)
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, 
                 colour = Species)) +
    geom_point() +
    scale_colour_paletteer_d("nord::frost")

colors <- palettes_d_names
palettes_d
colors <- palettes_dynamic_names
```

```{r short course in jsdms}
install.packages("mvabund")
Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS = TRUE)
devtools::install_github("gordy2x/ecoCopula",upgrade = "always")
Sys.setenv(R_REMOTES_NO_ERRORS_FROM_WARNINGS = FALSE)
```

```{r ecocopula}
library(mvabund)
library(ecoCopula)
data(spider)
abund <- mvabund(spider$abund)
X <- spider$x
spider_mod=manyglm(abund~1)
spid_lv=cord(spider_mod)
plot(spid_lv,biplot = TRUE)



data(spider)
abund <- mvabund(spider$abund)
X <- spider$x
spider_mod=manyglm(abund~X)
spid_graph=cgr(spider_mod)
plot(spid_graph,pad=1)

spid_lvx=cord(spider_mod)
plot(spid_lvx, biplot = TRUE)



library(ordinal)
library(mvabund)
library(ecoCopula)
library(corrplot)
mod <- manyglm(fhtnmdf_OTU_q ~ 1, family = "binomial")
str(fhtnmdf_OTU_q)
#str(mod)
cgr.mod <- cgr(mod, seed = 1) # seed for demonstration
plot(cgr.mod, pad = 1)
cord.mod <- cord(mod) # seed for demonstration
plot(cord.mod, biplot = TRUE)

corrplot(cor(fhtnmdf_OTU_q), diag = F, tl.cex = 0.3)
```

```{r nullabor}
library(tidyverse)
library(nullabor)
demo(lakers, package = "nullabor")
d <- lineup(null_permute("mpg"), mtcars)
head(d)
attr(d, "pos")
ggplot(data=d, aes(x=mpg, y=wt)) + geom_point() + facet_wrap(~ .sample)
# Rorschach protocol
d <- rorschach(null_permute("mpg"), mtcars, n = 20, p = 0)
ggplot(data=d, aes(x=mpg, y=wt)) + geom_point() + facet_wrap(~ .sample)
```

# inspect dataframes with plots. very nice!

```{r inspectdf}
library(inspectdf)
# Load dplyr for starwars data & pipe
library(dplyr)

# Load 'starwars' data
data("starwars", package = "dplyr")

# Horizontal bar plot for categorical column composition
x <- inspect_cat(starwars) 
show_plot(x)

# Correlation betwee numeric columns + confidence intervals
x <- inspect_cor(starwars)
show_plot(x)

# Bar plot of most frequent category for each categorical column
x <- inspect_imb(starwars)
show_plot(x)

# Bar plot showing memory usage for each column
x <- inspect_mem(starwars)
show_plot(x)

# Occurence of NAs in each column ranked in descending order
x <- inspect_na(starwars)
show_plot(x)

# Histograms for numeric columns
x <- inspect_num(starwars)
show_plot(x)

# Barplot of column types
x <- inspect_types(starwars)
show_plot(x)


# Single dataframe summary
# A tibble summarising or comparing the categorical features in one or a pair of dataframes.
inspect_cat(starwars)

# Paired dataframe comparison
inspect_cat(starwars, starwars[1:20, ])

# Grouped dataframe summary
starwars %>% group_by(gender) %>% inspect_cat()

# inspect_cor
# Single dataframe summary
inspect_cor(starwars)
# Only show correlations with 'mass' column
inspect_cor(starwars, with_col = "mass")

# Paired dataframe summary
inspect_cor(starwars, starwars[1:10, ])

# NOT RUN - change in correlation over time
# library(dplyr)
tech_grp <- tech %>%
        group_by(year) %>%
        inspect_cor()
tech_grp %>% show_plot()

# Single dataframe summary
inspect_imb(starwars)

# Paired dataframe comparison
inspect_imb(starwars, starwars[1:20, ])

# Grouped dataframe summary
starwars %>% group_by(gender) %>% inspect_imb()

```

```{r ecoPower}
# remotes::install_github("benmaslen/ecopower")
library(ecopower)
library(mvabund)
data(spider)
spiddat <- mvabund(spider$abund)
X <- data.frame(spider$x)

#Specify 'increasers' and 'decreasers'
increasers <- c("Alopacce","Arctlute" ,"Arctperi","Pardnigr", "Pardpull")
decreasers <- c("Alopcune","Alopfabr" ,"Zoraspin")

#Find power for continuous predictor, N=20 and effect.size=3
glm.spid <- manyglm(spiddat~soil.dry, family="negative.binomial",data=X)
effect.mat <- effect.alt(glm.spid,effect.size=3,
       pred="soil.dry",increasers,decreasers)
extend.fit <- extend.manyglm(glm.spid,N=10,
       coeffs=effect.mat) #not needed to be executed for power estimate
#powersim.manyglm(glm.spid,N=20,pred="soil.dry",
#      coeffs=effect.mat)

#Find power for categorical predictor with 4 levels, N=10, effect.size=1.5
X$Treatment <- rep(c("A","B","C","D"),each=7)
glm.spid <- manyglm(spiddat~Treatment, family="negative.binomial",data=X)
effect.mat <- effect.alt(glm.spid,effect.size=1.5,
     pred="Treatment",increasers,decreasers)
extend.fit <- extend.manyglm(glm.spid,N=20,
     coeffs=effect.mat) #not needed to be executed for power estimate
#powersim.manyglm(glm.spid,N=20,pred="Treatment",
#      coeffs=effect.mat)

#change effect size parameterisation
effect.mat <- effect.alt(glm.spid,effect.size=1.5,
                         pred="Treatment",increasers,decreasers,
                         K=c(3,1,2),OrderedLevels = FALSE)
#powersim.manyglm(glm.spid,N=20,pred="Treatment",
#                coeffs=effect.mat)

#change sampling design
X_new <- X
X_new$Treatment[6:7] <- c("B","B")
extend.fit <- extend.manyglm(glm.spid,N=20,
   coeffs=effect.mat,use.design = FALSE,newdata=X_new)
#powersim.manyglm(glm.spid,N=20,pred="Treatment",
#  coeffs=effect.mat,use.design = FALSE,newdata=X_new)
```

```{r rJava}
# download Java (JDK) for Developers from: https://www.oracle.com/java/technologies/javase-jdk14-downloads.html and

install.packages("rJava")

```

```{r}
# Mahalanobis distance
library(dismo)
logo <- stack(system.file("external/rlogo.grd", package="raster"))

#presence data
pts <- matrix(c(48.243420, 48.243420, 47.985820, 52.880230, 49.531423, 46.182616, 
  54.168232, 69.624263, 83.792291, 85.337894, 74.261072, 83.792291, 95.126713, 
  84.565092, 66.275456, 41.803408, 25.832176, 3.936132, 18.876962, 17.331359, 
  7.048974, 13.648543, 26.093446, 28.544714, 39.104026, 44.572240, 51.171810, 
  56.262906, 46.269272, 38.161230, 30.618865, 21.945145, 34.390047, 59.656971, 
  69.839163, 73.233228, 63.239594, 45.892154, 43.252326, 28.356155), ncol=2)

# fit model
m <- mahal(logo, pts)

# make a prediction
predict(m, logo[1])

x <- predict(m, logo)

# or x <- predict(logo, m) via raster::predict

# plot(x > 0)
require(graphics)

ma <- cbind(1:6, 1:3)
(S <-  var(ma))
mahalanobis(c(0, 0), 1:2, S)

x <- matrix(rnorm(100*3), ncol = 3)
stopifnot(mahalanobis(x, 0, diag(ncol(x))) == rowSums(x*x))
        ##- Here, D^2 = usual squared Euclidean distances

Sx <- cov(x)
D2 <- mahalanobis(x, colMeans(x), Sx)
plot(density(D2, bw = 0.5),
     main="Squared Mahalanobis distances, n=100, p=3") ; rug(D2)
qqplot(qchisq(ppoints(100), df = 3), D2,
       main = expression("Q-Q plot of Mahalanobis" * ~D^2 *
                         " vs. quantiles of" * ~ chi[3]^2))
abline(0, 1, col = 'gray')
```

```{r}
data(varespec)

## Bray-Curtis distances between samples
dis <- vegdist(varespec)

## First 16 sites grazed, remaining 8 sites ungrazed
groups <- factor(c(rep(1,16), rep(2,8)), labels = c("grazed","ungrazed"))

## Calculate multivariate dispersions
mod <- betadisper(dis, groups)
mod

## Perform test
anova(mod)

## Permutation test for F
permutest(mod, pairwise = TRUE, permutations = 99)

## Tukey's Honest Significant Differences
(mod.HSD <- TukeyHSD(mod))
plot(mod.HSD)

## Plot the groups and distances to centroids on the
## first two PCoA axes
plot(mod)

## with data ellipses instead of hulls
plot(mod, ellipse = TRUE, hull = FALSE) # 1 sd data ellipse
plot(mod, ellipse = TRUE, hull = FALSE, conf = 0.90) # 90% data ellipse

## can also specify which axes to plot, ordering respected
plot(mod, axes = c(3,1), seg.col = "forestgreen", seg.lty = "dashed")

## Draw a boxplot of the distances to centroid for each group
boxplot(mod)

## `scores` and `eigenvals` also work
scrs <- scores(mod)
str(scrs)
head(scores(mod, 1:4, display = "sites"))
# group centroids/medians 
scores(mod, 1:4, display = "centroids")
# eigenvalues from the underlying principal coordinates analysis
eigenvals(mod) 

## try out bias correction; compare with mod3
(mod3B <- betadisper(dis, groups, type = "median", bias.adjust=TRUE))
anova(mod3B)
permutest(mod3B, permutations = 99)

## should always work for a single group
group <- factor(rep("grazed", NROW(varespec))); group
(tmp <- betadisper(dis, group, type = "median"))
(tmp <- betadisper(dis, group, type = "centroid"))

## simulate missing values in 'd' and 'group'
## using spatial medians
groups[c(2,20)] <- NA
dis[c(2, 20)] <- NA
mod2 <- betadisper(dis, groups) ## messages
mod2
permutest(mod2, permutations = 99)
anova(mod2)
plot(mod2)
boxplot(mod2)
plot(TukeyHSD(mod2))

## Using group centroids
mod3 <- betadisper(dis, groups, type = "centroid")
mod3
permutest(mod3, permutations = 99)
anova(mod3)
plot(mod3)
boxplot(mod3)
plot(TukeyHSD(mod3))
```

```{r iNEXT}
library(iNEXT)
data(spider)
str(spider)
iNEXT(spider, q=0, datatype="abundance")
out <- iNEXT(spider, q=c(0, 1, 2), datatype="abundance", endpoint=500)
# Sample‐size‐based R/E curves, separating by "site""
ggiNEXT(out, type=1, facet.var="site")
## Not run:
# Sample‐size‐based R/E curves, separating by "order"
ggiNEXT(out, type=1, facet.var="order")
# display black‐white theme
ggiNEXT(out, type=1, facet.var="order", grey=TRUE)
## End(Not run) 


data(ant)
str(ant)
t <- seq(1, 700, by=10)
out.inc <- iNEXT(ant, q=0, datatype="incidence_freq", size=t)

# Sample‐size‐based R/E curves
ggiNEXT(out.inc, type=1, color.var="site") + 
  theme_bw(base_size = 18) + 
  theme(legend.position="none")

# Sample completeness curves
ggiNEXT(out.inc, type=2, color.var="site") +
  ylim(c(0.9,1)) +
  theme_bw(base_size = 18) + 
  theme(legend.position="none")
```

fs <https://fs.r-lib.org>

```{bash}
ssh ada
interactive
module add R
# module add R/4.0.0
R
```

```{r}
getwd()
dir_ls() # ls
dir_create("fs_test") # mkdir fs_test
dir_ls(glob = "fs_test") # ls fs_test
dir_create("fs_test") # mkdir fs_test but if path (here, fs_test) exists, it will not delete the pre-existing fs_test/ dir.  this makes it quite safe in one way. 
dir_ls(glob = "fs_test") # ls
# dir_delete("fs_test") # rm -rf fs_test # this will delete a directory regardless of what's in it.  it is dangerous
dir_ls(glob = "fs_test") # ls fs_test
```

```{r parallel dot plot}
library(robustbase)
library(patchwork)
help(epilepsy)

data(epilepsy)

names(epilepsy)

table(epilepsy$Trt)

epil<-reshape2::melt(epilepsy[ , c(1,11,2,3,4,5,8)], id=c("ID", "Trt"))

epil[epil$variable == "Base4", "dot.group"] <- "before"
epil[epil$variable != "Base4", "dot.group"] <- "after"
p1 <- ggplot(epil, aes(x=variable, y=value)) + 
  geom_dotplot(binaxis="y", binwidth = .5,stackdir = "centerwhole")

# recommended one
p2 <- ggplot(epil, aes(x=dot.group, y=value)) + 
  geom_path(aes(color = variable, group = factor(ID))) +
  geom_dotplot(binaxis="y", binwidth = .5, stackdir = "centerwhole")

p1 | p2
```

Feature reduction (PCA, cMDS, ICA, ...)

```{r parameters package}
library(parameters)
library(tidyverse)
data(attitude)
model <- lm(rating ~ ., data = attitude)
parameters(model)
newmodel <- reduce_parameters(model) # default PCA
parameters(newmodel) # 2 new parameters:  critical and all the others pooled

reduce_parameters(model, method = "cMDS") %>%  # cMDS classical MDS
  parameters()

# run pca separately
pca <- principal_components(insight::get_predictors(model), n = "auto")
pca
```

\#\> Parameter \| Coefficient \| SE \| 95% CI \| t \| df \| p \#\>
----------------------------------------------------------------------------------------------------------------------------------
\#\> (Intercept) \| 64.63 \| 1.57 \| [61.41, 67.85] \| 41.19 \| 27 \| \<
.001 \#\>
raises\_0.88/learning\_0.82/complaints\_0.78/privileges\_0.70/advance\_0.68
\| 4.62 \| 0.90 \| [ 2.78, 6.46] \| 5.16 \| 27 \| \< .001 \#\>
critical\_0.80 \| -3.41 \| 1.59 \| [-6.67, -0.14] \| -2.14 \| 27 \|
0.041

conditional filter

```{r}
(mtcars$type <- rownames(mtcars))
dplyr::filter(mtcars, grepl('Toyota|Mazda', type))

#    mpg cyl  disp  hp drat    wt  qsec vs am gear carb           type
# 1 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4      Mazda RX4
# 2 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  Mazda RX4 Wag
# 3 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 Toyota Corolla
# 4 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  Toyota Corona

dplyr::filter(mtcars, !grepl('Toyota|Mazda', type)) # don't include
```

equatiomatic The latex equations only appear upon knitting, but
textPreview can make the equation appear in the Viewer

```{r extra_eq1, results="asis"}
library(equatiomatic)
library(ordinal)
library(texPreview)
# fit a basic multiple linear regression model
m <- lm(bill_length_mm ~ bill_depth_mm + flipper_length_mm, penguins)
extract_eq(m)

m2 <- lm(bill_length_mm ~ bill_depth_mm*island, penguins)
extract_eq(m2)
extract_eq(m2, wrap = TRUE) # default terms_per_line = 4
extract_eq(m2, wrap = TRUE, terms_per_line = 2)
extract_eq(m2, wrap = TRUE, ital_vars = TRUE)
extract_eq(m2, wrap = TRUE, use_coefs = TRUE)
tex_preview(extract_eq(m2, wrap = TRUE, use_coefs = TRUE)) # eqn in Viewer


lr <- glm(sex ~ species*bill_length_mm, 
          data = penguins, 
          family = binomial(link = "logit"))
extract_eq(lr, wrap = TRUE) # latex code
tex_preview(extract_eq(lr, wrap = TRUE)) # eqn in Viewer

data(wine)
ordinal_ologit <- ordinal::clm(rating ~ temp * contact, 
                               data = ordinal::wine,
                               link = "logit")

extract_eq(ordinal_ologit, wrap = TRUE, terms_per_line = 2)
texPreview(extract_eq(ordinal_ologit, wrap = TRUE, terms_per_line = 2))

ordinal_probit <- ordinal::clm(rating ~ temp * contact, 
                               data = ordinal::wine,
                               link = "probit")
extract_eq(ordinal_probit, wrap = TRUE, terms_per_line = 2)

# appears in Viewer
tex_preview(extract_eq(ordinal_probit, wrap = TRUE, terms_per_line = 2))
```

Tjur's R2

```{r}
library(performance)
model <- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_tjur(model)
```

calculate rowSums for only some of the columns in a dataset

```{r}
fhtnmdf_filtered <- fhtnmdf %>%
  mutate(mbsum = select(., starts_with(c("Acti_", "Amph_"))) %>% 
            rowSums() ) %>% 
  filter(!is.na(mb_replicate)) %>% 
  filter(nm_Kit_ID != "FHTpc") %>% 
  filter(mbsum > 0) %>% 
  select(!starts_with(c("Aves_", "Mamm_")))

# the mutate() command stores in 'mbsum' the rowSums of a selection of columns (those that start with "Acti" and "Amph"). The mbsum column can then be used to filter out rows that mbsum == 0 values.  This way, i can remove rows from a full env and OTU table without first splitting the table. 
```

slice\_max() group by a variable, and extract the row with the max value
(or min value) of some variable from each group (groupwise rowwise
filtering)

```{r}
library(tidyverse)

ID    <- c(1,1,1,2,2,2,2,3,3)
Value <- c(2,3,5,2,5,8,17,3,5)
Event <- c(1,1,2,1,2,1,2,2,2)

(group <- data.frame(Subject=ID, pt=Value, Event=Event))

group %>% 
  group_by(Subject) %>% 
  slice_max(pt)
#> # A tibble: 3 x 3
#> # Groups:   Subject [3]
#>   Subject    pt Event
#>     <dbl> <dbl> <dbl>
#> 1       1     5     2
#> 2       2    17     2
#> 3       3     5     2
```

parallel version of purrr, using

```{r}
install.packages("furrr")
install.packages("future")
install.packages("tictoc")

# parallel implementation runs in 145 secs for the BF3BR2 dataset
future::plan(multisession) # uses all available cores
# future::plan(multisession, workers = 3) # uses 3 cores
tictoc::tic() # start time
genomecov_summ <- furrr::future_map_dfr(genomecov_files, loadFile2, .progress=TRUE) # output is a dataframe
tictoc::toc() # end time

# or use future_map() and then rbind the output list into a dataframe 
genomecov_summ <- do.call(rbind, genomecoverages_summ) 

```

https://rdrr.io/github/pkimes/sigclust2/
https://github.com/pkimes/sigclust2/
test for statistical significance in hierarchical clustering Ward
```{r}
BiocManager::install("pkimes/sigclust2")
library(sigclust2)

set.seed(1508)
n1 <- 60; n2 <- 40; n3 <- 50; n <- n1 + n2 + n3
p <- 100
data <- matrix(rnorm(n*p), nrow=n, ncol=p)
data[, 1] <- data[, 1] + c(rep(2, n1), rep(-2, n2), rep(0, n3))
data[, 2] <- data[, 2] + c(rep(0, n1+n2), rep(sqrt(3)*3, n3))

data_pc <- prcomp(data)
par(mfrow=c(1, 2))
plot(data_pc$x[, 2], data_pc$x[, 1], xlab="PC2", ylab="PC1")
plot(data_pc$x[, 3], data_pc$x[, 1], xlab="PC3", ylab="PC1")

shc_result <- shc(data, metric="euclidean", linkage="ward.D2")
summary(shc_result)
names(shc_result)

data.frame(result = head(shc_result$nd_type, 5),
           round(head(shc_result$p_norm, 5), 5),
           round(head(shc_result$p_emp, 5), 5))

shc_fwer <- shc(data, metric="euclidean", linkage="ward.D2", alpha=0.05)
data.frame(result = head(shc_fwer$nd_type, 10),
           round(head(shc_fwer$p_norm, 10), 5),
           round(head(shc_fwer$p_emp, 10), 5))


plot(shc_result, hang=.1)


diagnostic(shc_result, K=1, pty='background')
diagnostic(shc_result, K=1, pty='qq')

```

